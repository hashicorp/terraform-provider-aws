{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>The Terraform AWS Provider is the work of thousands of contributors, and is maintained by a small team within HashiCorp. This site contains extensive instructions about how to contribute and how the AWS provider works.</p> <p>Tip</p> <p>This documentation is intended for Terraform AWS Provider code developers. Typical operators writing and applying Terraform configurations do not need to read or understand this material.</p>"},{"location":"#contribute","title":"Contribute","text":"<p>Please follow the following steps to ensure your contribution goes smoothly.</p>"},{"location":"#1-configure-development-environment","title":"1. Configure Development Environment","text":"<p>Install Terraform and Go. Clone the repository, compile the provider, and set up testing. Refer to Configure Development Environment.</p>"},{"location":"#2-debug-code","title":"2. Debug Code","text":"<p>If you are looking to create or enhance code, such as a new resource or adding an argument to an existing resource, skip to the next step.</p> <p>Finding and fixing errors in the AWS Provider can be difficult. We have a debugging guide to help you get started.</p>"},{"location":"#3-change-code","title":"3. Change Code","text":"<p>Follow the guide for your contribution type and refer to the Development Reference materials as needed for additional details about provider design, expected naming conventions, guidance for error handling, etc.</p> Contribution Guide Description Small Changes Requirements for small additions or bug-fixes on existing resources/data sources Resources Allow the management of a logical resource within AWS by adding a new resource to the Terraform AWS Provider. Data Source Let your Terraform configurations use data from resources not under local management by creating ready only data sources. Services Allow Terraform (via the AWS Provider) to manage an entirely new AWS service by introducing the resources and data sources required to manage configuration of the service. AWS Region New regions are immediately usable with the provider with the caveat that a configuration workaround is required to skip validation of the region during cli operations. A small set of changes are required to make this workaround necessary. Resource Name Generation Allow a resource to either fully, or partially, generate its own resource names. This can be useful in cases where the resource name uniquely identifies the resource and it needs to be recreated. It can also be used when a name is required, but the specific name is not important. Tagging Support Many AWS resources allow assigning metadata via tags. However, frequently AWS services are launched without tagging support so this will often need to be added later. Import Support Adding import support allows <code>terraform import</code> to be run targeting an existing unmanaged resource and pulling its configuration into Terraform state. Typically import support is added during initial resource implementation but in some cases this will need to be added later. Enhanced Region Support Most AWS resources are Regional \u2013 they are created and exist in a single AWS Region. By default Regional resources have a top-level <code>region</code> argument that allows the Region to be configured. Documentation Changes The provider documentation is displayed on the Terraform Registry and is sourced and refreshed from the provider repository during the release process."},{"location":"#4-write-tests","title":"4. Write Tests","text":"<p>We require all changes to be covered by acceptance tests and/or unit tests, depending on the situation. In the context of the Terraform AWS Provider, acceptance tests are tests of interactions with AWS, such as creating, reading information about, and destroying AWS resources. In contrast, unit tests test functionality wholly within the provider itself, such as function tests.</p> <p>If you are unable to pay for acceptance tests for your contributions, mention this in your pull request. We will happily accept \"best effort\" acceptance tests implementations and run them for you on our side. Your PR may take longer to merge, but this is not a blocker for contributions.</p>"},{"location":"#5-continuous-integration","title":"5. Continuous Integration","text":"<p>When submitting a pull request, you'll notice that we run several automated processes on your proposed change.Some of these processes are tests to ensure your contribution aligns with our standards. While we strive for accuracy, some users may find these tests confusing. Check out Continuous Integration for additional clarity.</p>"},{"location":"#6-update-the-changelog","title":"6. Update the Changelog","text":"<p>HashiCorp's open-source projects have always maintained a user-friendly, readable CHANGELOG.md that allows users to tell at a glance whether a release should have any effect on them, and to gauge the risk of an upgrade. Not all changes require an entry in the changelog, refer to our Changelog Process for details about when and how to create a changelog.</p>"},{"location":"#7-create-a-pull-request","title":"7. Create a Pull Request","text":"<p>When your contribution is ready, Create a Pull Request in the AWS provider repository.</p> <p>Pull requests are usually triaged within a few days of creation and are prioritized based on community reactions. Our Prioritization Guides provide more details about the process.</p>"},{"location":"#submit-an-issue","title":"Submit an Issue","text":"<p>In addition to contributions, we welcome bug reports and feature requests.</p>"},{"location":"#join-the-contributors-slack","title":"Join the Contributors Slack","text":"<p>For frequent contributors, it's useful to join the contributors Slack channel hosted within the HashiCorp Slack workspace. This Slack channel is used to discuss topics such as general contribution questions, suggestions for improving the contribution process, coordinating on pair programming sessions, etc. The channel is not intended as a place to request status updates on open issues or pull requests. For prioritization questions, instead refer to the prioritization guide.</p> <p>To request to join, fill out the request form and allow time for the request to be reviewed and processed.</p>"},{"location":"acc-test-environment-variables/","title":"Acceptance Testing Environment Variable Dictionary","text":"<p>Environment variables (beyond standard AWS Go SDK ones) used by acceptance testing. See also the <code>internal/acctest</code> package.</p> Variable Description <code>ACM_CERTIFICATE_ROOT_DOMAIN</code> Root domain name to use with ACM Certificate testing. <code>ACM_TEST_CERTIFICATE_EXPORT</code> Flag to execute tests that enable exportable certificates. <code>ADM_CLIENT_ID</code> Identifier for Amazon Device Manager Client in Pinpoint testing. <code>AMPLIFY_DOMAIN_NAME</code> Domain name to use for Amplify domain association testing. <code>AMPLIFY_GITHUB_ACCESS_TOKEN</code> GitHub access token used for AWS Amplify testing. <code>AMPLIFY_GITHUB_REPOSITORY</code> GitHub repository used for AWS Amplify testing. <code>ADM_CLIENT_SECRET</code> Secret for Amazon Device Manager Client in Pinpoint testing. <code>APNS_BUNDLE_ID</code> Identifier for Apple Push Notification Service Bundle in Pinpoint testing. <code>APNS_CERTIFICATE</code> Certificate (PEM format) for Apple Push Notification Service in Pinpoint testing. <code>APNS_CERTIFICATE_PRIVATE_KEY</code> Private key for Apple Push Notification Service in Pinpoint testing. <code>APNS_SANDBOX_BUNDLE_ID</code> Identifier for Sandbox Apple Push Notification Service Bundle in Pinpoint testing. <code>APNS_SANDBOX_CERTIFICATE</code> Certificate (PEM format) for Sandbox Apple Push Notification Service in Pinpoint testing. <code>APNS_SANDBOX_CERTIFICATE_PRIVATE_KEY</code> Private key for Sandbox Apple Push Notification Service in Pinpoint testing. <code>APNS_SANDBOX_CREDENTIAL</code> Credential contents for Sandbox Apple Push Notification Service in SNS Application Platform testing. Conflicts with <code>APNS_SANDBOX_CREDENTIAL_PATH</code>. <code>APNS_SANDBOX_CREDENTIAL_PATH</code> Path to credential for Sandbox Apple Push Notification Service in SNS Application Platform testing. Conflicts with <code>APNS_SANDBOX_CREDENTIAL</code>. <code>APNS_SANDBOX_PRINCIPAL</code> Principal contents for Sandbox Apple Push Notification Service in SNS Application Platform testing. Conflicts with <code>APNS_SANDBOX_PRINCIPAL_PATH</code>. <code>APNS_SANDBOX_PRINCIPAL_PATH</code> Path to the principal for Sandbox Apple Push Notification Service in SNS Application Platform testing. Conflicts with <code>APNS_SANDBOX_PRINCIPAL</code>. <code>APNS_SANDBOX_TEAM_ID</code> Identifier for Sandbox Apple Push Notification Service Team in Pinpoint testing. <code>APNS_SANDBOX_TOKEN_KEY</code> Token key file content (.p8 format) for Sandbox Apple Push Notification Service in Pinpoint testing. <code>APNS_SANDBOX_TOKEN_KEY_ID</code> Identifier for Sandbox Apple Push Notification Service Token Key in Pinpoint testing. <code>APNS_TEAM_ID</code> Identifier for Apple Push Notification Service Team in Pinpoint testing. <code>APNS_TOKEN_KEY</code> Token key file content (.p8 format) for Apple Push Notification Service in Pinpoint testing. <code>APNS_TOKEN_KEY_ID</code> Identifier for Apple Push Notification Service Token Key in Pinpoint testing. <code>APNS_VOIP_BUNDLE_ID</code> Identifier for VOIP Apple Push Notification Service Bundle in Pinpoint testing. <code>APNS_VOIP_CERTIFICATE</code> Certificate (PEM format) for VOIP Apple Push Notification Service in Pinpoint testing. <code>APNS_VOIP_CERTIFICATE_PRIVATE_KEY</code> Private key for VOIP Apple Push Notification Service in Pinpoint testing. <code>APNS_VOIP_TEAM_ID</code> Identifier for VOIP Apple Push Notification Service Team in Pinpoint testing. <code>APNS_VOIP_TOKEN_KEY</code> Token key file content (.p8 format) for VOIP Apple Push Notification Service in Pinpoint testing. <code>APNS_VOIP_TOKEN_KEY_ID</code> Identifier for VOIP Apple Push Notification Service Token Key in Pinpoint testing. <code>APPRUNNER_CUSTOM_DOMAIN</code> A custom domain endpoint (root domain, subdomain, or wildcard) for AppRunner Custom Domain Association testing. <code>AUDITMANAGER_DEREGISTER_ACCOUNT_ON_DESTROY</code> Flag to execute tests that will disable AuditManager in the account upon destruction. <code>AUDITMANAGER_ORGANIZATION_ADMIN_ACCOUNT_ID</code> Organization admin account identifier for use in AuditManager testing. <code>AWS_ALTERNATE_ACCESS_KEY_ID</code> AWS access key ID with access to a secondary AWS account for tests requiring multiple accounts. Requires <code>AWS_ALTERNATE_SECRET_ACCESS_KEY</code>. Conflicts with <code>AWS_ALTERNATE_PROFILE</code>. <code>AWS_ALTERNATE_SECRET_ACCESS_KEY</code> AWS secret access key with access to a secondary AWS account for tests requiring multiple accounts. Requires <code>AWS_ALTERNATE_ACCESS_KEY_ID</code>. Conflicts with <code>AWS_ALTERNATE_PROFILE</code>. <code>AWS_ALTERNATE_PROFILE</code> AWS profile with access to a secondary AWS account for tests requiring multiple accounts. Conflicts with <code>AWS_ALTERNATE_ACCESS_KEY_ID</code> and <code>AWS_ALTERNATE_SECRET_ACCESS_KEY</code>. <code>AWS_ALTERNATE_REGION</code> Secondary AWS region for tests requiring multiple regions. Defaults to <code>us-east-1</code>. <code>AWS_API_GATEWAY_DOMAIN_NAME_CERTIFICATE_BODY</code> Certificate body of publicly trusted certificate for API Gateway Domain Name testing. <code>AWS_API_GATEWAY_DOMAIN_NAME_CERTIFICATE_CHAIN</code> Certificate chain of publicly trusted certificate for API Gateway Domain Name testing. <code>AWS_API_GATEWAY_DOMAIN_NAME_CERTIFICATE_PRIVATE_KEY</code> Private key of publicly trusted certificate for API Gateway Domain Name testing. <code>AWS_API_GATEWAY_DOMAIN_NAME_REGIONAL_CERTIFICATE_NAME_ENABLED</code> Flag to enable API Gateway Domain Name regional certificate upload testing. <code>AWS_BEDROCK_AGENTCORE_RUNTIME_IMAGE_V1_URI</code> ECR repository image URI (tagged as <code>v1</code>) for Bedrock AgentCore Agent Runtime acceptance tests. <code>AWS_BEDROCK_AGENTCORE_RUNTIME_IMAGE_V2_URI</code> ECR repository image URI (tagged as <code>v2</code>) for Bedrock AgentCore Agent Runtime acceptance <code>AWS_CODEBUILD_BITBUCKET_SOURCE_LOCATION</code> BitBucket source URL for CodeBuild testing. CodeBuild must have access to this repository via OAuth or Source Credentials. Defaults to <code>https://terraform@bitbucket.org/terraform/aws-test.git</code>. <code>AWS_CODEBUILD_GITHUB_SOURCE_LOCATION</code> GitHub source URL for CodeBuild testing. CodeBuild must have access to this repository via OAuth or Source Credentials. Defaults to <code>https://github.com/hashibot-test/aws-test.git</code>. <code>AWS_DEFAULT_REGION</code> Primary AWS region for tests. Defaults to <code>us-west-2</code>. <code>AWS_DETECTIVE_MEMBER_EMAIL</code> Email address for Detective Member testing. A valid email address associated with an AWS root account is required for tests to pass. <code>AWS_EC2_CLIENT_VPN_LIMIT</code> Concurrency limit for Client VPN acceptance tests. Default is 5 if not specified. <code>AWS_EC2_EIP_PUBLIC_IPV4_POOL</code> Identifier for EC2 Public IPv4 Pool for EC2 EIP testing. <code>AWS_EC2_TRANSIT_GATEWAY_LIMIT</code> Concurrency limit for Transit Gateway acceptance tests. Default is 5 if not specified. <code>AWS_EC2_VERIFIED_ACCESS_INSTANCE_LIMIT</code> Concurrency limit for Verified Access acceptance tests. Default is 5 if not specified. <code>AWS_GUARDDUTY_MEMBER_ACCOUNT_ID</code> Identifier of AWS Account for GuardDuty Member testing. DEPRECATED: Should be replaced with standard alternate account handling for tests. <code>AWS_GUARDDUTY_MEMBER_EMAIL</code> Email address for GuardDuty Member testing. DEPRECATED: It may be possible to use a placeholder email address instead. <code>AWS_LAMBDA_IMAGE_LATEST_ID</code> ECR repository image URI (tagged as <code>latest</code>) for Lambda container image acceptance tests. <code>AWS_LAMBDA_IMAGE_V1_ID</code> ECR repository image URI (tagged as <code>v1</code>) for Lambda container image acceptance tests. <code>AWS_LAMBDA_IMAGE_V2_ID</code> ECR repository image URI (tagged as <code>v2</code>) for Lambda container image acceptance tests. <code>AWS_THIRD_ACCESS_KEY_ID</code> AWS access key ID with access to a third AWS account for tests requiring multiple accounts. Requires <code>AWS_THIRD_SECRET_ACCESS_KEY</code>. Conflicts with <code>AWS_THIRD_PROFILE</code>. <code>AWS_THIRD_SECRET_ACCESS_KEY</code> AWS secret access key with access to a third AWS account for tests requiring multiple accounts. Requires <code>AWS_THIRD_ACCESS_KEY_ID</code>. Conflicts with <code>AWS_THIRD_PROFILE</code>. <code>AWS_THIRD_PROFILE</code> AWS profile with access to a third AWS account for tests requiring multiple accounts. Conflicts with <code>AWS_THIRD_ACCESS_KEY_ID</code> and <code>AWS_THIRD_SECRET_ACCESS_KEY</code>. <code>AWS_THIRD_REGION</code> Third AWS region for tests requiring multiple regions. Defaults to <code>us-east-2</code>. <code>CHATBOT_SLACK_CHANNEL_ID</code> ID of the Slack channel. <code>CHATBOT_SLACK_TEAM_ID</code> ID of the Slack workspace authorized with AWS Chatbot. <code>CHATBOT_TEAMS_CHANNEL_ID</code> ID of the Microsoft Teams channel. <code>CHATBOT_TEAMS_TEAM_ID</code> ID of the Microsoft Teams workspace authorized with AWS Chatbot. <code>CHATBOT_TEAMS_TENANT_ID</code> ID of the Microsoft Teams tenant. <code>CLOUD_HSM_CLUSTER_ID</code> Cloud HSM cluster identifier for KMS custom key store acceptance tests. <code>DX_CONNECTION_ID</code> Identifier for Direct Connect Connection testing. <code>DX_VIRTUAL_INTERFACE_ID</code> Identifier for Direct Connect Virtual Interface testing. <code>EC2_SECURITY_GROUP_RULES_PER_GROUP_LIMIT</code> EC2 Quota for Rules per Security Group. Defaults to 50. DEPRECATED: Can be augmented or replaced with Service Quotas lookup. <code>EVENT_BRIDGE_PARTNER_EVENT_BUS_NAME</code> Amazon EventBridge partner event bus name. <code>EVENT_BRIDGE_PARTNER_EVENT_SOURCE_NAME</code> Amazon EventBridge partner event source name. <code>FINSPACE_MANAGED_KX_LICENSE_ENABLED</code> Enables tests requiring a license to provision managed KX resources. <code>GCM_API_KEY</code> API Key for Google Cloud Messaging in Pinpoint and SNS Platform Application testing. <code>GITHUB_TOKEN</code> GitHub token for CodePipeline testing. <code>GLOBALACCERATOR_BYOIP_IPV4_ADDRESS</code> IPv4 address from a BYOIP CIDR of AWS Account used for testing Global Accelerator's BYOIP accelerator. <code>GRAFANA_SSO_GROUP_ID</code> AWS SSO group ID for Grafana testing. <code>GRAFANA_SSO_USER_ID</code> AWS SSO user ID for Grafana testing. <code>MACIE_MEMBER_ACCOUNT_ID</code> Identifier of AWS Account for Macie Member testing. DEPRECATED: Should be replaced with standard alternate account handling for tests. <code>QUICKSIGHT_NAMESPACE</code> QuickSight namespace name for testing. <code>QUICKSIGHT_ATHENA_TESTING_ENABLED</code> Enable QuickSight tests dependent on Amazon Athena resources. <code>ROUTE53DOMAINS_DOMAIN_NAME</code> Registered domain for Route 53 Domains testing. <code>RESOURCEEXPLORER_INDEX_TYPE</code> Index Type for Resource Explorer 2 Search datasource testing. <code>SAGEMAKER_IMAGE_VERSION_BASE_IMAGE</code> SageMaker base image to use for tests. <code>SERVICEQUOTAS_INCREASE_ON_CREATE_QUOTA_CODE</code> Quota Code for Service Quotas testing (submits support case). <code>SERVICEQUOTAS_INCREASE_ON_CREATE_SERVICE_CODE</code> Service Code for Service Quotas testing (submits support case). <code>SERVICEQUOTAS_INCREASE_ON_CREATE_VALUE</code> Value of quota increase for Service Quotas testing (submits support case). <code>SES_DOMAIN_IDENTITY_ROOT_DOMAIN</code> Root domain name of publicly accessible and Route 53 configurable domain for SES Domain Identity testing. <code>SES_DEDICATED_IP</code> Dedicated IP address for testing IP assignment with a \"Standard\" (non-managed) SES dedicated IP pool. <code>SWF_DOMAIN_TESTING_ENABLED</code> Enables SWF Domain testing (API does not support deletions). <code>TEST_AWS_ORGANIZATION_ACCOUNT_EMAIL_DOMAIN</code> Email address for Organizations Account testing. <code>TEST_AWS_SES_VERIFIED_EMAIL_ARN</code> Verified SES Email Identity for use in Cognito User Pool testing. <code>TF_ACC</code> Enables Go tests containing <code>resource.Test()</code> and <code>resource.ParallelTest()</code>. <code>TF_ACC_ASSUME_ROLE_ARN</code> Amazon Resource Name of existing IAM Role to use for limited permissions acceptance testing. <code>TF_AWS_BEDROCK_OSS_COLLECTION_NAME</code> Name of the OpenSearch Serverless collection to be used with an Amazon Bedrock Knowledge Base. <code>TF_AWS_CONTROLTOWER_CONTROL_OU_NAME</code> Organizational unit name to be targeted by the Control Tower control. <code>TF_AWS_CONTROLTOWER_BASELINE_ENABLE_BASELINE_ARN</code> Enable baseline ARN. <code>TF_AWS_DATAEXCHANGE_DATA_SET_ID</code> ID of DataExchange Data Set to use for testing. <code>TF_AWS_LICENSE_MANAGER_GRANT_HOME_REGION</code> Region where a License Manager license is imported. <code>TF_AWS_LICENSE_MANAGER_GRANT_LICENSE_ARN</code> ARN for a License Manager license imported into the current account. <code>TF_AWS_LICENSE_MANAGER_GRANT_PRINCIPAL</code> ARN of a principal to share the License Manager license with. Either a root user, Organization, or Organizational Unit. <code>TF_AWS_QUICKSIGHT_IDC_GROUP</code> Name of the IAM Identity Center Group to be assigned role membership. <code>TF_TEST_CLOUDFRONT_RETAIN</code> Flag to disable but dangle CloudFront Distributions during testing to reduce feedback time (must be manually destroyed afterwards). <code>TF_TEST_ELASTICACHE_RESERVED_CACHE_NODE</code> Flag to enable resource tests for ElastiCache reserved nodes. Set to <code>1</code> to run tests. <code>TRUST_ANCHOR_CERTIFICATE</code> Trust anchor certificate for KMS custom key store acceptance tests. <code>VPC_NETWORK_INTERFACE_TEST_MULTIPLE_CARDS</code> Flag to execute tests that enable to attach multiple network interfaces."},{"location":"add-a-new-datasource/","title":"Data source","text":""},{"location":"add-a-new-datasource/#adding-a-new-data-source","title":"Adding a New Data Source","text":"<p>New data sources are required when AWS adds a new service, or adds new features within an existing service which would require a new data source to allow practitioners to query existing resources of that type for use in their configurations. Anything with a Describe or Get endpoint could make a data source, but some are more useful than others.</p> <p>Each data source should be submitted for review in isolation, pull requests containing multiple data sources and/or resources are harder to review and the maintainers will normally ask for them to be broken apart.</p>"},{"location":"add-a-new-datasource/#prerequisites","title":"Prerequisites","text":"<p>If this is the first addition of a data source for a new service, please ensure the Service Client for the new service has been added and merged. See Adding a new Service for details.</p>"},{"location":"add-a-new-datasource/#steps-to-add-a-data-source","title":"Steps to Add a Data Source","text":""},{"location":"add-a-new-datasource/#fork-the-provider-and-create-a-feature-branch","title":"Fork the Provider and Create a Feature Branch","text":"<p>For a new data source use a branch named <code>f-{datasource name}</code> for example: <code>f-ec2-vpc</code>. See Raising a Pull Request for more details.</p>"},{"location":"add-a-new-datasource/#create-and-name-the-data-source","title":"Create and Name the Data Source","text":"<p>See the Naming Guide for details on how to name the new data source and the data source file. Not following the naming standards will cause extra delay as maintainers request that you make changes.</p> <p>Use the skaff provider scaffolding tool to generate new data source and test templates using your chosen name. Doing so will ensure that any boilerplate code, structural best practices and repetitive naming are done for you and always represent our most current standards.</p>"},{"location":"add-a-new-datasource/#fill-out-the-data-source-schema","title":"Fill out the Data Source Schema","text":"<p>In the <code>internal/service/&lt;service&gt;/&lt;service&gt;_data_source.go</code> file you will see a <code>Schema</code> property which exists as a map of <code>Schema</code> objects. This relates the AWS API data model with the Terraform resource itself. For each property you want to make available in Terraform, you will need to add it as an attribute, and choose the correct data type.</p> <p>Attribute names are to be specified in <code>snake_case</code> as opposed to the AWS API which is <code>CamelCase</code>.</p>"},{"location":"add-a-new-datasource/#implement-read-handler","title":"Implement Read Handler","text":"<p>These will map the AWS API response to the data source schema. You will also need to handle different response types (including errors correctly). For complex attributes you will need to implement Flattener or Expander functions. The Data Handling and Conversion Guide covers everything you need to know for mapping AWS API responses to Terraform State and vice-versa. The Error Handling Guide covers everything you need to know about handling AWS API responses consistently.</p>"},{"location":"add-a-new-datasource/#register-data-source-to-the-provider","title":"Register Data Source to the provider","text":"<p>Data Sources use a self-registration process that adds them to the provider using the <code>@FrameworkDataSource()</code> (Preferred) or <code>@SDKDataSource()</code> annotation in the data source's comments. Run <code>make gen</code> to register the data source. This will add an entry to the <code>service_package_gen.go</code> file located in the service package folder.</p> Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <pre><code>package something\n\nimport (\n\"github.com/hashicorp/terraform-plugin-framework/datasource\"\n\"github.com/hashicorp/terraform-provider-aws/internal/framework\"\n)\n\n// @FrameworkDataSource(\"aws_something_example\", name=\"Example\")\nfunc newExampleDataSource(_ context.Context) (datasource.DataSourceWithConfigure, error) {\nreturn &amp;exampleDataSource{}, nil\n}\n\ntype exampleDataSource struct {\nframework.DataSourceWithModel[exampleDataSourceModel]\n}\n\ntype exampleDataSourceModel {\n// Fields corresponding to attributes in the Schema.\n}\n</code></pre> <pre><code>package something\n\nimport \"github.com/hashicorp/terraform-plugin-sdk/v2/helper/schema\"\n\n// @SDKDataSource(\"aws_something_example\", name=\"Example\")\nfunc DataSourceExample() *schema.Resource {\nreturn &amp;schema.Resource{\n// some configuration\n}\n}\n</code></pre>"},{"location":"add-a-new-datasource/#write-passing-acceptance-tests","title":"Write Passing Acceptance Tests","text":"<p>To adequately test the data source we will need to write a complete set of Acceptance Tests. You will need an AWS account for this which allows the provider to read to state of the associated resource. See Writing Acceptance Tests for a detailed guide on how to approach these.</p> <p>You will need at a minimum:</p> <ul> <li>Basic Test - Tests full lifecycle (CRUD + Import) of a minimal configuration (all required fields, no optional).</li> <li>Disappears Test - Tests what Terraform does if a resource it is tracking can no longer be found.</li> <li>Per Attribute Tests - For each attribute a test should exist which tests that particular attribute in isolation alongside any required fields.</li> </ul>"},{"location":"add-a-new-datasource/#create-documentation-for-the-data-source","title":"Create Documentation for the Data Source","text":"<p>Add a file covering the use of the new data source in <code>website/docs/d/&lt;service&gt;_&lt;name&gt;.md</code>. You may want to also add examples of the data source in use particularly if its use is complex, or relies on resources in another service. This documentation will appear on the Terraform Registry when the data source is made available in a provider release. It is fine to link out to AWS Documentation where appropriate, particularly for values which are likely to change.</p>"},{"location":"add-a-new-datasource/#ensure-format-and-lint-checks-are-passing-locally","title":"Ensure Format and Lint Checks are Passing Locally","text":"<p>Run <code>go fmt</code> to format your code, and install and run all linters to detect and resolve any structural issues with the implementation or documentation.</p> <pre><code>make fmt\nmake tools        # install linters and dependencies\nmake lint         # run provider linters\nmake docs-lint    # run documentation linters\nmake website-lint # run website documentation linters\n</code></pre>"},{"location":"add-a-new-datasource/#raise-a-pull-request","title":"Raise a Pull Request","text":"<p>See Raising a Pull Request.</p>"},{"location":"add-a-new-datasource/#wait-for-prioritization","title":"Wait for Prioritization","text":"<p>In general, pull requests are triaged within a few days of creation and are prioritized based on community reactions. Please view our prioritization guide for full details of the process.</p>"},{"location":"add-a-new-ephemeral-resource/","title":"Ephemeral Resource","text":""},{"location":"add-a-new-ephemeral-resource/#adding-a-new-ephemeral-resource","title":"Adding a New Ephemeral Resource","text":"<p>New ephemeral resources are required when AWS introduces a new service or adds features to an existing service that necessitate a new ephemeral resource. Ephemeral resources produce ephemeral values and are never stored in the state. Any resource that produces a sensitive value can be an ephemeral resource, though some are more useful than others.</p> <p>Each ephemeral resource should be submitted for review individually. Pull requests containing multiple ephemeral resources or other resources are more difficult to review, and maintainers will typically request that they be split into separate submissions.</p>"},{"location":"add-a-new-ephemeral-resource/#prerequisites","title":"Prerequisites","text":"<p>If an ephemeral resource is the first addition for a new service, ensure that the Service Client for the service has been created and merged first. Refer to Adding a New Service for detailed instructions.</p>"},{"location":"add-a-new-ephemeral-resource/#steps-to-add-a-ephemeral-resource","title":"Steps to Add a Ephemeral Resource","text":""},{"location":"add-a-new-ephemeral-resource/#fork-the-provider-and-create-a-feature-branch","title":"Fork the Provider and Create a Feature Branch","text":"<p>For a new ephemeral resource, use a branch name in the format <code>f-{ephemeral-resource-name}</code>, for example: <code>f-kms-secret</code>. See Raising a Pull Request for more details.</p>"},{"location":"add-a-new-ephemeral-resource/#create-and-name-the-ephemeral-resource","title":"Create and Name the Ephemeral Resource","text":"<p>See the Naming Guide for details on how to name the new ephemeral resource and the ephemeral resource file. Not following the naming standards will cause extra delay as maintainers request that you make changes.</p> <p>Use the skaff provider scaffolding tool to generate new ephemeral resource and test templates using your chosen name. Doing so will ensure that any boilerplate code, structural best practices and repetitive naming are done for you and always represent our most current standards.</p>"},{"location":"add-a-new-ephemeral-resource/#fill-out-the-ephemeral-resource-schema","title":"Fill out the Ephemeral Resource Schema","text":"<p>In the <code>internal/service/&lt;service&gt;/&lt;service&gt;_ephemeral.go</code> file, you'll find a <code>Schema</code> property, which is a map of <code>Schema</code> objects. This maps the AWS API data model to the Terraform resource. To make a property available in Terraform, add it as an attribute with the appropriate data type.</p> <p>Define attributes using <code>snake_case</code>, instead of the <code>CamelCase</code> format used by the AWS API.</p>"},{"location":"add-a-new-ephemeral-resource/#implement-open-handler","title":"Implement Open Handler","text":"<p><code>Open</code> will map the AWS API response to the ephemeral resource schema. You\u2019ll also need to handle different response types, including errors, correctly. You will typically use <code>Autoflex</code> for mapping AWS API responses to Terraform models and vice versa. The Error Handling Guide covers best practices for handling AWS API responses consistently.</p>"},{"location":"add-a-new-ephemeral-resource/#register-ephemeral-resource-to-the-provider","title":"Register Ephemeral Resource to the provider","text":"<p>Ephemeral resources use a self-registration process that adds them to the provider via the <code>@EphemeralResource()</code> annotation in the resource's comments. To register the ephemeral resource, run <code>make gen</code>. This will generate an entry in the <code>service_package_gen.go</code> file located in the service package folder.</p> <pre><code>package something\n\nimport (\n\"context\"\n\n\"github.com/hashicorp/terraform-plugin-framework/ephemeral\"\n\"github.com/hashicorp/terraform-provider-aws/internal/framework\"\n)\n\n// @EphemeralResource(\"aws_something_example\", name=\"Example\")\nfunc newExampleEphemeralResource(_ context.Context) (ephemeral.EphemeralResourceWithConfigure, error) {\nreturn &amp;exampleEphemeralResource{}, nil\n}\n\ntype exampleEphemeralResource struct {\nframework.EphemeralResourceWithModel[exampleEphemeralResourceModel]\n}\n\ntype exampleEphemeralResourceModel {\n// Fields corresponding to attributes in the Schema.\n}\n</code></pre>"},{"location":"add-a-new-ephemeral-resource/#write-passing-acceptance-tests","title":"Write Passing Acceptance Tests","text":"<p>To properly test the ephemeral resource, write a complete set of acceptance tests. An AWS account is required, which allows the provider to communicate with the AWS API. For a detailed guide on how to write and run these tests, refer to Writing Acceptance Tests.</p> <p>You will need at a minimum:</p> <ul> <li>Basic Test - Tests full lifecycle (CRUD + Import) of a minimal configuration (all required fields, no optional).</li> <li>Per Attribute Tests - For each attribute a test should exist which tests that particular attribute in isolation alongside any required fields.</li> </ul>"},{"location":"add-a-new-ephemeral-resource/#create-documentation-for-the-ephemeral-resource","title":"Create Documentation for the Ephemeral Resource","text":"<p>Create a file documenting the use of the new ephemeral resource in <code>website/docs/ephemeral-resources/&lt;service&gt;_&lt;name&gt;.md</code> including a basic example. You may also want to include additional examples of the resource in use, especially if its usage is complex or depends on resources from another service. This documentation will appear on the Terraform Registry when the ephemeral resource is included in a provider release. It\u2019s acceptable to link to AWS Documentation where appropriate, particularly for values that are likely to change.</p>"},{"location":"add-a-new-ephemeral-resource/#ensure-format-and-lint-checks-are-passing-locally","title":"Ensure Format and Lint Checks are Passing Locally","text":"<p>Run <code>go fmt</code> to format your code, and install and run all linters to detect and resolve any structural issues with the implementation or documentation.</p> <pre><code>make fmt\nmake tools        # install linters and dependencies\nmake lint         # run provider linters\nmake docs-lint    # run documentation linters\nmake website-lint # run website documentation linters\n</code></pre>"},{"location":"add-a-new-ephemeral-resource/#raise-a-pull-request","title":"Raise a Pull Request","text":"<p>See Raising a Pull Request.</p>"},{"location":"add-a-new-ephemeral-resource/#wait-for-prioritization","title":"Wait for Prioritization","text":"<p>In general, pull requests are triaged within a few days of creation and are prioritized based on community reactions. Please view our prioritization guide for full details of the process.</p>"},{"location":"add-a-new-function/","title":"Adding a New Function","text":"<p>Provider-defined functions were introduced with Terraform 1.8, enabling provider developers to expose functions specific to a given cloud provider or use case. Functions in the AWS provider provide a utility that is valuable when paired with AWS resources.</p> <p>See the Terraform Plugin Framework Function documentation for additional details.</p>"},{"location":"add-a-new-function/#prerequisites","title":"Prerequisites","text":"<p>The only prerequisite for creating a function is ensuring the desired functionality is appropriate for a provider-defined function. Functions must be reproducible across executions (\"pure\" functions), where the same input always results in the same output. This requirement precludes the use of network calls, so operations requiring an AWS API call should instead consider utilizing a data source. Data manipulation tasks tend to be the most common use cases.</p>"},{"location":"add-a-new-function/#steps-to-add-a-function","title":"Steps to add a function","text":""},{"location":"add-a-new-function/#fork-the-provider-and-create-a-feature-branch","title":"Fork the provider and create a feature branch","text":"<p>For a new function use a branch named <code>f-{function name}</code>, for example, <code>f-arn_parse</code>. See Raising a Pull Request for more details.</p>"},{"location":"add-a-new-function/#generate-function-scaffolding","title":"Generate function scaffolding","text":"<p>The <code>skaff function</code> subcommand can be used to generate an outline for the new function.</p> <p>First, install <code>skaff</code> and navigate to the directory where provider functions are defined (<code>internal/function</code>).</p> <pre><code>make skaff\n</code></pre> <pre><code>cd internal/function\n</code></pre> <p>Next, run the <code>skaff function</code> subcommand. The name and description flags are required. The name argument should be mixed caps (ie. <code>FooBar</code>), and the generator will handle converting the name to snake case where appropriate.</p> <pre><code>skaff function -n Example -d \"Makes some output from an input.\"\n</code></pre> <p>This will generate files storing the function definition, unit tests, and registry documentation. The following steps describe how to complete the function implementation.</p>"},{"location":"add-a-new-function/#fill-out-the-function-parameters-and-return-value","title":"Fill out the function parameter(s) and return value","text":"<p>The function struct's <code>Definition</code> method will document the expected parameters and return value. Parameter names and return values should be specified in <code>snake_case</code>.</p> <pre><code>func (f exampleFunction) Definition(ctx context.Context, req function.DefinitionRequest, resp *function.DefinitionResponse) {\nresp.Definition = function.Definition{\nParameters: []function.Parameter{\nfunction.StringParameter{Name: \"some_arg\"},\n},\nReturn: function.StringReturn{},\n}\n}\n</code></pre> <p>The example above defines a function which accepts a string parameter, <code>some_arg</code>, and returns a string value.</p>"},{"location":"add-a-new-function/#implement-the-function-logic","title":"Implement the function logic","text":"<p>The function struct's <code>Run</code> method will contain the function logic. This includes processing the arguments, setting the return value, and any data processing that needs to happen in between.</p> <pre><code>func (f exampleFunction) Run(ctx context.Context, req function.RunRequest, resp *function.RunResponse) {\nvar data string\n\nresp.Error = function.ConcatFuncErrors(req.Arguments.Get(ctx, &amp;data))\nif resp.Error != nil {\nreturn\n}\n\n//\n// Function logic goes here\n//\n\nresp.Error = function.ConcatFuncErrors(resp.Result.Set(ctx, data))\n}\n</code></pre>"},{"location":"add-a-new-function/#register-function-to-the-provider","title":"Register function to the provider","text":"<p>Once the function is implemented, it must be registered to the provider to be used. As only Terraform Plugin Framework supports provider-defined functions, registration occurs on the Plugin Framework provider inside <code>internal/provider/fwprovider/provider.go</code>. Add the <code>New*</code> factory function in the <code>Functions</code> method to register it.</p> <pre><code>func (p *fwprovider) Functions(_ context.Context) []func() function.Function {\nreturn []func() function.Function{\n// Append to list of existing functions here\ntffunction.NewExampleFunction,\n}\n}\n</code></pre>"},{"location":"add-a-new-function/#write-passing-acceptance-tests","title":"Write passing acceptance tests","text":"<p>All functions should have corresponding acceptance tests. For functions with variadic arguments, or which can potentially return an error, tests should be written to exercise those conditions.</p> <p>An example outline is included below:</p> <pre><code>// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\npackage function_test\n\nimport (\n\"fmt\"\n\"testing\"\n\n\"github.com/hashicorp/go-version\"\n\"github.com/hashicorp/terraform-plugin-testing/helper/resource\"\n\"github.com/hashicorp/terraform-plugin-testing/tfversion\"\n\"github.com/hashicorp/terraform-provider-aws/internal/acctest\"\n)\n\nfunc TestExampleFunction_basic(t *testing.T) {\nt.Parallel()\n\nresource.UnitTest(t, resource.TestCase{\nProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories,\nTerraformVersionChecks: []tfversion.TerraformVersionCheck{\ntfversion.SkipBelow(version.Must(version.NewVersion(\"1.8.0\"))),\n},\nSteps: []resource.TestStep{\n{\nConfig: testExampleFunctionConfig(\"foo\"),\nCheck: resource.ComposeAggregateTestCheckFunc(\nresource.TestCheckOutput(\"test\", \"foo\"),\n),\n},\n},\n})\n}\n\nfunc testExampleFunctionConfig(arg string) string {\nreturn fmt.Sprintf(`\noutput \"test\" {\n  value = provider::aws::example(%[1]q)\n}`, arg)\n}\n</code></pre> <p>With Terraform 1.8+ installed, individual tests can be run like:</p> <pre><code>go test -run='^TestExampleFunction' -v ./internal/function/\n</code></pre>"},{"location":"add-a-new-function/#create-documentation-for-the-resource","title":"Create documentation for the resource","text":"<p><code>skaff</code> will have generated framed out registry documentation in <code>website/docs/functions/&lt;function name&gt;.html.markdown</code>. The <code>Example Usage</code>, <code>Signature</code>, and <code>Arguments</code> sections should all be updated with the appropriate content. Once released, this documentation will appear on the Terraform Registry.</p>"},{"location":"add-a-new-function/#raise-a-pull-request","title":"Raise a pull request","text":"<p>See Raising a Pull Request.</p>"},{"location":"add-a-new-function/#wait-for-prioritization","title":"Wait for prioritization","text":"<p>In general, pull requests are triaged within a few days of creation and are prioritized based on community reactions. Please view our Prioritization Guide for full details of the process.</p>"},{"location":"add-a-new-region/","title":"Adding a Newly Released AWS Region","text":"<p>New regions can typically be used immediately with the provider, with two important caveats:</p> <ul> <li>Regions often need to be explicitly enabled via the AWS console. See ap-east-1 launch blog for an example of how to enable a new region for use.</li> <li>Until the provider is aware of the new region, automatic region validation will fail. To use the region before validation support is added to the provider you will need to disable region validation by doing the following:</li> </ul> <pre><code>provider \"aws\" {\n  # ... potentially other configuration ...\n\nregion                 = \"me-south-1\"\nskip_region_validation = true\n}\n</code></pre>"},{"location":"add-a-new-region/#enabling-region-validation","title":"Enabling Region Validation","text":"<p>Support for region validation requires that the provider has an updated AWS SDK Go Base dependency that includes the new region. This also needs to be done in the core Terraform binary itself to enable it for the S3 backend. Many of the authentication and provider-level configuration interactions are also located in the <code>aws-go-sdk-base</code> library. As all of these things take direct dependencies and as a result there end up being quite a few places where these dependency updates need to be made.</p>"},{"location":"add-a-new-region/#update-aws-go-sdk-base","title":"Update aws-go-sdk-base","text":"<p>aws-go-sdk-base</p> <ul> <li>Update aws-go-sdk-v2</li> </ul>"},{"location":"add-a-new-region/#update-terraform-aws-provider","title":"Update Terraform AWS Provider","text":"<p>provider</p> <ul> <li>Update aws-go-sdk-v2</li> <li>Update aws-go-sdk-base</li> </ul>"},{"location":"add-a-new-region/#update-terraform-core-s3-backend","title":"Update Terraform Core (S3 Backend)","text":"<p>core</p> <ul> <li>Update aws-go-sdk-v2</li> <li>Update aws-go-sdk-base</li> </ul> <p>See the Changelog Process document for example changelog format.</p>"},{"location":"add-a-new-region/#update-region-specific-values-in-static-data-sources","title":"Update Region Specific values in static Data Sources","text":"<p>Some data sources include static values specific to regions that are not available via a standard AWS API call. These will need to be manually updated. AWS employees can code search previous region values to find new region values in internal packages like RIPStaticConfig if they are not documented yet.</p> <ul> <li>Check Elastic Load Balancing endpoints and quotas and add Route53 Hosted Zone ID if available to <code>internal/service/elb/hosted_zone_id_data_source.go</code> and <code>internal/service/elbv2/hosted_zone_id_data_source.go</code></li> <li>Check Amazon Simple Storage Service endpoints and quotas and add Route53 Hosted Zone ID if available to <code>internal/service/s3/hosted_zones.go</code></li> <li>Check AWS Elastic Beanstalk endpoints and quotas and add Route53 Hosted Zone ID if available to <code>internal/service/elasticbeanstalk/hosted_zone_data_source.go</code></li> <li>Check SageMaker docs and add AWS Account IDs if available to <code>internal/service/sagemaker/prebuilt_ecr_image_data_source.go</code></li> <li>Check App Runner docs and add Route53 Hosted Zone ID if available to <code>internal/service/apprunner/hosted_zone_id_data_source.go</code></li> </ul>"},{"location":"add-a-new-resource/","title":"Resource","text":""},{"location":"add-a-new-resource/#adding-a-new-resource","title":"Adding a New Resource","text":"<p>New resources are required when AWS adds a new service, or adds new features within an existing service which would require a new resource to manage in Terraform. Typically anything with a new set of CRUD API endpoints is a great candidate for a new resource.</p> <p>Each resource should be submitted for review in isolation. Pull requests containing multiple resources are harder to review and the maintainers will normally ask for them to be broken apart.</p>"},{"location":"add-a-new-resource/#prerequisites","title":"Prerequisites","text":"<p>If this is the first resource for a new service, please ensure the Service Client for the new service has been added and merged. See Adding a new Service for details.</p>"},{"location":"add-a-new-resource/#steps-to-add-a-resource","title":"Steps to Add a Resource","text":""},{"location":"add-a-new-resource/#fork-the-provider-and-create-a-feature-branch","title":"Fork the provider and create a feature branch","text":"<p>For new resources use a branch named <code>f-{resource name}</code> for example: <code>f-ec2-vpc</code>. See Raising a Pull Request for more details.</p>"},{"location":"add-a-new-resource/#create-and-name-the-resource","title":"Create and Name the Resource","text":"<p>See the Naming Guide for details on how to name the new resource and the resource file. Not following the naming standards will cause extra delay as maintainers request that you make changes.</p> <p>Use the skaff provider scaffolding tool to generate new resource and test templates using your chosen name. Doing so will ensure that any boilerplate code, structural best practices and repetitive naming are done for you and always represent our most current standards.</p>"},{"location":"add-a-new-resource/#fill-out-the-resource-schema","title":"Fill out the Resource Schema","text":"<p>In the <code>internal/service/&lt;service&gt;/&lt;service&gt;.go</code> file you will see a <code>Schema</code> property which exists as a map of <code>Schema</code> objects. This relates the AWS API data model with the Terraform resource itself. For each property you want to make available in Terraform, you will need to add it as an attribute, choose the correct data type and supply the correct Schema Behaviors to ensure Terraform knows how to correctly handle the value.</p> <p>Typically you will add arguments to represent the values that are under control by Terraform, and attributes to supply read-only values as references for Terraform. These are distinguished by Schema Behavior.</p> <p>Attribute names are to be specified in <code>snake_case</code> as opposed to the AWS API which is <code>CamelCase</code>.</p>"},{"location":"add-a-new-resource/#implement-crud-handlers","title":"Implement CRUD handlers","text":"<p>These will map the planned Terraform state to the AWS API call, or an AWS API response to an applied Terraform state. You will also need to handle different response types (including errors correctly). For complex attributes, you will need to implement Flattener or Expander functions. The Data Handling and Conversion Guide covers everything you need to know for mapping AWS API responses to Terraform State and vice-versa. The Error Handling Guide covers everything you need to know about handling AWS API responses consistently.</p>"},{"location":"add-a-new-resource/#register-resource-to-the-provider","title":"Register Resource to the provider","text":"<p>Resources use a self-registration process that adds them to the provider using the <code>@FrameworkResource()</code> or <code>@SDKResource()</code> annotation in the resource's comments. Run <code>make gen</code> to register the resource. This will add an entry to the <code>service_package_gen.go</code> file located in the service package folder.</p> Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <pre><code>package something\n\nimport (\n\"github.com/hashicorp/terraform-plugin-framework/resource\"\n\"github.com/hashicorp/terraform-provider-aws/internal/framework\"\n)\n\n// @FrameworkResource(\"aws_something_example\", name=\"Example\")\nfunc newExampleResource(_ context.Context) (resource.ResourceWithConfigure, error) {\nreturn &amp;resourceExample{}, nil\n}\n\ntype exampleResource struct {\nframework.ResourceWithModel[exampleResourceModel]\n}\n\ntype exampleResourceModel struct {\n// Fields corresponding to attributes in the Schema.\n}\n</code></pre> <pre><code>package something\n\nimport \"github.com/hashicorp/terraform-plugin-sdk/v2/helper/schema\"\n\n// @SDKResource(\"aws_something_example\", name=\"Example)\nfunc ResourceExample() *schema.Resource {\nreturn &amp;schema.Resource{\n// some configuration\n}\n}\n</code></pre>"},{"location":"add-a-new-resource/#write-passing-acceptance-tests","title":"Write passing Acceptance Tests","text":"<p>To adequately test the resource we will need to write a complete set of Acceptance Tests. You will need an AWS account for this which allows the creation of that resource. See Writing Acceptance Tests for a detailed guide on how to approach these.</p> <p>You will need at a minimum:</p> <ul> <li>Basic Test - Tests full lifecycle (CRUD + Import) of a minimal configuration (all required fields, no optional).</li> <li>Disappears Test - Tests what Terraform does if a resource it is tracking can no longer be found.</li> <li>Argument Tests - All arguments should be tested in a pragmatic way. Ensure that each argument can be initially set, updated, and cleared, as applicable. Depending on the logic and interaction of arguments, this may take one to several separate tests.</li> </ul>"},{"location":"add-a-new-resource/#create-documentation-for-the-resource","title":"Create documentation for the resource","text":"<p>Add a file covering the use of the new resource in <code>website/docs/r/&lt;service&gt;_&lt;name&gt;.md</code>. Add more examples if it is complex or relies on resources in another service. This documentation will appear on the Terraform Registry when the resource is made available in a provider release. Link to AWS Documentation where appropriate, particularly for values which are likely to change.</p>"},{"location":"add-a-new-resource/#ensure-format-and-lint-checks-are-passing-locally","title":"Ensure format and lint checks are passing locally","text":"<p>Format your code and check linters to detect various issues.</p> <pre><code>make fmt\nmake tools     # install linters and dependencies\nmake lint      # run provider linters\nmake docs-lint # run documentation linters\n</code></pre>"},{"location":"add-a-new-resource/#raise-a-pull-request","title":"Raise a Pull Request","text":"<p>See Raising a Pull Request.</p>"},{"location":"add-a-new-resource/#wait-for-prioritization","title":"Wait for Prioritization","text":"<p>In general, pull requests are triaged within a few days of creation and are prioritized based on community reactions. Please view our Prioritization Guide for full details of the process.</p>"},{"location":"add-a-new-service/","title":"Service","text":""},{"location":"add-a-new-service/#adding-a-new-aws-service","title":"Adding a New AWS Service","text":"<p>AWS frequently launches new services, and Terraform support is frequently desired by the community shortly after launch. Depending on the API surface area of the new service, this could be a major undertaking. The following steps should be followed to prepare for adding the resources that allow for Terraform management of that service.</p>"},{"location":"add-a-new-service/#perform-service-design","title":"Perform Service Design","text":"<p>Before adding a new service to the provider it's a good idea to familiarize yourself with the primary workflows practitioners are likely to want to accomplish with the provider to ensure the provider design can solve this. It's not always necessary to cover 100% of the AWS service offering to unblock most workflows.</p> <p>You should have an idea of what resources and data sources should be added, their dependencies and relative importance concerning the workflow. This should give you an idea of the order in which resources are to be added. It's important to note that generally, we like to review and merge resources in isolation, and avoid combining multiple new resources in one Pull Request.</p> <p>Using the AWS API documentation as a reference, identify the various APIs that correspond to the CRUD operations which consist of the management surface for that resource. These will be the set of APIs called from the new resource. The API's model attributes will correspond to your resource schema.</p> <p>From there begin to map out the list of resources you would like to implement, and note your plan on the GitHub issue relating to the service (or create one if one does not exist) for the community and maintainers to feedback.</p>"},{"location":"add-a-new-service/#add-a-service-client","title":"Add a Service Client","text":"<p>Before new resources are submitted, please raise a separate pull request containing just the new AWS SDK for Go service client.</p> <p>To add an AWS SDK for Go service client:</p> <ol> <li> <p>Check the file <code>names/data/names_data.hcl</code> for the service.</p> </li> <li> <p>If the service is there and the <code>not_implemented</code> attribute does not exist, you are ready to implement the first resource or data source.</p> </li> <li> <p>If the service is there and the <code>not_implemented</code> attribute is true, remove it and submit the client pull request as described below.</p> </li> <li> <p>Otherwise, determine the service identifier using the rule described in the Naming Guide.</p> </li> <li> <p>In <code>names/data/names_data.hcl</code>, add a new hcl block with all the requested information for the service following the guidance in the <code>names</code> README.</p> <p>Tip</p> <p>Be very careful when adding or changing data in <code>names_data.hcl</code>! The Provider and generators depend on the file being correct. We strongly recommend using an editor with HCL support.</p> </li> </ol> <p>Once the names data is ready, create a new service directory with the appropriate service name.</p> <pre><code>mkdir internal/service/&lt;service&gt;\n</code></pre> <p>Add a new file <code>internal/service/&lt;service&gt;/generate.go</code> with the following content. This will generate the structs required for resource self-registration.</p> <pre><code>// Copyright (c) HashiCorp, Inc.\n// SPDX-License-Identifier: MPL-2.0\n\n//go:generate go run ../../generate/servicepackage/main.go\n// ONLY generate directives and package declaration! Do not add anything else to this file.\n\npackage &lt;service&gt;\n</code></pre> <p>Next, generate the client and ensure all dependencies are fetched.</p> <pre><code>make gen\n</code></pre> <pre><code>go mod tidy\n</code></pre> <p>At this point a pull request with the re-generated files and new service client can be submitted.</p> <p>Once the service client has been added, implement the first resource or data source in a separate PR.</p>"},{"location":"add-a-new-service/#adding-a-custom-service-client","title":"Adding a Custom Service Client","text":"<p>If the service API's endpoint must be accessed via a single AWS Region, then:</p> <ol> <li>Add a <code>endpoint_region_overrides</code> map attribute to the <code>endpoint_info</code> for the service in <code>names/data/names_data.hcl</code></li> </ol> <pre><code>  endpoint_info {\nendpoint_api_call = ...\nendpoint_region_overrides = {\n\"aws\" = \"us-east-1\"\n}\n}\n</code></pre> <ol> <li>Run <code>make gen</code></li> </ol>"},{"location":"add-a-new-service/#customizing-a-new-service-client","title":"Customizing a new Service Client","text":"<p>If an AWS service must be customized after creation, for example, retry handling must be changed, then:</p> <ol> <li>Add a file <code>internal/&lt;service&gt;/service_package.go</code> that contains an API client customization function, for example:</li> </ol> <pre><code>package apigateway\n\nimport (\n\"context\"\n\n\"github.com/aws/aws-sdk-go-v2/aws\"\n\"github.com/aws/aws-sdk-go-v2/aws/retry\"\n\"github.com/aws/aws-sdk-go-v2/service/apigateway\"\n\"github.com/aws/aws-sdk-go-v2/service/apigateway/types\"\n\"github.com/hashicorp/terraform-provider-aws/internal/conns\"\n\"github.com/hashicorp/terraform-provider-aws/internal/errs\"\n)\n\nfunc (p *servicePackage) withExtraOptions(_ context.Context, config map[string]any) []func(*apigateway.Options) {\ncfg := *(config[\"aws_sdkv2_config\"].(*aws.Config))\n\nreturn []func(*apigateway.Options){\nfunc(o *apigateway.Options) {\no.Retryer = conns.AddIsErrorRetryables(cfg.Retryer().(aws.RetryerV2), retry.IsErrorRetryableFunc(func(err error) aws.Ternary {\n// Many operations can return an error such as:\n//   ConflictException: Unable to complete operation due to concurrent modification. Please try again later.\n// Handle them all globally for the service client.\nif errs.IsAErrorMessageContains[*types.ConflictException](err, \"try again later\") {\nreturn aws.TrueTernary\n}\nreturn aws.UnknownTernary // Delegate to configured Retryer.\n}))\n},\n}\n}\n</code></pre>"},{"location":"add-import-support/","title":"Adding Resource Import Support","text":"<p>Adding import support for Terraform resources will allow existing infrastructure to be managed within Terraform. This type of enhancement generally requires a small to moderate amount of code changes. Comprehensive code examples and information about resource import support can be found in the Terraform Plugin Framework documentation.</p> <ul> <li>Resource Code: In the resource code (e.g., <code>internal/service/{service}/{thing}.go</code>),<ul> <li>Plugin Framework (Preferred) Implement the <code>ImportState</code> method on the resource struct. When possible, prefer using the <code>resource.ImportStatePassthroughID</code> function.</li> <li>Plugin SDK V2: Implement an <code>Importer</code> <code>State</code> function. When possible, prefer using <code>schema.ImportStatePassthroughContext</code>.</li> </ul> </li> <li>Resource Acceptance Tests: In the resource acceptance tests (e.g., <code>internal/service/{service}/{thing}_test.go</code>), implement one or more tests containing a <code>TestStep</code> with <code>ImportState: true</code>.</li> <li>Resource Documentation: In the resource documentation (e.g., <code>website/docs/r/service_thing.html.markdown</code>), add an <code>Import</code> section at the bottom of the page.</li> </ul>"},{"location":"adding-a-tag-resource/","title":"Adding a New Tag Resource","text":"<p>Adding a tag resource, similar to the <code>aws_ecs_tag</code> resource, has its own implementation procedure since the resource code and initial acceptance testing functions are automatically generated. The rest of the resource acceptance testing and resource documentation must still be manually created.</p> <ul> <li>In <code>internal/generate</code>: Ensure the service is supported by all generators. Run <code>make gen</code> after any modifications.</li> <li>In <code>internal/service/{service}/generate.go</code>: Add the new <code>//go:generate</code> call with the correct generator directives. Run <code>make gen</code> after any modifications.</li> <li>In <code>internal/provider/provider.go</code>: Add the new resource.</li> <li>Run <code>make test</code> and ensure there are no failures.</li> <li>Create <code>internal/service/{service}/tag_gen_test.go</code> with initial acceptance testing similar to the following (where the parent resource is simple to provision):</li> </ul> <pre><code>import (\n\"fmt\"\n\"testing\"\n\n\"github.com/hashicorp/terraform-plugin-testing/helper/acctest\"\n\"github.com/hashicorp/terraform-plugin-testing/helper/resource\"\n\"github.com/hashicorp/terraform-provider-aws/names\"\n)\n\nfunc TestAcc{Service}Tag_basic(t *testing.T) {\nctx := acctest.Context(t)\nrName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\nresourceName := \"aws_{service}_tag.test\"\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck:                 func() { acctest.PreCheck(ctx, t) },\nErrorCheck:               acctest.ErrorCheck(t, names.{Service}ServiceID),\nProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories,\nCheckDestroy:             testAccCheck{Service}TagDestroy(ctx),\nSteps: []resource.TestStep{\n{\nConfig: testAcc{Service}TagConfig(rName, \"key1\", \"value1\"),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheck{Service}TagExists(ctx, resourceName),\nresource.TestCheckResourceAttr(resourceName, \"key\", \"key1\"),\nresource.TestCheckResourceAttr(resourceName, \"value\", \"value1\"),\n),\n},\n{\nResourceName:      resourceName,\nImportState:       true,\nImportStateVerify: true,\n},\n},\n})\n}\n\nfunc TestAcc{Service}Tag_disappears(t *testing.T) {\nctx := acctest.Context(t)\nrName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\nresourceName := \"aws_{service}_tag.test\"\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck:                 func() { acctest.PreCheck(ctx, t) },\nErrorCheck:               acctest.ErrorCheck(t, names.{Service}ServiceID),\nProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories,\nCheckDestroy:             testAccCheck{Service}TagDestroy(ctx),\nSteps: []resource.TestStep{\n{\nConfig: testAcc{Service}TagConfig(rName, \"key1\", \"value1\"),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheck{Service}TagExists(ctx, resourceName),\nacctest.CheckResourceDisappears(ctx, acctest.Provider, resourceAws{Service}Tag(), resourceName),\n),\nExpectNonEmptyPlan: true,\n},\n},\n})\n}\n\nfunc TestAcc{Service}Tag_Value(t *testing.T) {\nctx := acctest.Context(t)\nrName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\nresourceName := \"aws_{service}_tag.test\"\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck:                 func() { acctest.PreCheck(ctx, t) },\nErrorCheck:               acctest.ErrorCheck(t, names.{Service}ServiceID),\nProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories,\nCheckDestroy:             testAccCheck{Service}TagDestroy(ctx),\nSteps: []resource.TestStep{\n{\nConfig: testAcc{Service}TagConfig(rName, \"key1\", \"value1\"),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheck{Service}TagExists(ctx, resourceName),\nresource.TestCheckResourceAttr(resourceName, \"key\", \"key1\"),\nresource.TestCheckResourceAttr(resourceName, \"value\", \"value1\"),\n),\n},\n{\nResourceName:      resourceName,\nImportState:       true,\nImportStateVerify: true,\n},\n{\nConfig: testAcc{Service}TagConfig(rName, \"key1\", \"value1updated\"),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheck{Service}TagExists(ctx, resourceName),\nresource.TestCheckResourceAttr(resourceName, \"key\", \"key1\"),\nresource.TestCheckResourceAttr(resourceName, \"value\", \"value1updated\"),\n),\n},\n},\n})\n}\n\nfunc testAcc{Service}TagConfig(rName string, key string, value string) string {\nreturn fmt.Sprintf(`\nresource \"aws_{service}_{thing}\" \"test\" {\n  name = %[1]q\n\n  lifecycle {\n    ignore_changes = [tags]\n  }\n}\n\nresource \"aws_{service}_tag\" \"test\" {\n  resource_arn = aws_{service}_{thing}.test.arn\n  key          = %[2]q\n  value        = %[3]q\n}\n`, rName, key, value)\n}\n</code></pre> <ul> <li>Run <code>make testacc TESTS=TestAcc{Service}Tags_ PKG={Service}</code> and ensure there are no failures.</li> <li>Create <code>website/docs/r/{service}_tag.html.markdown</code> with initial documentation similar to the following:</li> </ul> <pre><code>---\nsubcategory: \"{SERVICE}\"\nlayout: \"aws\"\npage_title: \"AWS: aws_{service}_tag\"\ndescription: |-\n  Manages an individual {SERVICE} resource tag\n---\n\n# Resource: aws_{service}_tag\n\nManages an individual {SERVICE} resource tag. This resource should only be used in cases where {SERVICE} resources are created outside Terraform (e.g., {SERVICE} {THING}s implicitly created by {OTHER SERVICE THING}).\n\n~&gt; **NOTE:** This tagging resource should not be combined with the Terraform resource for managing the parent resource. For example, using `aws_{service}_{thing}` and `aws_{service}_tag` to manage tags of the same {SERVICE} {THING} will cause a perpetual difference where the `aws_{service}_{thing}` resource will try to remove the tag being added by the `aws_{service}_tag` resource.\n\n~&gt; **NOTE:** This tagging resource does not use the [provider `ignore_tags` configuration](/docs/providers/aws/index.html#ignore_tags).\n\n## Example Usage\n\n```terraform\nresource \"aws_{service}_tag\" \"example\" {\nresource_arn = \"...\"\nkey          = \"Name\"\nvalue        = \"Hello World\"\n}\n```\n\n## Argument Reference\n\nThis resource supports the following arguments:\n\n* `resource_arn` - (Required) ARN of the {SERVICE} resource to tag.\n* `key` - (Required) Tag name.\n* `value` - (Required) Tag value.\n\n## Attribute Reference\n\nThis resource exports the following attributes in addition to the arguments above:\n\n* `id` - {SERVICE} resource identifier and key, separated by a comma (`,`)\n\n## Import\n\nImport `aws_{service}_tag` using the {SERVICE} resource identifier and key, separated by a comma (`,`). For example:\n\n```console\n$ terraform import aws_{service}_tag.example arn:aws:{service}:us-east-1:123456789012:{thing}/example,Name\n```\n</code></pre>"},{"location":"ai-agents/","title":"AI Agents","text":"<p>The <code>AI Agent Guides</code> section on the navbar serves as an index of documents which can be provided to AI agents as context to solve specific tasks. The following points should be taken into consideration when developing new guides.</p> <ul> <li>Use generalized instructions and remain tool agnostic.</li> <li>Limit scope to a single task per document.</li> <li>Structure documents so they are useful to both AI agents and human readers.</li> </ul>"},{"location":"aws-sdk-go-base/","title":"AWS SDK Go Base","text":"<p>AWS SDK Go Base is a shared library used by the AWS Provider, AWSCC Provider and the Terraform S3 Backend to handle authentication and other non-service level AWS interactions consistently.</p> <p>Changes are infrequent and normally performed by HashiCorp maintainers. It should not be necessary to change this library for the majority of provider contributions.</p>"},{"location":"bugs-and-enhancements/","title":"Making Small Changes to Existing Resources","text":"<p>Most contributions to the provider will take the form of small additions or bug-fixes on existing resources/data sources. In this case the existing resource will give you the best guidance on how the change should be structured, but we require the following to allow the change to be merged:</p> <ul> <li>Acceptance test coverage of new behavior: Existing resources each    have a set of acceptance tests covering their functionality.    These tests should exercise all the behavior of the resource. Whether you are    adding something or fixing a bug, the idea is to have an acceptance test that    fails if your code were to be removed. Sometimes it is sufficient to    \"enhance\" an existing test by adding an assertion or tweaking the config    that is used, but it's often better to add a new test. You can copy/paste an    existing test and follow the conventions you see there, modifying the test    to exercise the behavior of your code.</li> <li>Documentation updates: If your code makes any changes that need to    be documented, you should include those documentation changes    in the same PR. This includes things like new resource attributes or changes in default values.</li> <li>Well-formed Code: Do your best to follow existing conventions you    see in the codebase, and ensure your code is formatted with <code>go fmt</code>.    The PR reviewers can help out on this front, and may provide comments with    suggestions on how to improve the code.</li> <li>Dependency updates: Create a separate PR if you are updating dependencies.    This is to avoid conflicts as version updates tend to be fast-    moving targets. We will plan to merge the PR with this change first.</li> <li>Changelog entry: Assuming the code change affects Terraform operators,    the relevant PR ought to include a user-facing changelog entry    describing the new behavior.</li> </ul>"},{"location":"changelog-process/","title":"Changelog Process","text":"<p>HashiCorp\u2019s open-source projects have always maintained user-friendly, readable <code>CHANGELOG.md</code> that allows users to tell at a glance whether a release should have any effect on them, and to gauge the risk of an upgrade.</p> <p>We use go-changelog to generate the changelog from files created in the <code>.changelog/</code> directory. It is important that when you raise your pull request, there is a changelog entry which describes the changes your contribution makes. Not all changes require an entry in the changelog, guidance follows on what changes do.</p>"},{"location":"changelog-process/#changelog-format","title":"Changelog format","text":"<p>The changelog format requires an entry in the following format, where HEADER corresponds to the changelog category, and the entry is the changelog entry itself. The entry should be included in a file in the <code>.changelog</code> directory with the naming convention <code>{PR-NUMBER}.txt</code>. For example, to create a changelog entry for pull request 1234, there should be a file named <code>.changelog/1234.txt</code>.</p> <pre><code>```release-note:{HEADER}\n{ENTRY}\n```\n</code></pre> <p>If a pull request should contain multiple changelog entries, then multiple blocks can be added to the same changelog file. For example:</p> <pre><code>```release-note:note\nresource/aws_example_thing: The `broken` attribute has been deprecated. All configurations using `broken` should be updated to use the new `not_broken` attribute instead.\n```\n\n```release-note:enhancement\nresource/aws_example_thing: Add `not_broken` attribute\n```\n</code></pre>"},{"location":"changelog-process/#pull-request-types-to-changelog","title":"Pull request types to CHANGELOG","text":"<p>The CHANGELOG is intended to show operator-impacting changes to the codebase for a particular version. If every change or commit to the code resulted in an entry, the CHANGELOG would become less useful for operators. The lists below are general guidelines and examples for when a decision needs to be made to decide whether a change should have an entry.</p>"},{"location":"changelog-process/#changes-that-should-have-a-changelog-entry","title":"Changes that should have a CHANGELOG entry","text":""},{"location":"changelog-process/#new-resource","title":"New resource","text":"<p>A new resource entry should only contain the name of the resource, and use the <code>release-note:new-resource</code> header.</p> <pre><code>```release-note:new-resource\naws_secretsmanager_secret_policy\n```\n</code></pre>"},{"location":"changelog-process/#new-data-source","title":"New data source","text":"<p>A new data source entry should only contain the name of the data source, and use the <code>release-note:new-data-source</code> header.</p> <pre><code>```release-note:new-data-source\naws_workspaces_workspace\n```\n</code></pre>"},{"location":"changelog-process/#new-full-length-documentation-guides-eg-eks-getting-started-guide-iam-policy-documents-with-terraform","title":"New full-length documentation guides (e.g., EKS Getting Started Guide, IAM Policy Documents with Terraform)","text":"<p>A new full-length documentation entry gives the title of the documentation added, using the <code>release-note:new-guide</code> header.</p> <pre><code>```release-note:new-guide\nCustom Service Endpoint Configuration\n```\n</code></pre>"},{"location":"changelog-process/#resource-and-provider-bug-fixes","title":"Resource and provider bug fixes","text":"<p>A new bug entry should use the <code>release-note:bug</code> header and have a prefix indicating the resource or data source it corresponds to, a colon, then followed by a brief summary. Use a <code>provider</code> prefix for provider-level fixes.</p> <pre><code>```release-note:bug\nresource/aws_glue_classifier: Fix quote_symbol being optional\n```\n</code></pre>"},{"location":"changelog-process/#resource-and-provider-enhancements","title":"Resource and provider enhancements","text":"<p>A new enhancement entry should use the <code>release-note:enhancement</code> header and have a prefix indicating the resource or data source it corresponds to, a colon, then followed by a brief summary. Use a <code>provider</code> prefix for provider-level enhancements.</p> <pre><code>```release-note:enhancement\nresource/aws_eip: Add network_border_group argument\n```\n</code></pre>"},{"location":"changelog-process/#deprecations","title":"Deprecations","text":"<p>A deprecation entry should use the <code>release-note:note</code> header and have a prefix indicating the resource or data source it corresponds to, a colon, then followed by a brief summary. Use a <code>provider</code> prefix for provider-level changes.</p> <pre><code>```release-note:note\nresource/aws_dx_gateway_association: The vpn_gateway_id attribute is being deprecated in favor of the new associated_gateway_id attribute to support transit gateway associations\n```\n</code></pre>"},{"location":"changelog-process/#breaking-changes-and-removals","title":"Breaking changes and removals","text":"<p>A breaking-change entry should use the <code>release-note:breaking-change</code> header and have a prefix indicating the resource or data source it corresponds to, a colon, then followed by a brief summary. Use a <code>provider</code> prefix for provider-level changes.</p> <pre><code>```release-note:breaking-change\nresource/aws_lambda_alias: Resource import no longer converts Lambda Function name to ARN\n```\n</code></pre>"},{"location":"changelog-process/#region-validation-support","title":"Region validation support","text":"<pre><code>```release-note:note\nprovider: Region validation now automatically supports the new `XX-XXXXX-#` (Location) region. For AWS operations to work in the new region, the region must be explicitly enabled as outlined in the [AWS Documentation](https://docs.aws.amazon.com/general/latest/gr/rande-manage.html#rande-manage-enable). When the region is not enabled, the Terraform AWS Provider will return errors during credential validation (e.g., `error validating provider credentials: error calling sts:GetCallerIdentity: InvalidClientTokenId: The security token included in the request is invalid`) or AWS operations will throw their own errors (e.g., `data.aws_availability_zones.available: Error fetching Availability Zones: AuthFailure: AWS was not able to validate the provided access credentials`). [GH-####]\n```\n\n```release-note:enhancement\nprovider: Support automatic region validation for `XX-XXXXX-#` [GH-####]\n```\n</code></pre>"},{"location":"changelog-process/#changes-that-may-have-a-changelog-entry","title":"Changes that may have a CHANGELOG entry","text":"<p>Dependency updates: If the update contains relevant bug fixes or enhancements that affect operators, those should be called out. Any changes which do not fit into the above categories but warrant highlighting. Use resource/data source/provider prefixes where appropriate.</p> <pre><code>```release-note:note\nresource/aws_lambda_alias: Resource import no longer converts Lambda Function name to ARN\n```\n</code></pre>"},{"location":"changelog-process/#changes-that-should-not-have-a-changelog-entry","title":"Changes that should not have a CHANGELOG entry","text":"<ul> <li>Resource and provider documentation updates</li> <li>Testing updates</li> <li>Code refactoring</li> </ul>"},{"location":"continuous-integration/","title":"Continuous Integration","text":"<p>Continuous integration (CI) includes processes that run when you submit a pull request (PR). These processes can be divided into two broad categories: enrichment and testing.</p>"},{"location":"continuous-integration/#ci-enrichment","title":"CI: Enrichment","text":"<p>The focus of this guide is on CI testing, but for completeness, we also mention enrichment processes. These include generating the changelog when a PR is merged, releasing new versions, labeling a PR based on its size and the AWS service it relates to, and adding automated comments to the PR.</p> <p>These processes will not usually produce the dreaded red X signifying that a PR has failed CI. For this reason, we shift our focus to CI testing.</p>"},{"location":"continuous-integration/#ci-testing-overview","title":"CI: Testing Overview","text":"<p>To help place testing performed as part of CI in context, here is an overview of the Terraform AWS Provider's three types of tests.</p> <ol> <li>Acceptance tests are end-to-end evaluations of interactions with AWS. They validate functionalities like creating, reading, and destroying resources within AWS.</li> <li>Unit tests focus on testing isolated units of code within the software, typically at the function level. They assess functionalities solely within the provider itself.</li> <li>Continuous integration tests (You are here!) encompass a suite of automated tests that are executed on every pull request and include linting, compiling code, running unit tests, and performing static analysis.</li> </ol>"},{"location":"continuous-integration/#rationale","title":"Rationale","text":"<p>Continuous integration (CI) plays a pivotal role in maintaining the health and quality of a large project like the Terraform AWS Provider. CI tests are crucial for automatically assessing code changes for compliance with project standards and functionality expectations, greatly reducing the review burden on maintainers. By executing a battery of tests upon each pull request submission, CI ensures that new contributions integrate seamlessly with the existing codebase, minimizing the risk of regressions and enhancing overall stability.</p> <p>Additionally, these tests provide rapid feedback to contributors, enabling them to identify and rectify issues early in the development cycle. In essence, CI tests serve as a safeguard, bolstering the reliability and maintainability of the project while fostering a collaborative and iterative development environment.</p>"},{"location":"continuous-integration/#using-make-to-run-specific-tests-locally","title":"Using <code>make</code> to Run Specific Tests Locally","text":"<p>NOTE: We've made a great effort to ensure that tests running on GitHub have a close-as-possible equivalent in the Makefile. If you notice a difference, please open an issue to let us know.</p> <p>The Makefile included with the Terraform AWS Provider allows you to run many of the CI tests locally before submitting your PR. The file is located in the provider's root directory and is called <code>GNUmakefile</code>. You should be able to use <code>make</code> with a variety of Linux-type shells that support <code>bash</code>, such as a macOS terminal.</p> <p>NOTE: See the Makefile Cheat Sheet for detailed information about the Makefile.</p> <p>There are many different tests, and they change often. This guide doesn't cover everything CI does because, as noted above, many of the CI processes enrich the pull request, such as adding labels. If you notice something important that isn't reflected in this documentation, let us know!</p> <p>NOTE: Many tests simply exit without error if passing. \"No news is good news.\"</p>"},{"location":"continuous-integration/#before-running-tests","title":"Before Running Tests","text":"<p>CI tests run on GitHub when you submit a pull request. However, these tests can take a while to complete. If you prefer, you can run most tests locally. Before running tests locally, you need to clone the repository, which you've likely already done if you're working on a PR, and install the necessary tools.</p> <p>Use the <code>tools</code> target to install a variety of tools used by different CI tests:</p> <pre><code>make tools\n</code></pre>"},{"location":"continuous-integration/#running-all-available-ci-tests","title":"Running All Available CI Tests","text":"<p>Use the <code>ci</code> target to run all the tests listed below:</p> <pre><code>make ci\n</code></pre> <p>NOTE: Depending on your machine, running all the tests can take a long time!</p> <p>To run most of the tests but exclude the longer-running ones, use the <code>ci-quick</code> target. \"Quick\" may not be quick precisely, but relative to the full <code>ci</code> target, it is quicker:</p> <pre><code>make ci-quick\n</code></pre> <p>Use the <code>clean-make-tests</code> target to clean up artifacts left behind by <code>make</code> tests, although they should be ignored by Git:</p> <pre><code>make clean-make-tests\n</code></pre>"},{"location":"continuous-integration/#acceptance-test-linting","title":"Acceptance Test Linting","text":"<p>Acceptance test linting involves thoroughly testing the Terraform configuration associated with acceptance tests. Currently, this process extracts configuration embedded as strings in Go files. However, as we move testing configurations to <code>.tf</code> files, linting will involve testing those files for correctness.</p> <p>Acceptance test linting has two components: <code>terrafmt</code> and <code>tflint</code>. The <code>make</code> tool provides several targets to help with this.</p>"},{"location":"continuous-integration/#running-all-acceptance-test-linting-checks","title":"Running All Acceptance Test Linting Checks","text":"<p>Use the <code>acctest-lint</code> target to run all the acceptance test linting checks using both <code>terrafmt</code> and <code>tflint</code>:</p> <pre><code>make acctest-lint\n</code></pre>"},{"location":"continuous-integration/#limiting-linting-to-a-specific-service-package","title":"Limiting Linting to a Specific Service Package","text":"<p>You can limit the test to a service package by using the <code>PKG</code> environment variable:</p> <pre><code>PKG=rds make acctest-lint\n</code></pre> <p>The command above is equivalent to using <code>SVC_DIR</code> with the full relative path:</p> <pre><code>SVC_DIR=./internal/service/rds make acctest-lint\n</code></pre>"},{"location":"continuous-integration/#terrafmt","title":"<code>terrafmt</code>","text":"<p>Use the <code>testacc-lint</code> target to run only the <code>terrafmt</code> test. This is useful if you want to skip <code>tflint</code>, which takes a long time to run:</p> <pre><code>make testacc-lint\n</code></pre> <p>Use the <code>testacc-lint-fix</code> target to automatically fix issues found by <code>terrafmt</code>:</p> <pre><code>make testacc-lint-fix\n</code></pre>"},{"location":"continuous-integration/#validate-acceptance-tests-with-tflint","title":"Validate Acceptance Tests with <code>tflint</code>","text":"<p>Use the <code>testacc-tflint</code> target to run only the <code>tflint</code> test. This is useful if you want to skip <code>terrafmt</code>:</p> <pre><code>make testacc-tflint\n</code></pre> <p>To run <code>tflint</code> only against acceptance test configurations in <code>.tf</code> files, use the <code>testacc-tflint-dir</code> target:</p> <pre><code>make testacc-tflint-dir\n</code></pre> <p>To run <code>tflint</code> only against embedded configurations, use the <code>testacc-tflint-embedded</code> target:</p> <pre><code>make testacc-tflint-embedded\n</code></pre>"},{"location":"continuous-integration/#copyright-checks-add-headers-check","title":"Copyright Checks / add headers check","text":"<p>This CI check simply checks to make sure after running the tool, no files have been modified. No modifications signifies that everything already has the proper header.</p> <p>Use the <code>copyright</code> target to add the appropriate copyright headers to all files:</p> <pre><code>make copyright\n</code></pre> <p>NOTE: Install tools before running this check.</p>"},{"location":"continuous-integration/#dependency-checks-go_mod","title":"Dependency Checks / go_mod","text":"<p>Dependency checks include a variety of tests including who should edit certain types of files. The test that generally trips people up is <code>go_mod</code>.</p> <p>Use the <code>deps-check</code> target to make sure that the Go mods files are tidy. This will also install the version of Go defined in the <code>.go-version</code> file in the root of the repository.</p> <pre><code>make deps-check\n</code></pre>"},{"location":"continuous-integration/#documentation-checks","title":"Documentation Checks","text":"<p>\"Documentation\" is the context of these checks is the documentation found in the <code>docs/</code> directory of the provider. This include contributor and related guides. This is developer-facing unlike the Examples and Website checks.</p>"},{"location":"continuous-integration/#markdown-link-check","title":"markdown-link-check","text":"<p>Use the target <code>docs-link-check</code> to check links found in the contributor documentation:</p> <pre><code>make docs-link-check\n</code></pre> <p>NOTE: Install Docker to run this check.</p>"},{"location":"continuous-integration/#markdown-lint","title":"markdown-lint","text":"<p>Use the target <code>docs-markdown-lint</code> to lint the contributor documentation:</p> <pre><code>make docs-markdown-lint\n</code></pre> <p>NOTE: Install Docker to run this check.</p>"},{"location":"continuous-integration/#misspell","title":"misspell","text":"<p>Use the target <code>docs-misspell</code> to spellcheck the contributor documentation:</p> <pre><code>make docs-misspell\n</code></pre> <p>NOTE: Install tools before running this check.</p>"},{"location":"continuous-integration/#examples-checks","title":"Examples Checks","text":"<p>These checks help ensure that examples included with the provider are correct.</p>"},{"location":"continuous-integration/#tflint","title":"tflint","text":"<p>Use the target <code>examples-tflint</code> to lint the examples:</p> <pre><code>make examples-tflint\n</code></pre> <p>NOTE: Install tools before running this check.</p>"},{"location":"continuous-integration/#validate-terraform-01231","title":"validate-terraform (0.12.31)","text":"<p>This check is not currently available in the Makefile.</p>"},{"location":"continuous-integration/#validate-terraform-106","title":"validate-terraform (1.0.6)","text":"<p>This check is not currently available in the Makefile.</p>"},{"location":"continuous-integration/#golangci-lint-checks","title":"golangci-lint Checks","text":"<p>golangci-lint checks runs a variety of linters on the provider's code. This is done in two stages with the first stage acting as a gatekeeper since subsequent stages takes considerably longer to run.</p> <p>Before running these checks locally, you need to install golangci-lint locally. This can be done in several ways including using Homebrew on macOS:</p> <pre><code>brew install golangci-lint\n</code></pre> <p>Use the target <code>golangci-lint</code> to run all checks sequentially:</p> <pre><code>make golangci-lint\n</code></pre> <p>You can limit the checks to a specific service package. For example:</p> <pre><code>PKG=rds make golangci-lint\n</code></pre>"},{"location":"continuous-integration/#1-of-5","title":"1 of 5","text":"<p>Use the <code>golangci-lint1</code> target to run only the first step of these checks:</p> <pre><code>make golangci-lint1\n</code></pre>"},{"location":"continuous-integration/#2-through-5-of-5","title":"2 through 5 of 5","text":"<p>Use the <code>golangci-lint2</code>, <code>golangci-lint3</code>, <code>golangci-lint4</code>, or <code>golangci-lint5</code> targets to run subsequent steps of these checks:</p> <pre><code>make golangci-lint2\n</code></pre> <p>Tip: Running the second step against the entire codebase often takes the longest of all CI tests. If you're only working in one service package, you can save a lot of time limiting the scan to that service:</p> <pre><code>PKG=rds make golangci-lint2\n</code></pre>"},{"location":"continuous-integration/#goreleaser-ci-build-32-bit","title":"GoReleaser CI / build-32-bit","text":"<p>GoReleaser CI build-32-bit ensures that GoReleaser can build a 32-bit binary. This check catches rare but important edge cases. Currently, we do not offer a <code>make</code> target to run this check locally.</p>"},{"location":"continuous-integration/#modern-go-check","title":"Modern Go Check","text":"<p>This check ensures that code uses current idiomatic Go. Currently, the check is only run on a subset of services. To determine which services must have modern Go, check the <code>.github/workflows/modern_go.yml</code> file.</p>"},{"location":"continuous-integration/#pr-target-check","title":"PR Target Check","text":"<p>This check ensures that the <code>pull_request_target</code> event is only used in approved workflows. Unlike <code>pull_request</code>, which runs workflows against the pull request\u2019s changes, <code>pull_request_target</code> runs against the base branch. This can cause issues to go undetected if the workflow is intended to validate the pull request itself. Restricting its use helps ensure that CI checks reflect the actual content of proposed changes.</p>"},{"location":"continuous-integration/#provider-checks","title":"Provider Checks","text":"<p>Provider checks are a suite of tests that ensure Go code functions and markdown is correct.</p>"},{"location":"continuous-integration/#go-build","title":"go-build","text":"<p>This check determines if the code compiles correctly, there are syntax errors, or there are any unresolved references.</p> <p>There are two ways to run this check that are basically equivalent.</p> <p>Use the <code>go-build</code> target to build the provider using <code>go build</code>, installing the provider in the <code>terraform-plugin-dir</code> directory:</p> <pre><code>make go-build\n</code></pre> <p>Similarly, use the <code>build</code> target to install the provider binary locally using <code>go install</code>:</p> <pre><code>make build\n</code></pre>"},{"location":"continuous-integration/#go_generate","title":"go_generate","text":"<p><code>go_generate</code> checks to make sure nothing changes after you run the code generators. In other words, generated code and committed code should be in sync or we'll say, \"bye bye bye.\"</p> <p>Use the <code>gen-check</code> target to run the check:</p> <pre><code>make gen-check\n</code></pre> <p>Use the <code>gen</code> target to run all the generators associated with the provider. Unless you're working on the generators or have inadvertently edited generated code, there should be no changes to the codebase after the generators finish:</p> <pre><code>make gen\n</code></pre> <p>NOTE: While running the generators, you may see hundreds or thousands of code changes as <code>make</code> and the generators delete and recreate files.</p>"},{"location":"continuous-integration/#go_test","title":"go_test","text":"<p><code>go_test</code> compiles the code and runs all tests except the acceptance tests. This check may also find higher level code errors than building alone finds.</p> <p>Use the <code>test</code> target to run this test:</p> <pre><code>make test\n</code></pre> <p>You can limit <code>test</code> to a single service package with the <code>PKG</code> environment variable:</p> <pre><code>PKG=rds make test\n</code></pre> <p>NOTE: <code>test</code> and <code>golangci-lint2</code> are generally the longest running checks and, depending on your computer, may take considerable time to finish.</p>"},{"location":"continuous-integration/#import-lint","title":"import-lint","text":"<p><code>import-lint</code> uses impi to make sure that imports in Go files follow the order of standard, third party, and then local. Besides neatness, enforcing the order helps avoid meaningless Git differences. In earlier days of Go, it was possible to order imports more freely. This check may no longer be needed but we need additional verification.</p> <p>To run this check locally, you will need to install <code>impi</code>, which is done as part of <code>make tools</code>.</p> <p>Use the <code>import-lint</code> target to run <code>impi</code> with the appropriate parameters:</p> <pre><code>make import-lint\n</code></pre>"},{"location":"continuous-integration/#markdown-lint_1","title":"markdown-lint","text":"<p><code>markdown-lint</code> can be a little confusing since it shows up in CI in three different contexts, each performing slightly different checks:</p> <ol> <li>Provider Check / markdown-lint (this check)</li> <li>Documentation Checks / markdown-lint</li> <li>Website Checks / markdown-lint</li> </ol> <p>This particular check uses markdownlint to check all Markdown files in the provider except those in <code>docs</code> and <code>website/docs</code>, the CHANGELOG, and an example.</p> <p>Use the <code>provider-markdown-lint</code> target to run this test:</p> <pre><code>make provider-markdown-lint\n</code></pre> <p>NOTE: Install Docker to run this check.</p>"},{"location":"continuous-integration/#misspell_1","title":"misspell","text":"<p>Use <code>go-misspell</code> to check the provider code for misspellings:</p> <pre><code>make go-misspell\n</code></pre> <p>NOTE: Install tools before running this check.</p>"},{"location":"continuous-integration/#terraform-providers-schema","title":"terraform providers schema","text":"<p>This process generates the Terraform AWS Provider schema for use by the <code>tfproviderdocs</code> check. In the <code>make</code> file, this is done as part of the <code>tfproviderdocs</code> target test.</p>"},{"location":"continuous-integration/#tfproviderdocs","title":"tfproviderdocs","text":"<p>NOTE: To run this test, you need Terraform installed locally. On macOS, you can use Homebrew to install Terraform:</p> <pre><code>brew install terraform\n</code></pre> <p>This test builds the provider binary, loads the provider with Terraform, generates the provider schema, and then uses the tfproviderdocs tool to ensure the provider (via the schema) and documentation are consistent with each other.</p> <p>Use the <code>tfproviderdocs</code> target to run this test:</p> <pre><code>make tfproviderdocs\n</code></pre>"},{"location":"continuous-integration/#sweeper-functions-not-linked","title":"Sweeper Functions Not Linked","text":"<p>This check builds the Terraform AWS Provider in two different configurations, with sweepers and without, to make sure sweepers are properly included or excluded from the builds. The normal build you would receive from the Terraform Registry does not include sweepers and this ensures they aren't accidentally included.</p> <p>Use the <code>sweeper-check</code> target to run both tests:</p> <pre><code>make sweeper-check\n</code></pre> <p>You can also run the checks separately.</p> <p>Use the <code>sweeper-linked</code> target to ensure sweeper are included in a sweeper build:</p> <pre><code>make sweeper-linked\n</code></pre> <p>Use the <code>sweeper-unlinked</code> target to ensure sweeper are not included in a normal build:</p> <pre><code>make sweeper-unlinked\n</code></pre>"},{"location":"continuous-integration/#providerlint-checks-providerlint","title":"ProviderLint Checks / providerlint","text":"<p>ProviderLint checks for a variety of best practices. For more details on specific checks and errors, see providerlint.</p> <p>Use the <code>provider-lint</code> target to run the check just as it runs in CI:</p> <pre><code>make provider-lint\n</code></pre>"},{"location":"continuous-integration/#semgrep-checks","title":"Semgrep Checks","text":"<p>We use Semgrep for many types of checks and cannot describe all of them here. They are broken into rough groupings for parallel CI processing, as described below.</p> <p>To locally run Semgrep checks using <code>make</code>, you'll need to install Semgrep locally. On macOS, you can do this easily using Homebrew:</p> <pre><code>brew install semgrep\n</code></pre>"},{"location":"continuous-integration/#code-quality-scan","title":"Code Quality Scan","text":"<p>This scan looks for a hodgepodge of issues, best practices, and problems we've found over the years.</p> <p>Use the <code>semgrep-code-quality</code> target to run the same check CI runs:</p> <pre><code>make semgrep-code-quality\n</code></pre> <p>You can limit the scan to a service package by using the <code>PKG</code> environment variable:</p> <pre><code>PKG=rds make semgrep-code-quality\n</code></pre>"},{"location":"continuous-integration/#naming-scan-capsawsec2","title":"Naming Scan Caps/AWS/EC2","text":"<p>Idiomatic Go uses mixed caps for multiword names, not camel case. In camel case, a name with the words \"SMTP thing\" would be <code>SmtpThing</code>. This is wrong in Go. In mixed caps, and therefore idiomatic Go, <code>SMTPThing</code> is correct. This scan ensures that many acronyms and initialisms are capitalized correctly in code.</p> <p>Use the <code>semgrep-naming-cae</code> target to run the same check CI runs:</p> <pre><code>make semgrep-naming-cae\n</code></pre> <p>You can limit the scan to a service package by using the <code>PKG</code> environment variable:</p> <pre><code>PKG=rds make semgrep-naming-cae\n</code></pre>"},{"location":"continuous-integration/#service-name-scan-a-z","title":"Service Name Scan A-Z","text":"<p>This scan ensures that AWS service names are used fairly consistently from one service package to the next.</p> <p>Use the <code>semgrep-service-naming</code> target to run the same check CI runs:</p> <pre><code>make semgrep-service-naming\n</code></pre> <p>You can limit the scan to a service package by using the <code>PKG</code> environment variable:</p> <pre><code>PKG=rds make semgrep-service-naming\n</code></pre>"},{"location":"continuous-integration/#test-configs-scan","title":"Test Configs Scan","text":"<p>This scan checks for consistency in naming of test-related functions.</p> <p>Use the <code>semgrep-naming</code> target to run the same check CI runs:</p> <pre><code>make semgrep-naming\n</code></pre> <p>You can limit the scan to a service package by using the <code>PKG</code> environment variable:</p> <pre><code>PKG=rds make semgrep-naming\n</code></pre>"},{"location":"continuous-integration/#skaff-checks-compile-skaff","title":"Skaff Checks / Compile skaff","text":"<p>Use the <code>skaff-check-compile</code> target to test building Skaff:</p> <pre><code>make skaff-check-compile\n</code></pre>"},{"location":"continuous-integration/#website-checks","title":"Website Checks","text":"<p>These checks help ensure that user-facing documentation on the website is correct.</p>"},{"location":"continuous-integration/#markdown-link-check-a-h-markdown","title":"markdown-link-check-a-h-markdown","text":"<p>Use the target <code>website-link-check-markdown</code> to check links found in the website:</p> <pre><code>make website-link-check-markdown\n</code></pre> <p>NOTE: Install Docker to run this check.</p>"},{"location":"continuous-integration/#markdown-link-check-i-z-markdown","title":"markdown-link-check-i-z-markdown","text":"<p>This range is also checked as part of the \"a-h\" check above.</p>"},{"location":"continuous-integration/#markdown-link-check-md","title":"markdown-link-check-md","text":"<p>Use the target <code>website-link-check-md</code> to check links found in the website:</p> <pre><code>make website-link-check-md\n</code></pre> <p>NOTE: Install Docker to run this check.</p>"},{"location":"continuous-integration/#markdown-lint_2","title":"markdown-lint","text":"<p>Use the target <code>website-markdown-lint</code> to lint the website documentation:</p> <pre><code>make website-markdown-lint\n</code></pre> <p>NOTE: Install Docker to run this check.</p>"},{"location":"continuous-integration/#misspell_2","title":"misspell","text":"<p>Use the target <code>website-misspell</code> to spellcheck the documentation:</p> <pre><code>make website-misspell\n</code></pre> <p>NOTE: Install tools before running this check.</p>"},{"location":"continuous-integration/#terrafmt_1","title":"terrafmt","text":"<p>Use the target <code>website-terrafmt</code> to check formatting of Terraform configuration in documentation:</p> <pre><code>make website-terrafmt\n</code></pre> <p>NOTE: Install tools before running this check.</p>"},{"location":"continuous-integration/#tflint_1","title":"tflint","text":"<p>Use the target <code>website-tflint</code> to check formatting of Terraform configuration in documentation:</p> <pre><code>make website-tflint\n</code></pre> <p>NOTE: Install tools before running this check.</p>"},{"location":"continuous-integration/#workflow-linting-actionlint","title":"Workflow Linting / actionlint","text":"<p>Use the <code>gh-workflow-lint</code> target to perform the check:</p> <pre><code>make gh-workflow-lint\n</code></pre> <p>NOTE: Install tools before running this check.</p>"},{"location":"continuous-integration/#yaml-linting-yamllint","title":"YAML Linting / yamllint","text":"<p>YAMLlint checks the validity of YAML files.</p> <p>To run YAMLlint locally using <code>make</code>, you'll need to install it locally. On macOS, you can install it using Homebrew:</p> <pre><code>brew install yamllint\n</code></pre> <p>Use the <code>yamllint</code> target to perform the check:</p> <pre><code>make yamllint\n</code></pre>"},{"location":"core-services/","title":"Terraform AWS Provider Core Services","text":"<p>Core Services are AWS services we have identified as critical for a large majority of our users. Our goal is to continually increase the depth of coverage for these services. We will work to prioritize features and enhancements to these services in each weekly release, even if they are not necessarily highlighted in our quarterly roadmap.</p> <p>The core services we have identified are:</p> <ul> <li> <p>EC2</p> </li> <li> <p>Lambda</p> </li> <li> <p>EKS</p> </li> <li> <p>ECS</p> </li> <li> <p>VPC</p> </li> <li> <p>S3</p> </li> <li> <p>RDS</p> </li> <li> <p>DynamoDB</p> </li> <li> <p>IAM</p> </li> <li> <p>Autoscaling (ASG)</p> </li> <li> <p>ElastiCache</p> </li> </ul> <p>We'll continue to evaluate the selected services as our user base grows and changes.</p>"},{"location":"data-handling-and-conversion/","title":"Data Handling and Conversion","text":""},{"location":"data-handling-and-conversion/#data-handling-and-conversion","title":"Data Handling and Conversion","text":"<p>The Terraform AWS Provider codebase bridges the implementation of a Terraform Plugin and an AWS API client to support AWS operations and data types as Terraform Resources. Data handling and conversion is a large portion of resource implementation given the domain-specific implementations of each side of the provider. The first is where Terraform is a generic infrastructure as code tool with a generic data model and the other is where the details are driven by AWS API data modeling concepts. This guide is intended to explain and show preferred Terraform AWS Provider code implementations required to successfully translate data between these two systems.</p> <p>At the bottom of this documentation is a Glossary section, which may be a helpful reference while reading the other sections.</p>"},{"location":"data-handling-and-conversion/#data-conversions-in-terraform-providers","title":"Data Conversions in Terraform Providers","text":"<p>Before getting into highly specific documentation about the Terraform AWS Provider handling of data, it may be helpful to briefly highlight how Terraform Plugins (Terraform Providers in this case) interact with Terraform CLI and the Terraform State in general and where this documentation fits into the whole process.</p> <p>There are two primary data flows that are typically handled by resources within a Terraform Provider. Data is either converted from a planned new Terraform State into making a remote system request, referred to as \"Expanding\", or a remote system response is converted into an applied new Terraform State, referred to as \"Flattening\". The semantics of how the data of the planned new Terraform State is surfaced to the resource implementation is determined by where a resource is in its lifecycle and is mainly handled by Terraform CLI. This concept can be explored further in the Terraform Resource Instance Change Lifecycle documentation, with the caveat that some additional behaviors occur within the Terraform Plugin SDK as well (if the Terraform Plugin uses that implementation detail).</p> <p>As a generic walkthrough, the following data handling occurs when creating a Terraform Resource:</p> <ul> <li>An operator creates a Terraform configuration with a new resource defined and runs <code>terraform apply</code></li> <li>Terraform CLI merges an empty prior state for the resource, along with the given configuration state, to create a planned new state for the resource</li> <li>Terraform CLI sends a Terraform Plugin Protocol request to create the new resource with its planned new state data</li> <li>If the Terraform Plugin is using a higher-level library, such as the Terraform Plugin Framework, that library receives the request and translates the Terraform Plugin Protocol data types into the expected library types</li> <li>Terraform Plugin invokes the resource creation function with the planned new state data<ul> <li>The planned new state data is converted into a remote system request (e.g., API creation request) that is invoked</li> <li>The remote system response is received and the data is converted into an applied new state</li> </ul> </li> <li>If the Terraform Plugin is using a higher-level library, such as the Terraform Plugin Framework, that library translates the library types back into Terraform Plugin Protocol data types</li> <li>Terraform Plugin responds to Terraform Plugin Protocol request with the new state data</li> <li>Terraform CLI verifies and stores the new state</li> </ul> <p>The lines in bold above are the focus of this page.</p>"},{"location":"data-handling-and-conversion/#implicit-state-passthrough","title":"Implicit State Passthrough","text":"<p>An important behavior to note with Terraform State handling is if the value of a particular root attribute or block is not refreshed during plan or apply operations, then the prior Terraform State is implicitly deep copied to the new Terraform State for that attribute or block.</p> <p>Given a resource with a writeable root attribute named <code>not_set_attr</code> that never explicitly writes a value, the following happens:</p> <ul> <li>If the Terraform configuration contains <code>not_set_attr = \"anything\"</code> on resource creation, the Terraform State contains <code>not_set_attr</code> equal to <code>\"anything\"</code> after apply.</li> <li>If the Terraform configuration is updated to <code>not_set_attr = \"updated\"</code>, the Terraform State contains <code>not_set_attr</code> equal to <code>\"updated\"</code> after apply.</li> <li>If the attribute was meant to be associated with a remote system value, it will never update the Terraform State on plan or apply with the remote value. Effectively, it cannot perform drift detection with the remote value.</li> </ul> <p>This however does not apply to nested attributes and blocks if the parent block is refreshed. Given a resource with a root block named <code>parent</code>, with nested child attributes <code>set_attr</code> and <code>not_set_attr</code>, a read operation which updates the value of <code>parent</code> (and the nested <code>set_attr</code> attribute) will not copy the Terraform State for the nested <code>not_set_attr</code> attribute.</p> <p>There are valid use cases for passthrough attribute values such as these (see the Virtual Attributes section), however the behavior can be confusing or incorrect for operators if the drift detection is expected. Typically these types of drift detection issues can be discovered by implementing resource import testing with state verification.</p>"},{"location":"data-handling-and-conversion/#terraform-plugin-framework-versus-plugin-sdk-v2","title":"Terraform Plugin Framework versus Plugin SDK V2","text":"<p>Perhaps the most distinct difference between Terraform Plugin Framework and Terraform Plugin SDKv2 is data handling. With Terraform Plugin Framework state data is strongly typed, while Plugin SDK V2 based resources represent state data generically (each attribute is an <code>interface{}</code>) and types must be asserted at runtime. Strongly typed data eliminates an entire class of runtime bugs and crashes, but does require compile type declarations and a slightly different approach to reading and writing data. The sections below contain examples for both plugin libraries, but Terraform Plugin Framework is required for all net-new resources.</p>"},{"location":"data-handling-and-conversion/#data-conversions-in-the-terraform-aws-provider","title":"Data Conversions in the Terraform AWS Provider","text":"<p>To expand on the data handling that occurs specifically within the Terraform AWS Provider resource implementations, the above resource creation items become the below in practice given our current usage of the Terraform Plugin SDK:</p> Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <ul> <li>The <code>Create</code> method of a resource is invoked with <code>resource.CreateRequest</code> containing the planned new state data (<code>req.Plan</code>) and an AWS API client (stored in the <code>Meta()</code> method of the resource struct).<ul> <li>Before reaching this point, the <code>Plan</code> data was already translated from the Terraform Plugin Protocol data types by the Terraform Plugin Framework so values can be read by invoking <code>req.Plan.Get(ctx, &amp;plan)</code>, where <code>plan</code> is an instance of the struct representing the resources data.</li> </ul> </li> <li>An AWS Go SDK operation input type (e.g., <code>ec2.CreateVpcInput</code>) is initialized</li> <li>For each necessary field to configure in the operation input type, the data is read from the <code>plan</code> struct and converted into the AWS Go SDK type for the field (e.g., <code>*string</code>)</li> <li>The AWS Go SDK operation is invoked and the output type (e.g., <code>*ec2.CreateVpcOutput</code>) is initialized</li> <li>For each necessary Attribute, Block, or resource identifier to be saved in the state, the data is read from the AWS Go SDK type for the field (<code>*string</code>), if necessary converted into the equivalent Plugin Framework compatible type, and saved into a mutated data struct</li> <li>Function is returned</li> </ul> <ul> <li>The <code>CreateWithoutTimeout</code> function of a <code>schema.Resource</code> is invoked with <code>*schema.ResourceData</code> containing the planned new state data (conventionally named <code>d</code>) and an AWS API client (conventionally named <code>meta</code>).<ul> <li>Before reaching this point, the <code>ResourceData</code> was already translated from the Terraform Plugin Protocol data types by the Terraform Plugin SDK so values can be read by invoking <code>d.Get()</code> and <code>d.GetOk()</code> receiver methods with Attribute and Block names from the <code>Schema</code> of the <code>schema.Resource</code>.</li> </ul> </li> <li>An AWS Go SDK operation input type (e.g., <code>ec2.CreateVpcInput</code>) is initialized</li> <li>For each necessary field to configure in the operation input type, the data is read from the <code>ResourceData</code> (e.g., <code>d.Get()</code>, <code>d.GetOk()</code>) and converted into the AWS Go SDK type for the field (e.g., <code>*string</code>)</li> <li>The AWS Go SDK operation is invoked and the output type (e.g., <code>*ec2.CreateVpcOutput</code>) is initialized</li> <li>For each necessary Attribute, Block, or resource identifier to be saved in the state, the data is read from the AWS Go SDK type for the field (<code>*string</code>), if necessary converted into a <code>ResourceData</code> compatible type, and saved into a mutated <code>ResourceData</code> (e.g., <code>d.Set()</code>, <code>d.SetId()</code>)</li> <li>Function is returned</li> </ul>"},{"location":"data-handling-and-conversion/#type-mapping","title":"Type Mapping","text":"<p>To further understand the necessary data conversions used throughout the Terraform AWS Provider codebase between AWS Go SDK types and the Terraform Plugin SDK, the following table can be referenced for most scenarios:</p> Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 AWS API Model AWS Go SDK V2 Terraform Plugin Framework Terraform Language/State <code>boolean</code> <code>bool</code> <code>types.Bool</code> <code>bool</code> <code>float</code> <code>*float64</code> <code>types.Float64</code> <code>number</code> <code>integer</code> <code>*int64</code> <code>types.Int64</code> <code>number</code> <code>list</code> <code>[]*T</code> <code>types.List</code> <code>types.Set</code> <code>list(any)</code><code>set(any)</code> <code>map</code> <code>map[T1]*T2</code> <code>types.Map</code> <code>map(any)</code> <code>string</code> <code>*string</code> <code>types.String</code> <code>string</code> <code>structure</code> <code>struct</code> <code>types.List</code> with <code>MaxItems: 1</code> <code>list(object(any))</code> <code>timestamp</code> <code>*time.Time</code> <code>types.String</code> (typically RFC3339 formatted) <code>string</code> <p>Types are built into the Terraform Plugin Framework library and handle null and unknown values in accordance with the Terraform type system. This eliminates the need for any special handling of zero values and provides better change detection on unset values.</p> AWS API Model AWS Go SDK Terraform Plugin SDK Terraform Language/State <code>boolean</code> <code>*bool</code> <code>TypeBool</code> (<code>bool</code>) <code>bool</code> <code>float</code> <code>*float64</code> <code>TypeFloat</code> (<code>float64</code>) <code>number</code> <code>integer</code> <code>*int64</code> <code>TypeInt</code> (<code>int</code>) <code>number</code> <code>list</code> <code>[]*T</code> <code>TypeList</code> (<code>[]interface{}</code> of <code>T</code>)<code>TypeSet</code> (<code>*schema.Set</code> of <code>T</code>) <code>list(any)</code><code>set(any)</code> <code>map</code> <code>map[T1]*T2</code> <code>TypeMap</code> (<code>map[string]interface{}</code>) <code>map(any)</code> <code>string</code> <code>*string</code> <code>TypeString</code> (<code>string</code>) <code>string</code> <code>structure</code> <code>struct</code> <code>TypeList</code> (<code>[]interface{}</code> of <code>map[string]interface{}</code>) with <code>MaxItems: 1</code> <code>list(object(any))</code> <code>timestamp</code> <code>*time.Time</code> <code>TypeString</code> (typically RFC3339 formatted) <code>string</code> <p>You may notice there are type encoding differences between the AWS Go SDK and Terraform Plugin SDK:</p> <ul> <li>AWS Go SDK types are all Go pointer types, while Terraform Plugin SDK types are not.</li> <li>AWS Go SDK structures are the Go <code>struct</code> type, while there is no semantically equivalent Terraform Plugin SDK type. Instead they are represented as a slice of interfaces with an underlying map of interfaces.</li> <li>AWS Go SDK types are all Go concrete types, while the Terraform Plugin SDK types for collections and maps are interfaces.</li> <li>AWS Go SDK whole numeric type is always 64-bit, while the Terraform Plugin SDK type is implementation-specific.</li> </ul> <p>Conceptually, the first and second items above are the most problematic in the Terraform AWS Provider codebase. The first item because non-pointer types in Go cannot implement the concept of no value (<code>nil</code>). The Zero Value Mapping section will go into more detail about the implications of this limitation. The second item because it can be confusing to always handle a structure (\"object\") type as a list.</p>"},{"location":"data-handling-and-conversion/#default-values","title":"Default Values","text":"<p>If an AWS API sets a default value on the server side, no default should be set on the provider side. Instead, the argument should be marked as <code>Optional</code> and <code>Computed</code>. This avoids potential future conflicts if a server side default value changes.</p> <p>As a general rule, provider side default values should be avoided unless strictly necessary for a resource to function properly.</p>"},{"location":"data-handling-and-conversion/#zero-value-mapping","title":"Zero Value Mapping","text":"<p>Note</p> <p>This section only applies to Plugin SDK V2 based resources. Terraform Plugin Framework based resources will handle null and unknown values distinctly from zero values.</p> <p>As mentioned in the Type Mapping section for Terraform Plugin SDK V2, there is a discrepancy between how the Terraform Plugin SDK represents values and the reality that a Terraform State may not configure an Attribute. These values will default to the matching underlying Go type \"zero value\" if not set:</p> Terraform Plugin SDK Go Type Zero Value <code>TypeBool</code> <code>bool</code> <code>false</code> <code>TypeFloat</code> <code>float64</code> <code>0.0</code> <code>TypeInt</code> <code>int</code> <code>0</code> <code>TypeString</code> <code>string</code> <code>\"\"</code> <p>For Terraform resource logic this means that these special values must always be accounted for in implementation. The semantics of the API and its meaning of the zero value will determine whether:</p> <ul> <li>If it is not used/needed, then generally the zero value can safely be used to store an \"unset\" value and should be ignored when sending to the API.</li> <li>If it is used/needed, whether:<ul> <li>A value can always be set and it is safe to always send to the API. Generally, boolean values fall into this category.</li> <li>A different default/sentinel value must be used as the \"unset\" value so it can either match the default of the API or be ignored when sending to the API.</li> <li>A special type implementation is required within the schema to work around the limitation.</li> </ul> </li> </ul> <p>The maintainers can provide guidance on appropriate solutions for cases not mentioned in the Recommended Implementation section.</p>"},{"location":"data-handling-and-conversion/#root-attributes-versus-block-attributes","title":"Root Attributes Versus Block Attributes","text":"Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <p>All Attributes and Blocks at the top level of a resource structs <code>Schema</code> method are considered \"root\" attributes. These will always be handled with the <code>Plan</code> and <code>State</code> fields from the request and response pointers passed in as arguments the the CRUD methods on the resource struct. Values are read from and written to the underlying data structure during CRUD operations, and finally written to state in the response object with a call like <code>resp.Diagnostics.Append(resp.State.Set(ctx, &amp;plan)...)</code>.</p> <p>All Attributes and Blocks at the top level of <code>schema.Resource</code> <code>Schema</code> are considered \"root\" attributes. These will always be handled with receiver methods on <code>ResourceData</code>, such as reading with <code>d.Get()</code>, <code>d.GetOk()</code>, etc. and writing with <code>d.Set()</code>. Any nested Attributes and Blocks inside those root Blocks will then be handled with standard Go types according to the table in the Type Mapping section.</p> <p>Warning</p> <p>While it is possible in certain type scenarios to deeply read and write ResourceData information for a Block Attribute, this practice is discouraged in preference of only handling root Attributes and Blocks.</p>"},{"location":"data-handling-and-conversion/#recommended-implementations","title":"Recommended Implementations","text":"<p>Given the complexities around conversions between AWS and Terraform Plugin type systems, this section contains recommended implementations for Terraform AWS Provider resources.</p> <p>Tip</p> <p>Some of these coding patterns may not be well represented in the codebase, as refactoring the many older styles over years of community development is a large task. However this is meant to represent the preferred implementations today. These will continue to evolve as this codebase and the Terraform Plugin ecosystem changes.</p> <p>When using the Terraform Plugin Framework, there are two approaches for flattening and expanding Terraform data. The preferred is AutoFlex, which automatically converts between provider and AWS API structures by analyzing type information. Alternatively, provider developers can define flattening and expanding functions manually.</p> <p>When using the Terraform Plugin SDK v2, flattening and expanding functions must be defined manually.</p>"},{"location":"data-handling-and-conversion/#autoflex-for-terraform-plugin-framework-preferred","title":"AutoFlex for Terraform Plugin Framework (Preferred)","text":"<p>AutoFlex provides two entry-point functions, <code>Flatten</code> and <code>Expand</code> defined in the package <code>github.com/hashicorp/terraform-provider-aws/internal/framework/flex</code>. Without configuration, these two functions should be able to convert most provider and AWS API structures.</p> <p>AutoFlex uses field names to map between the source and target structures:</p> <ol> <li>An exact, case-sensitive match</li> <li>An exact, case-insensitive match</li> <li>Comparing plural and singular field names</li> <li>Adding a field name prefix set using the AutoFlex options function <code>flex.WithFieldNamePrefix</code>, e.g. Lex v2 Intents in <code>internal/service/lexv2models/intent.go</code></li> </ol> <p>By default, AutoFlex ignores fields with the name <code>Tags</code>, as AWS resource tags are handled separately. Additional fields can be ignored and the <code>Tags</code> field can be included by passing optional <code>flex.AutoFlexOptionsFunc</code>s to <code>Flatten</code> or <code>Expand</code>. For example, to add an additional ignored field, use</p> <pre><code>diags := flex.Expand(ctx, source, &amp;target, flex.WithIgnoredFieldNamesAppend(\"OtherField\"))\n</code></pre> <p>This will ignore both <code>Tags</code> and <code>OtherField</code>. To empty the list of ignored fields, use <code>flex.WithNoIgoredFieldNames</code>. For example, to include <code>Tags</code>, call</p> <pre><code>diags := flex.Expand(ctx, source, &amp;target, flex.WithNoIgnoredFieldNames())\n</code></pre> <p>AutoFlex is able to convert single-element lists from Terraform blocks into single struct or pointer values in AWS API structs.</p>"},{"location":"data-handling-and-conversion/#customizing-struct-field-flexing","title":"Customizing Struct Field Flexing","text":"<p>The flexing of individual struct fields can be customized by using Go struct tags, with the namespace <code>autoflex</code>.</p> <p>Tag values are comma-separated lists of options, with a leading comma.</p> <p>The option <code>legacy</code> can be used when migrating a resource or data source from the Terraform Plugin SDK to the Terraform Plugin Framework. This will preserve certain behaviors from the Plugin SDK, such as treating zero-values, i.e. the empty string or a numeric zero, equivalently to <code>null</code> values. This is equivalent to calling the <code>fwflex.&lt;Type&gt;&lt;To/From&gt;FrameworkLegacy</code> functions.</p> <p>For example, from the struct <code>resourceManagedUserPoolClientModel</code> for the Cognito IDP Managed User Pool Client:</p> <pre><code>type resourceManagedUserPoolClientModel struct {\nAccessTokenValidity                      types.Int64  `tfsdk:\"access_token_validity\" autoflex:\",legacy\"`\nAllowedOauthFlows                        types.Set    `tfsdk:\"allowed_oauth_flows\"`\n...\nClientSecret                             types.String `tfsdk:\"client_secret\"`\nDefaultRedirectUri                       types.String `tfsdk:\"default_redirect_uri\" autoflex:\",legacy\"`\n...\nID                                       types.String `tfsdk:\"id\"`\nIdTokenValidity                          types.Int64  `tfsdk:\"id_token_validity\" autoflex:\",legacy\"`\nLogoutUrls                               types.Set    `tfsdk:\"logout_urls\"`\n...\n}\n</code></pre> <p>The option <code>omitempty</code> can be used with <code>string</code> values to store a <code>null</code> value when an empty string is returned.</p> <p>For example, from the struct <code>refreshOnDayModel</code> for the QuickSight Refresh Schedule:</p> <pre><code>type refreshOnDayModel struct {\nDayOfMonth types.String `tfsdk:\"day_of_month\"`\nDayOfWeek  types.String `tfsdk:\"day_of_week\" autoflex:\",omitempty\"`\n}\n</code></pre> <p>To completely ignore a field, use the tag value <code>-</code>.</p> <p>For example, from the struct <code>scheduleModel</code> for the QuickSight Refresh Schedule:</p> <pre><code>type scheduleModel struct {\nRefreshType        types.String                                           `tfsdk:\"refresh_type\"`\nScheduleFrequency  fwtypes.ListNestedObjectValueOf[refreshFrequencyModel] `tfsdk:\"schedule_frequency\"`\nStartAfterDateTime types.String                                           `tfsdk:\"start_after_date_time\" autoflex:\"-\"`\n}\n</code></pre> <p>To ignore a field when flattening, but include it when expanding, use the option <code>noflatten</code>.</p> <p>For example, from the struct <code>dataSourceReservedCacheNodeOfferingModel</code> for the ElastiCache Reserved Cache Node Offering:</p> <pre><code>type dataSourceReservedCacheNodeOfferingModel struct {\nCacheNodeType      types.String            `tfsdk:\"cache_node_type\"`\nDuration           fwtypes.RFC3339Duration `tfsdk:\"duration\" autoflex:\",noflatten\"`\nFixedPrice         types.Float64           `tfsdk:\"fixed_price\"`\nOfferingID         types.String            `tfsdk:\"offering_id\"`\nOfferingType       types.String            `tfsdk:\"offering_type\"`\nProductDescription types.String            `tfsdk:\"product_description\"`\n}\n</code></pre>"},{"location":"data-handling-and-conversion/#overriding-default-behavior","title":"Overriding Default Behavior","text":"<p>In some cases, flattening and expanding need conditional handling. One important case is new AWS API implementations where the input or output structs make use of union types. The AWS implementation uses an interface as the common type, along with various concrete implementations. Because the Terraform schema does not support union types (see this issue for discussion), the provider defines nested schemas for each type with a restriction to allow only one.</p> <p>To override flattening behavior, implement the interface <code>flex.Flattener</code> on the model. The function should have a pointer receiver, as it will modify the struct in-place. From the Mainframe Modernization (M2) environment (<code>internal/service/m2/environment.go</code>):</p> <pre><code>type storageConfigurationModel struct {\nEFS fwtypes.ListNestedObjectValueOf[efsStorageConfigurationModel] `tfsdk:\"efs\"`\nFSX fwtypes.ListNestedObjectValueOf[fsxStorageConfigurationModel] `tfsdk:\"fsx\"`\n}\n\nfunc (m *storageConfigurationModel) Flatten(ctx context.Context, v any) (diags diag.Diagnostics) {\nswitch t := v.(type) {\ncase awstypes.StorageConfigurationMemberEfs:\nvar model efsStorageConfigurationModel\nd := fwflex.Flatten(ctx, t.Value, &amp;model)\ndiags.Append(d...)\nif diags.HasError() {\nreturn diags\n}\n\nm.EFS = fwtypes.NewListNestedObjectValueOfPtrMust(ctx, &amp;model)\n\nreturn diags\n\ncase awstypes.StorageConfigurationMemberFsx:\nvar model fsxStorageConfigurationModel\nd := fwflex.Flatten(ctx, t.Value, &amp;model)\ndiags.Append(d...)\nif diags.HasError() {\nreturn diags\n}\n\nm.FSX = fwtypes.NewListNestedObjectValueOfPtrMust(ctx, &amp;model)\n\nreturn diags\n\ndefault:\nreturn diags\n}\n}\n</code></pre> <p>To override expanding behavior, implement the interface <code>flex.Expander</code> on the model. As the function should not modify the struct in-place, it should not have a pointer receiver. From the Mainframe Modernization (M2) environment (<code>internal/service/m2/environment.go</code>):</p> <pre><code>type storageConfigurationModel struct {\nEFS fwtypes.ListNestedObjectValueOf[efsStorageConfigurationModel] `tfsdk:\"efs\"`\nFSX fwtypes.ListNestedObjectValueOf[fsxStorageConfigurationModel] `tfsdk:\"fsx\"`\n}\n\nfunc (m storageConfigurationModel) Expand(ctx context.Context) (result any, diags diag.Diagnostics) {\nswitch {\ncase !m.EFS.IsNull():\nefsStorageConfigurationData, d := m.EFS.ToPtr(ctx)\ndiags.Append(d...)\nif diags.HasError() {\nreturn nil, diags\n}\n\nvar r awstypes.StorageConfigurationMemberEfs\ndiags.Append(fwflex.Expand(ctx, efsStorageConfigurationData, &amp;r.Value)...)\nif diags.HasError() {\nreturn nil, diags\n}\n\nreturn &amp;r, diags\n\ncase !m.FSX.IsNull():\nfsxStorageConfigurationData, d := m.FSX.ToPtr(ctx)\ndiags.Append(d...)\nif diags.HasError() {\nreturn nil, diags\n}\n\nvar r awstypes.StorageConfigurationMemberFsx\ndiags.Append(fwflex.Expand(ctx, fsxStorageConfigurationData, &amp;r.Value)...)\nif diags.HasError() {\nreturn nil, diags\n}\n\nreturn &amp;r, diags\n}\n\nreturn nil, diags\n}\n</code></pre> <p>In some cases, the result types for expanding will be different when creating or updating a resource. For example, for the Verified Permissions identity source, the create operation takes a <code>Configuration</code> struct while the update operation takes an <code>UpdateConfiguration</code>, even though the contents are identical. In this case, implement the interface <code>flex.TypedExpander</code> on the model. From the Verified Permissions identity source (<code>internal/service/verifiedpermissions/identity_source.go</code>):</p> <pre><code>type configuration struct {\nCognitoUserPoolConfiguration fwtypes.ListNestedObjectValueOf[cognitoUserPoolConfiguration] `tfsdk:\"cognito_user_pool_configuration\"`\nOpenIDConnectConfiguration   fwtypes.ListNestedObjectValueOf[openIDConnectConfiguration]   `tfsdk:\"open_id_connect_configuration\"`\n}\n\n\nfunc (m configuration) ExpandTo(ctx context.Context, targetType reflect.Type) (result any, diags diag.Diagnostics) {\nswitch targetType {\ncase reflect.TypeFor[awstypes.Configuration]():\nreturn m.expandToConfiguration(ctx)\n\ncase reflect.TypeFor[awstypes.UpdateConfiguration]():\nreturn m.expandToUpdateConfiguration(ctx)\n}\nreturn nil, diags\n}\n\nfunc (m configuration) expandToConfiguration(ctx context.Context) (result awstypes.Configuration, diags diag.Diagnostics) {\nswitch {\ncase !m.CognitoUserPoolConfiguration.IsNull():\nvar result awstypes.ConfigurationMemberCognitoUserPoolConfiguration\ndiags.Append(flex.Expand(ctx, m.CognitoUserPoolConfiguration, &amp;result.Value)...)\nif diags.HasError() {\nreturn nil, diags\n}\nreturn &amp;result, diags\n\ncase !m.OpenIDConnectConfiguration.IsNull():\nvar result awstypes.ConfigurationMemberOpenIdConnectConfiguration\ndiags.Append(flex.Expand(ctx, m.OpenIDConnectConfiguration, &amp;result.Value)...)\nif diags.HasError() {\nreturn nil, diags\n}\nreturn &amp;result, diags\n}\n\nreturn nil, diags\n}\n\nfunc (m configuration) expandToUpdateConfiguration(ctx context.Context) (result awstypes.UpdateConfiguration, diags diag.Diagnostics) {\nswitch {\ncase !m.CognitoUserPoolConfiguration.IsNull():\nvar result awstypes.UpdateConfigurationMemberCognitoUserPoolConfiguration\ndiags.Append(flex.Expand(ctx, m.CognitoUserPoolConfiguration, &amp;result.Value)...)\nif diags.HasError() {\nreturn nil, diags\n}\nreturn &amp;result, diags\n\ncase !m.OpenIDConnectConfiguration.IsNull():\nvar result awstypes.UpdateConfigurationMemberOpenIdConnectConfiguration\ndiags.Append(flex.Expand(ctx, m.OpenIDConnectConfiguration, &amp;result.Value)...)\nif diags.HasError() {\nreturn nil, diags\n}\nreturn &amp;result, diags\n}\n\nreturn nil, diags\n}\n</code></pre>"},{"location":"data-handling-and-conversion/#troubleshooting","title":"Troubleshooting","text":"<p>AutoFlex can output detailed logging as it flattens or expands a value. To turn on logging for AutoFlex, use the environment variable <code>TF_LOG_AWS_AUTOFLEX</code> to set the logging level. Valid values are <code>ERROR</code>, <code>WARN</code>, <code>INFO</code>, <code>DEBUG</code>, and <code>TRACE</code>. By default, AutoFlex logging is set to <code>ERROR</code>.</p>"},{"location":"data-handling-and-conversion/#manually-defined-flattening-and-expanding-functions","title":"Manually Defined Flattening and Expanding Functions","text":"<p>By convention in the codebase, each level of Block handling beyond root attributes should be separated into \"expand\" functions that convert Terraform Plugin SDK data into the equivalent AWS Go SDK type (typically named <code>expand{Service}{Type}</code>) and \"flatten\" functions that convert an AWS Go SDK type into the equivalent Terraform Plugin SDK data (typically named <code>flatten{Service}{Type}</code>).</p> <p>Define FLatten and EXpand (i.e., flex) functions at the most local level possible. This table provides guidance on the preferred place to define flex functions based on usage.</p> Where Used Where to Define Include Service in Name One resource (e.g., <code>aws_instance</code>) Resource file (e.g., <code>internal/service/ec2/instance.go</code>) No Few resources in one service (e.g., <code>EC2</code>) Resource file or service flex file (e.g., <code>internal/service/ec2/flex.go</code>) No Widely used in one service (e.g., <code>EC2</code>) Service flex file (e.g., <code>internal/service/ec2/flex.go</code>) No Two services (e.g., <code>EC2</code> and <code>EKS</code>) Define a copy in each service If helpful 3+ services <code>internal/flex/flex.go</code> Yes"},{"location":"data-handling-and-conversion/#expand-functions-for-blocks","title":"Expand Functions for Blocks","text":"Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <pre><code>func expandStructure(tfList []structureData) *service.Structure {\nif len(tfList) == 0 {\nreturn nil\n}\n\ntfObj := tfList[0]\napiObject := &amp;service.Structure{}\n\n// ... nested attribute handling ...\n\nreturn apiObject\n}\n\nfunc expandStructures(tfList []structureData) []*service.Structure {\nif len(tfList) == 0 {\nreturn nil\n}\n\nvar apiObjects []*service.Structure\nfor _, tfObj := range tfList {\napiObject := &amp;service.Structure{}\n\n// ... nested attribute handling ...\n\napiObjects = append(apiObjects, apiObject)\n}\n\nreturn apiObjects\n}\n</code></pre> <pre><code>func expandStructure(tfMap map[string]interface{}) *service.Structure {\nif tfMap == nil {\nreturn nil\n}\n\napiObject := &amp;service.Structure{}\n\n// ... nested attribute handling ...\n\nreturn apiObject\n}\n\nfunc expandStructures(tfList []interface{}) []*service.Structure {\nif len(tfList) == 0 {\nreturn nil\n}\n\nvar apiObjects []*service.Structure\n\nfor _, tfMapRaw := range tfList {\ntfMap, ok := tfMapRaw.(map[string]interface{})\n\nif !ok {\ncontinue\n}\n\napiObject := expandStructure(tfMap)\n\nif apiObject == nil {\ncontinue\n}\n\napiObjects = append(apiObjects, apiObject)\n}\n\nreturn apiObjects\n}\n</code></pre>"},{"location":"data-handling-and-conversion/#flatten-functions-for-blocks","title":"Flatten Functions for Blocks","text":"Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <pre><code>func flattenStructure(ctx context.Context, apiObject *service.Structure) (types.List, diag.Diagnostics) {\nvar diags diag.Diagnostics\nelemType := types.ObjectType{AttrTypes: structureAttrTypes}\n\nif apiObject == nil {\nreturn types.ListNull(elemType), diags\n}\n\nobj := map[string]attr.Value{\n// ... nested attribute handling ...\n}\nobjVal, d := types.ObjectValue(structureAttrTypes, obj)\ndiags.Append(d...)\n\nlistVal, d := types.ListValue(elemType, []attr.Value{objVal})\ndiags.Append(d...)\n\nreturn listVal, diags\n}\n\nfunc flattenStructures(ctx context.Context, apiObjects []*service.Structure) (types.List, diag.Diagnostics) {\nvar diags diag.Diagnostics\nelemType := types.ObjectType{AttrTypes: structureAttrTypes}\n\nif len(apiObjects) == 0 {\nreturn types.ListNull(elemType), diags\n}\n\nelems := []attr.Value{}\nfor _, apiObject := range apiObjects {\nif apiObject == nil {\ncontinue\n}\n\nobj := map[string]attr.Value{\n// ... nested attribute handling ...\n}\nobjVal, d := types.ObjectValue(structureAttrTypes, obj)\ndiags.Append(d...)\n\nelems = append(elems, objVal)\n}\n\nlistVal, d := types.ListValue(elemType, elems)\ndiags.Append(d...)\n\nreturn listVal, diags\n}\n</code></pre> <pre><code>func flattenStructure(apiObject *service.Structure) map[string]interface{} {\nif apiObject == nil {\nreturn nil\n}\n\ntfMap := map[string]interface{}{}\n\n// ... nested attribute handling ...\n\nreturn tfMap\n}\n\nfunc flattenStructures(apiObjects []*service.Structure) []interface{} {\nif len(apiObjects) == 0 {\nreturn nil\n}\n\nvar tfList []interface{}\n\nfor _, apiObject := range apiObjects {\nif apiObject == nil {\ncontinue\n}\n\ntfList = append(tfList, flattenStructure(apiObject))\n}\n\nreturn tfList\n}\n</code></pre>"},{"location":"data-handling-and-conversion/#root-bool-and-aws-boolean","title":"Root Bool and AWS Boolean","text":"Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <p>To read, if always sending the attribute value is correct:</p> <pre><code>input := service.ExampleOperationInput{\nAttributeName: plan.AttributeName.ValueBoolPointer()\n}\n</code></pre> <p>Alternatively, if only sending the attribute value when <code>true</code>:</p> <pre><code>input := service.ExampleOperationInput{}\n\nif v := plan.AttributeName.ValueBool(); v {\ninput.AttributeName = aws.Bool(v)\n}\n</code></pre> <p>Or, if only sending the attribute value when it is known and not null:</p> <pre><code>input := service.ExampleOperationInput{}\n\nif !plan.AttributeName.IsUnknown() &amp;&amp; !plan.AttributeName.IsNull() {\ninput.AttributeName = plan.AttributeName.ValueBoolPointer()\n}\n</code></pre> <p>To write:</p> <pre><code>plan.AttributeName = flex.BoolToFramework(output.Thing.AttributeName)\n</code></pre> <p>To read, if always sending the attribute value is correct:</p> <pre><code>input := service.ExampleOperationInput{\nAttributeName: aws.String(d.Get(\"attribute_name\").(bool))\n}\n</code></pre> <p>Otherwise to read, if only sending the attribute value when <code>true</code> is preferred (<code>!ok</code> for opposite):</p> <pre><code>input := service.ExampleOperationInput{}\n\nif v, ok := d.GetOk(\"attribute_name\"); ok {\ninput.AttributeName = aws.Bool(v.(bool))\n}\n</code></pre> <p>To write:</p> <pre><code>d.Set(\"attribute_name\", output.Thing.AttributeName)\n</code></pre>"},{"location":"data-handling-and-conversion/#root-float-and-aws-float","title":"Root Float and AWS Float","text":"Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <p>To read:</p> <pre><code>input := service.ExampleOperationInput{}\n\nif !plan.AttributeName.IsNull() {\ninput.AttributeName = plan.AttributeName.ValueFloat64Pointer()\n}\n</code></pre> <p>To write:</p> <pre><code>plan.AttributeName = flex.Float64ToFramework(output.Thing.AttributeName)\n</code></pre> <p>To read:</p> <pre><code>input := service.ExampleOperationInput{}\n\nif v, ok := d.GetOk(\"attribute_name\"); ok {\ninput.AttributeName = aws.Float64(v.(float64))\n}\n</code></pre> <p>To write:</p> <pre><code>d.Set(\"attribute_name\", output.Thing.AttributeName)\n</code></pre>"},{"location":"data-handling-and-conversion/#root-int-and-aws-integer","title":"Root Int and AWS Integer","text":"Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <p>To read:</p> <pre><code>input := service.ExampleOperationInput{}\n\nif !plan.AttributeName.IsNull() {\ninput.AttributeName = plan.AttributeName.ValueInt64Pointer()\n}\n</code></pre> <p>To write:</p> <pre><code>plan.AttributeName = flex.Int64ToFramework(output.Thing.AttributeName)\n</code></pre> <p>To read:</p> <pre><code>input := service.ExampleOperationInput{}\n\nif v, ok := d.GetOk(\"attribute_name\"); ok {\ninput.AttributeName = aws.Int64(int64(v.(int)))\n}\n</code></pre> <p>To write:</p> <pre><code>d.Set(\"attribute_name\", output.Thing.AttributeName)\n</code></pre>"},{"location":"data-handling-and-conversion/#root-list-of-resource-and-aws-list-of-structure","title":"Root List of Resource and AWS List of Structure","text":"Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <p>To read:</p> <pre><code>input := service.ExampleOperationInput{}\n\nif !plan.AttributeName.IsNull() {\nvar tfList []attributeNameData\nresp.Diagnostics.Append(plan.AttributeName.ElementsAs(ctx, &amp;tfList, false)...)\nif resp.Diagnostics.HasError() {\nreturn\n}\n\ninput.AttributeName = expandStructures(tfList)\n}\n</code></pre> <p>To write:</p> <pre><code>attributeName, d := flattenStructures(ctx, output.Thing.AttributeName))\nresp.Diagnostics.Append(d...)\nstate.AttributeName = attributeName\n</code></pre> <p>To read:</p> <pre><code>input := service.ExampleOperationInput{}\n\nif v, ok := d.GetOk(\"attribute_name\"); ok &amp;&amp; len(v.([]interface{})) &gt; 0 {\ninput.AttributeName = expandStructures(v.([]interface{}))\n}\n</code></pre> <p>To write:</p> <pre><code>if err := d.Set(\"attribute_name\", flattenStructures(output.Thing.AttributeName)); err != nil {\nreturn sdkdiag.AppendErrorf(diags, \"setting attribute_name: %s\", err)\n}\n</code></pre>"},{"location":"data-handling-and-conversion/#root-list-of-resource-and-aws-structure","title":"Root List of Resource and AWS Structure","text":"Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <p>To read:</p> <pre><code>input := service.ExampleOperationInput{}\n\nif !plan.AttributeName.IsNull() {\nvar tfList []attributeNameData\nresp.Diagnostics.Append(plan.AttributeName.ElementsAs(ctx, &amp;tfList, false)...)\nif resp.Diagnostics.HasError() {\nreturn\n}\n\n// expander handles translating list with 1 item to a single AWS object\ninput.AttributeName = expandStructure(tfList)\n}\n</code></pre> <p>To write:</p> <pre><code>// flattener handles nil output, returning the equivalent null Terraform type\nattributeName, d := flattenStructures(ctx, output.Thing.AttributeName))\nresp.Diagnostics.Append(d...)\nstate.AttributeName = attributeName\n</code></pre> <p>To read:</p> <pre><code>input := service.ExampleOperationInput{}\n\nif v, ok := d.GetOk(\"attribute_name\"); ok &amp;&amp; len(v.([]interface{})) &gt; 0 &amp;&amp; v.([]interface{})[0] != nil {\ninput.AttributeName = expandStructure(v.([]interface{})[0].(map[string]interface{}))\n}\n</code></pre> <p>To write (likely to have helper function introduced soon):</p> <pre><code>if output.Thing.AttributeName != nil {\nif err := d.Set(\"attribute_name\", []interface{}{flattenStructure(output.Thing.AttributeName)}); err != nil {\nreturn sdkdiag.AppendErrorf(diags, \"setting attribute_name: %s\", err)\n}\n} else {\nd.Set(\"attribute_name\", nil)\n}\n</code></pre>"},{"location":"data-handling-and-conversion/#root-list-of-string-and-aws-list-of-string","title":"Root List of String and AWS List of String","text":"Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <p>To read:</p> <pre><code>input := service.ExampleOperationInput{}\n\nif !plan.AttributeName.IsNull() {\ninput.AttributeName = flex.ExpandFrameworkStringValueList(ctx, plan.AttributeName)\n}\n</code></pre> <p>To write:</p> <pre><code>plan.AttributeName = flex.FlattenFrameworkStringValueList(output.Thing.AttributeName)\n</code></pre> <p>To read:</p> <pre><code>input := service.ExampleOperationInput{}\n\nif v, ok := d.GetOk(\"attribute_name\"); ok &amp;&amp; len(v.([]interface{})) &gt; 0 {\ninput.AttributeName = flex.ExpandStringList(v.([]interface{}))\n}\n</code></pre> <p>To write:</p> <pre><code>d.Set(\"attribute_name\", aws.StringValueSlice(output.Thing.AttributeName))\n</code></pre>"},{"location":"data-handling-and-conversion/#root-map-of-string-and-aws-map-of-string","title":"Root Map of String and AWS Map of String","text":"Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <p>To read:</p> <pre><code>input := service.ExampleOperationInput{}\n\nif !plan.AttributeName.IsNull() {\ninput.AttributeName = flex.ExpandFrameworkStringValueMap(ctx, plan.AttributeName)\n}\n</code></pre> <p>To write:</p> <pre><code>plan.AttributeName = flex.FlattenFrameworkStringValueMap(output.Thing.AttributeName)\n</code></pre> <p>To read:</p> <pre><code>input := service.ExampleOperationInput{}\n\nif v, ok := d.GetOk(\"attribute_name\"); ok &amp;&amp; len(v.(map[string]interface{})) &gt; 0 {\ninput.AttributeName = flex.ExpandStringMap(v.(map[string]interface{}))\n}\n</code></pre> <p>To write:</p> <pre><code>d.Set(\"attribute_name\", aws.StringValueMap(output.Thing.AttributeName))\n</code></pre>"},{"location":"data-handling-and-conversion/#root-set-of-resource-and-aws-list-of-structure","title":"Root Set of Resource and AWS List of Structure","text":"Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <p>To read:</p> <pre><code>input := service.ExampleOperationInput{}\n\nif !plan.AttributeName.IsNull() {\nvar tfList []attributeNameData\nresp.Diagnostics.Append(plan.AttributeName.ElementsAs(ctx, &amp;tfList, false)...)\nif resp.Diagnostics.HasError() {\nreturn\n}\n\ninput.AttributeName = expandStructure(tfList)\n}\n</code></pre> <p>To write:</p> <pre><code>// flattener handles nil output, returning the equivalent null Terraform type\nattributeName, d := flattenStructures(ctx, output.Thing.AttributeName))\nresp.Diagnostics.Append(d...)\nstate.AttributeName = attributeName\n</code></pre> <p>To read:</p> <pre><code>input := service.ExampleOperationInput{}\n\nif v, ok := d.GetOk(\"attribute_name\"); ok &amp;&amp; v.(*schema.Set).Len() &gt; 0 {\ninput.AttributeName = expandStructures(v.(*schema.Set).List())\n}\n</code></pre> <p>To write:</p> <pre><code>if err := d.Set(\"attribute_name\", flattenStructures(output.Thing.AttributeNames)); err != nil {\nreturn sdkdiag.AppendErrorf(diags, \"setting attribute_name: %s\", err)\n}\n</code></pre>"},{"location":"data-handling-and-conversion/#root-set-of-string-and-aws-list-of-string","title":"Root Set of String and AWS List of String","text":"Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <p>To read:</p> <pre><code>input := service.ExampleOperationInput{}\n\nif !plan.AttributeName.IsNull() {\ninput.AttributeName = flex.ExpandFrameworkStringValueSet(ctx, plan.AttributeName)\n}\n</code></pre> <p>To write:</p> <pre><code>plan.AttributeName = flex.FlattenFrameworkStringValueSet(output.Thing.AttributeName)\n</code></pre> <p>To read:</p> <pre><code>input := service.ExampleOperationInput{}\n\nif v, ok := d.GetOk(\"attribute_name\"); ok &amp;&amp; v.(*schema.Set).Len() &gt; 0 {\ninput.AttributeName = flex.ExpandStringSet(v.(*schema.Set))\n}\n</code></pre> <p>To write:</p> <pre><code>d.Set(\"attribute_name\", aws.StringValueSlice(output.Thing.AttributeName))\n</code></pre>"},{"location":"data-handling-and-conversion/#root-string-and-aws-string","title":"Root String and AWS String","text":"Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <p>To read:</p> <pre><code>input := service.ExampleOperationInput{}\n\nif !plan.AttributeName.IsNull() {\ninput.AttributeName = plan.AttributeName.ValueStringPointer()\n}\n</code></pre> <p>To write:</p> <pre><code>plan.AttributeName = flex.StringToFramework(output.Thing.AttributeName)\n</code></pre> <p>To read:</p> <pre><code>input := service.ExampleOperationInput{}\n\nif v, ok := d.GetOk(\"attribute_name\"); ok {\ninput.AttributeName = aws.String(v.(string))\n}\n</code></pre> <p>To write:</p> <pre><code>d.Set(\"attribute_name\", output.Thing.AttributeName)\n</code></pre>"},{"location":"data-handling-and-conversion/#root-string-and-aws-timestamp","title":"Root String and AWS Timestamp","text":"Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <p>To ensure that parsing the read string value does not fail, use the RFC3339 timetype.</p> <p>To read:</p> <pre><code>input := service.ExampleOperationInput{}\n\nif !plan.AttributeName.IsNull() {\nattributeName, d := plan.AttributeName.ValueRFC3339Time()\nresp.Diagnostics.Append(d...)\ninput.AttributeName = aws.Time(attributeName)\n}\n</code></pre> <p>To write:</p> <pre><code>plan.AttributeName = timetypes.NewRFC3339ValueMust(aws.ToTime(output.Thing.AttributeName).Format(time.RFC3339))\n</code></pre> <p>To ensure that parsing the read string value does not fail, define <code>attribute_name</code>'s <code>schema.Schema</code> with an appropriate <code>ValidateFunc</code>:</p> <pre><code>\"attribute_name\": {\nType:         schema.TypeString,\n// ...\nValidateFunc: validation.IsRFC3339Time,\n},\n</code></pre> <p>To read:</p> <pre><code>input := service.ExampleOperationInput{}\n\nif v, ok := d.GetOk(\"attribute_name\"); ok {\nv, _ := time.Parse(time.RFC3339, v.(string))\n\ninput.AttributeName = aws.Time(v)\n}\n</code></pre> <p>To write:</p> <pre><code>if output.Thing.AttributeName != nil {\nd.Set(\"attribute_name\", aws.TimeValue(output.Thing.AttributeName).Format(time.RFC3339))\n} else {\nd.Set(\"attribute_name\", nil)\n}\n</code></pre>"},{"location":"data-handling-and-conversion/#nested-bool-and-aws-boolean","title":"Nested Bool and AWS Boolean","text":"Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <p>To read, if always sending the attribute value is correct:</p> <pre><code>func expandStructure(tfList []structureData) *service.Structure {\n// ...\n\napiObject.NestedAttributeName = tfObj.NestedAttributeName.ValueBoolPointer()\n\n// ...\n}\n</code></pre> <p>To read, if only sending the attribute value when known and not nil:</p> <pre><code>func expandStructure(tfList []structureData) *service.Structure {\n// ...\n\nif !tfObj.NestedAttributeName.IsUnknown() &amp;&amp; !tfObj.NestedAttributeName.IsNull() {\napiObject.NestedAttributeName = tfObj.NestedAttributeName.ValueBoolPointer()\n}\n\n// ...\n}\n</code></pre> <p>To write:</p> <pre><code>func flattenStructure(ctx context.Context, apiObject *service.Structure) (types.List, diag.Diagnostics) {\n// ...\n\n// flex will handle setting null when appropriate\nobj[\"nested_attribute_name\"] = flex.BoolToFramework(ctx, apiObject.NestedAttributeName)\n\n// ...\n}\n</code></pre> <p>To read, if always sending the attribute value is correct:</p> <pre><code>func expandStructure(tfMap map[string]interface{}) *service.Structure {\n// ...\n\nif v, ok := tfMap[\"nested_attribute_name\"].(bool); ok {\napiObject.NestedAttributeName = aws.Bool(v)\n}\n\n// ...\n}\n</code></pre> <p>To read, if only sending the attribute value when <code>true</code> is preferred (<code>!v</code> for opposite):</p> <pre><code>func expandStructure(tfMap map[string]interface{}) *service.Structure {\n// ...\n\nif v, ok := tfMap[\"nested_attribute_name\"].(bool); ok &amp;&amp; v {\napiObject.NestedAttributeName = aws.Bool(v)\n}\n\n// ...\n}\n</code></pre> <p>To write:</p> <pre><code>func flattenStructure(apiObject *service.Structure) map[string]interface{} {\n// ...\n\nif v := apiObject.NestedAttributeName; v != nil {\ntfMap[\"nested_attribute_name\"] = aws.ToBool(v)\n}\n\n// ...\n}\n</code></pre>"},{"location":"data-handling-and-conversion/#nested-float-and-aws-float","title":"Nested Float and AWS Float","text":"Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <p>To read:</p> <pre><code>func expandStructure(tfList []structureData) *service.Structure {\n// ...\n\nif !tfObj.NestedAttributeName.IsUnknown() &amp;&amp; !tfObj.NestedAttributeName.IsNull() {\napiObject.NestedAttributeName = tfObj.NestedAttributeName.ValueFloat64Pointer()\n}\n\n// ...\n}\n</code></pre> <p>To write:</p> <pre><code>func flattenStructure(ctx context.Context, apiObject *service.Structure) (types.List, diag.Diagnostics) {\n// ...\n\n// flex will handle setting null when appropriate\nobj[\"nested_attribute_name\"] = flex.Float64ToFramework(ctx, apiObject.NestedAttributeName)\n\n// ...\n}\n</code></pre> <p>To read:</p> <pre><code>func expandStructure(tfMap map[string]interface{}) *service.Structure {\n// ...\n\nif v, ok := tfMap[\"nested_attribute_name\"].(float64); ok &amp;&amp; v != 0.0 {\napiObject.NestedAttributeName = aws.Float64(v)\n}\n\n// ...\n}\n</code></pre> <p>To write:</p> <pre><code>func flattenStructure(apiObject *service.Structure) map[string]interface{} {\n// ...\n\nif v := apiObject.NestedAttributeName; v != nil {\ntfMap[\"nested_attribute_name\"] = aws.ToFloat64(v)\n}\n\n// ...\n}\n</code></pre>"},{"location":"data-handling-and-conversion/#nested-int-and-aws-integer","title":"Nested Int and AWS Integer","text":"Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <p>To read:</p> <pre><code>func expandStructure(tfList []structureData) *service.Structure {\n// ...\n\nif !tfObj.NestedAttributeName.IsUnknown() &amp;&amp; !tfObj.NestedAttributeName.IsNull() {\napiObject.NestedAttributeName = tfObj.NestedAttributeName.ValueInt64Pointer()\n}\n\n// ...\n}\n</code></pre> <p>To write:</p> <pre><code>func flattenStructure(ctx context.Context, apiObject *service.Structure) (types.List, diag.Diagnostics) {\n// ...\n\n// flex will handle setting null when appropriate\nobj[\"nested_attribute_name\"] = flex.Int64ToFramework(ctx, apiObject.NestedAttributeName)\n\n// ...\n}\n</code></pre> <p>To read:</p> <pre><code>func expandStructure(tfMap map[string]interface{}) *service.Structure {\n// ...\n\nif v, ok := tfMap[\"nested_attribute_name\"].(int); ok &amp;&amp; v != 0 {\napiObject.NestedAttributeName = aws.Int64(int64(v))\n}\n\n// ...\n}\n</code></pre> <p>To write:</p> <pre><code>func flattenStructure(apiObject *service.Structure) map[string]interface{} {\n// ...\n\nif v := apiObject.NestedAttributeName; v != nil {\ntfMap[\"nested_attribute_name\"] = aws.ToInt64(v)\n}\n\n// ...\n}\n</code></pre>"},{"location":"data-handling-and-conversion/#nested-list-of-resource-and-aws-list-of-structure","title":"Nested List of Resource and AWS List of Structure","text":"Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <p>To read:</p> <pre><code>func expandStructure(ctx context.Context, tfList []structureData) (*service.Structure, diag.Diagnostics) {\n// ...\n\nvar nested []nestedAttributeNameData\ndiags.Append(tfObj.NestedAttributeName.ElementsAs(ctx, &amp;nested, false)...)\n\n// expand will handle null when appropriate\napiObject.NestedAttributeName = expandNestedAttributeName(nested)\n\n// ...\n}\n</code></pre> <p>To write:</p> <pre><code>func flattenStructure(ctx context.Context, apiObject *service.Structure) (types.List, diag.Diagnostics) {\n// ...\n\n// flatten will handle setting null when appropriate\nobj[\"nested_attribute_name\"] = flattenNestedAttributeName(ctx, v)\n\n// ...\n}\n</code></pre> <p>To read:</p> <pre><code>func expandStructure(tfMap map[string]interface{}) *service.Structure {\n// ...\n\nif v, ok := tfMap[\"nested_attribute_name\"].([]interface{}); ok &amp;&amp; len(v) &gt; 0 {\napiObject.NestedAttributeName = expandStructures(v)\n}\n\n// ...\n}\n</code></pre> <p>To write:</p> <pre><code>func flattenStructure(apiObject *service.Structure) map[string]interface{} {\n// ...\n\nif v := apiObject.NestedAttributeName; v != nil {\ntfMap[\"nested_attribute_name\"] = flattenNestedStructures(v)\n}\n\n// ...\n}\n</code></pre>"},{"location":"data-handling-and-conversion/#nested-list-of-resource-and-aws-structure","title":"Nested List of Resource and AWS Structure","text":"Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <p>To read:</p> <pre><code>func expandStructure(ctx context.Context, tfList []structureData) (*service.Structure, diag.Diagnostics) {\n// ...\n\nvar nested []nestedAttributeNameData\ndiags.Append(tfObj.NestedAttributeName.ElementsAs(ctx, &amp;nested, false)...)\n\n// expand will handle null when appropriate\napiObject.NestedAttributeName = expandNestedAttributeName(nested)\n\n// ...\n}\n</code></pre> <p>To write:</p> <pre><code>func flattenStructure(ctx context.Context, apiObject *service.Structure) (types.List, diag.Diagnostics) {\n// ...\n\n// flatten will handle setting null when appropriate\nobj[\"nested_attribute_name\"] = flattenNestedAttributeName(ctx, v)\n\n// ...\n}\n</code></pre> <p>To read:</p> <pre><code>func expandStructure(tfMap map[string]interface{}) *service.Structure {\n// ...\n\nif v, ok := tfMap[\"nested_attribute_name\"].([]interface{}); ok &amp;&amp; len(v) &gt; 0 &amp;&amp; v[0] != nil {\napiObject.NestedAttributeName = expandStructure(v[0].(map[string]interface{}))\n}\n\n// ...\n}\n</code></pre> <p>To write:</p> <pre><code>func flattenStructure(apiObject *service.Structure) map[string]interface{} {\n// ...\n\nif v := apiObject.NestedAttributeName; v != nil {\ntfMap[\"nested_attribute_name\"] = []interface{}{flattenNestedStructure(v)}\n}\n\n// ...\n}\n</code></pre>"},{"location":"data-handling-and-conversion/#nested-list-of-typestring-and-aws-list-of-string","title":"Nested List of TypeString and AWS List of String","text":"Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <p>To read:</p> <pre><code>func expandStructure(ctx context.Context, tfList []structureData) (*service.Structure, diag.Diagnostics) {\n// ...\n\nif !tfObj.NestedAttributeName.IsUnknown() &amp;&amp; !tfObj.NestedAttributeName.IsNull() {\napiObject.NestedAttributeName = flex.ExpandFrameworkStringList(ctx, tfObj.NestedAttributeName)\n}\n\n// ...\n}\n</code></pre> <p>To write:</p> <pre><code>func flattenStructure(ctx context.Context, apiObject *service.Structure) (types.List, diag.Diagnostics) {\n// ...\n\n// flex will handle setting null when appropriate\nobj[\"nested_attribute_name\"] = flex.FlattenFrameworkStringList(ctx, v)\n\n// ...\n}\n</code></pre> <p>To read:</p> <pre><code>func expandStructure(tfMap map[string]interface{}) *service.Structure {\n// ...\n\nif v, ok := tfMap[\"nested_attribute_name\"].([]interface{}); ok &amp;&amp; len(v) &gt; 0 {\napiObject.NestedAttributeName = flex.ExpandStringList(v)\n}\n\n// ...\n}\n</code></pre> <p>To write:</p> <pre><code>func flattenStructure(apiObject *service.Structure) map[string]interface{} {\n// ...\n\nif v := apiObject.NestedAttributeName; v != nil {\ntfMap[\"nested_attribute_name\"] = aws.StringValueSlice(v)\n}\n\n// ...\n}\n</code></pre>"},{"location":"data-handling-and-conversion/#nested-map-of-string-and-aws-map-of-string","title":"Nested Map of String and AWS Map of String","text":"Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <p>To read:</p> <pre><code>func expandStructure(ctx context.Context, tfList []structureData) (*service.Structure, diag.Diagnostics) {\n// ...\n\nif !tfObj.NestedAttributeName.IsUnknown() &amp;&amp; !tfObj.NestedAttributeName.IsNull() {\napiObject.NestedAttributeName = flex.ExpandFrameworkStringMap(ctx, tfObj.NestedAttributeName)\n}\n\n// ...\n}\n</code></pre> <p>To write:</p> <pre><code>func flattenStructure(ctx context.Context, apiObject *service.Structure) (types.List, diag.Diagnostics) {\n// ...\n\n// flex will handle setting null when appropriate\nobj[\"nested_attribute_name\"] = flex.FlattenFrameworkStringMap(ctx, v)\n\n// ...\n}\n</code></pre> <p>To read:</p> <pre><code>input := service.ExampleOperationInput{}\n\nif v, ok := tfMap[\"nested_attribute_name\"].(map[string]interface{}); ok &amp;&amp; len(v) &gt; 0 {\napiObject.NestedAttributeName = flex.ExpandStringMap(v)\n}\n</code></pre> <p>To write:</p> <pre><code>func flattenStructure(apiObject *service.Structure) map[string]interface{} {\n// ...\n\nif v := apiObject.NestedAttributeName; v != nil {\ntfMap[\"nested_attribute_name\"] = aws.StringValueMap(v)\n}\n\n// ...\n}\n</code></pre>"},{"location":"data-handling-and-conversion/#nested-set-of-resource-and-aws-list-of-structure","title":"Nested Set of Resource and AWS List of Structure","text":"Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <p>To read:</p> <pre><code>func expandStructure(ctx context.Context, tfList []structureData) (*service.Structure, diag.Diagnostics) {\n// ...\n\nvar nested []nestedAttributeNameData\ndiags.Append(tfObj.NestedAttributeName.ElementsAs(ctx, &amp;nested, false)...)\n\n// expand will handle null when appropriate\napiObject.NestedAttributeName = expandNestedAttributeName(nested)\n\n// ...\n}\n</code></pre> <p>To write:</p> <pre><code>func flattenStructure(ctx context.Context, apiObject *service.Structure) (types.List, diag.Diagnostics) {\n// ...\n\n// flatten will handle setting null when appropriate\nobj[\"nested_attribute_name\"] = flattenNestedAttributeName(ctx, v)\n\n// ...\n}\n</code></pre> <p>To read:</p> <pre><code>func expandStructure(tfMap map[string]interface{}) *service.Structure {\n// ...\n\nif v, ok := tfMap[\"nested_attribute_name\"].(*schema.Set); ok &amp;&amp; v.Len() &gt; 0 {\napiObject.NestedAttributeName = expandStructures(v.List())\n}\n\n// ...\n}\n</code></pre> <p>To write:</p> <pre><code>func flattenStructure(apiObject *service.Structure) map[string]interface{} {\n// ...\n\nif v := apiObject.NestedAttributeName; v != nil {\ntfMap[\"nested_attribute_name\"] = flattenNestedStructures(v)\n}\n\n// ...\n}\n</code></pre>"},{"location":"data-handling-and-conversion/#nested-set-of-typestring-and-aws-list-of-string","title":"Nested Set of TypeString and AWS List of String","text":"Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <p>To read:</p> <pre><code>func expandStructure(ctx context.Context, tfList []structureData) (*service.Structure, diag.Diagnostics) {\n// ...\n\nif !tfObj.NestedAttributeName.IsUnknown() &amp;&amp; !tfObj.NestedAttributeName.IsNull() {\napiObject.NestedAttributeName = flex.ExpandFrameworkStringSet(ctx, tfObj.NestedAttributeName)\n}\n\n// ...\n}\n</code></pre> <p>To write:</p> <pre><code>func flattenStructure(ctx context.Context, apiObject *service.Structure) (types.List, diag.Diagnostics) {\n// ...\n\n// flex will handle setting null when appropriate\nobj[\"nested_attribute_name\"] = flex.FlattenFrameworkStringSet(ctx, v)\n\n// ...\n}\n</code></pre> <p>To read:</p> <pre><code>func expandStructure(tfMap map[string]interface{}) *service.Structure {\n// ...\n\nif v, ok := tfMap[\"nested_attribute_name\"].(*schema.Set); ok &amp;&amp; v.Len() &gt; 0 {\napiObject.NestedAttributeName = flex.ExpandStringSet(v)\n}\n\n// ...\n}\n</code></pre> <p>To write:</p> <pre><code>func flattenStructure(apiObject *service.Structure) map[string]interface{} {\n// ...\n\nif v := apiObject.NestedAttributeName; v != nil {\ntfMap[\"nested_attribute_name\"] = aws.StringValueSlice(v)\n}\n\n// ...\n}\n</code></pre>"},{"location":"data-handling-and-conversion/#nested-typestring-and-aws-string","title":"Nested TypeString and AWS String","text":"Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <p>To read:</p> <pre><code>func expandStructure(tfList []structureData) *service.Structure {\n// ...\n\nif !tfObj.NestedAttributeName.IsUnknown() &amp;&amp; !tfObj.NestedAttributeName.IsNull() {\napiObject.NestedAttributeName = tfObj.NestedAttributeName.ValueStringPointer()\n}\n\n// ...\n}\n</code></pre> <p>To write:</p> <pre><code>func flattenStructure(ctx context.Context, apiObject *service.Structure) (types.List, diag.Diagnostics) {\n// ...\n\n// flex will handle setting null when appropriate\nobj[\"nested_attribute_name\"] = flex.StringToFramework(ctx, apiObject.NestedAttributeName)\n\n// ...\n}\n</code></pre> <p>To read:</p> <pre><code>func expandStructure(tfMap map[string]interface{}) *service.Structure {\n// ...\n\nif v, ok := tfMap[\"nested_attribute_name\"].(string); ok &amp;&amp; v != \"\" {\napiObject.NestedAttributeName = aws.String(v)\n}\n\n// ...\n}\n</code></pre> <p>To write:</p> <pre><code>func flattenStructure(apiObject *service.Structure) map[string]interface{} {\n// ...\n\nif v := apiObject.NestedAttributeName; v != nil {\ntfMap[\"nested_attribute_name\"] = aws.ToString(v)\n}\n\n// ...\n}\n</code></pre>"},{"location":"data-handling-and-conversion/#nested-string-and-aws-timestamp","title":"Nested String and AWS Timestamp","text":"Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <p>To ensure that parsing the read string value does not fail, use the RFC3339 timetype.</p> <p>To read:</p> <pre><code>func expandStructure(tfList []structureData) (*service.Structure, diag.Diagnostics) {\n// ...\n\nif !tfObj.NestedAttributeName.IsUnknown() &amp;&amp; !tfObj.NestedAttributeName.IsNull() {\nnested := tfObj.NestedAttributeName.ValueRFC3339Time()\ndiags.Append(tfObj.NestedAttributeName.ElementsAs(ctx, &amp;nested, false)...)\n\napiObject.NestedAttributeName = aws.Time(nested)\n}\n\n// ...\n}\n</code></pre> <p>To write:</p> <pre><code>func flattenStructure(ctx context.Context, apiObject *service.Structure) (types.List, diag.Diagnostics) {\n// ...\n\nobj[\"nested_attribute_name\"] = timetypes.NewRFC3339ValueMust(aws.ToTime(apiObject.NestedAttributeName).Format(time.RFC3339))\n\n\n// ...\n}\n</code></pre> <p>To ensure that parsing the read string value does not fail, define <code>nested_attribute_name</code>'s <code>schema.Schema</code> with an appropriate <code>ValidateFunc</code>:</p> <pre><code>\"nested_attribute_name\": {\nType:         schema.TypeString,\n// ...\nValidateFunc: validation.IsRFC3339Time,\n},\n</code></pre> <p>To read:</p> <pre><code>func expandStructure(tfMap map[string]interface{}) *service.Structure {\n// ...\n\nif v, ok := tfMap[\"nested_attribute_name\"].(string); ok &amp;&amp; v != \"\" {\nv, _ := time.Parse(time.RFC3339, v)\n\napiObject.NestedAttributeName = aws.Time(v)\n}\n\n// ...\n}\n</code></pre> <p>To write:</p> <pre><code>func flattenStructure(apiObject *service.Structure) map[string]interface{} {\n// ...\n\nif v := apiObject.NestedAttributeName; v != nil {\ntfMap[\"nested_attribute_name\"] = aws.ToTime(v).Format(time.RFC3339)\n}\n\n// ...\n}\n</code></pre>"},{"location":"data-handling-and-conversion/#further-guidelines","title":"Further Guidelines","text":"<p>This section includes additional topics related to data design and decision making from the Terraform AWS Provider maintainers.</p>"},{"location":"data-handling-and-conversion/#binary-values","title":"Binary Values","text":"<p>Certain resources may need to interact with binary (non UTF-8) data while the Terraform State only supports UTF-8 data. Configurations attempting to pass binary data to an attribute will receive an error from Terraform CLI. These attributes should expect and store the value as a Base64 string while performing any necessary encoding or decoding in the resource logic.</p>"},{"location":"data-handling-and-conversion/#destroy-state-values","title":"Destroy State Values","text":"<p>During resource destroy operations, only previously applied Terraform State values are available to resource logic. Even if the configuration is updated in a manner where both the resource destroy is triggered (e.g., setting the resource meta-argument <code>count = 0</code>) and an attribute value is updated, the resource logic will only have the previously applied data values.</p> <p>Any usage of attribute values during destroy should explicitly note in the resource documentation that the desired value must be applied into the Terraform State before any apply to destroy the resource.</p>"},{"location":"data-handling-and-conversion/#hashed-values","title":"Hashed Values","text":"<p>Attribute values may be very lengthy or potentially contain Sensitive Values. A potential solution might be to use a hashing algorithm, such as MD5 or SHA256, to convert the value before saving in the Terraform State to reduce its relative size or attempt to obfuscate the value. However, there are a few reasons not to do so:</p> <ul> <li>Terraform expects any planned values to match applied values. Ensuring proper handling during the various Terraform operations such as difference planning and Terraform State storage can be a burden.</li> <li>Hashed values are generally unusable in downstream attribute references. If a value is hashed, it cannot be successfully used in another resource or provider configuration that expects the real value.</li> <li>Terraform plan differences are meant to be human-readable. If a value is hashed, operators will only see the relatively unhelpful hash differences <code>abc123 -&gt; def456</code> in plans.</li> </ul> <p>Any value hashing implementation will not be accepted. An exception to this guidance is if the remote system explicitly provides a separate hash value in responses, in which a resource can provide a separate attribute with that hashed value.</p>"},{"location":"data-handling-and-conversion/#sensitive-values","title":"Sensitive Values","text":"<p>Marking an Attribute in the Terraform Plugin Framework Schema with <code>Sensitive</code> has the following real-world implications:</p> <ul> <li>All occurrences of the Attribute will have the value hidden in plan difference output. In the context of an Attribute within a Block, all Blocks will hide all values of the Attribute.</li> <li>In Terraform CLI 0.14 (with the <code>provider_sensitive_attrs</code> experiment enabled) and later, any downstream references to the value in other configurations will hide the value in plan difference output.</li> </ul> <p>The value is either always hidden or not as the Terraform Plugin Framework does not currently implement conditional support for this functionality. Since Terraform Configurations have no control over the behavior, hiding values from the plan difference can incur a potentially undesirable user experience cost for operators.</p> <p>Given that and especially with the improvements in Terraform CLI 0.14, the Terraform AWS Provider maintainers guiding principles for determining whether an Attribute should be marked as <code>Sensitive</code> is if an Attribute value:</p> <ul> <li>Objectively will always contain a credential, password, or other secret material. Operators can have differing opinions on what constitutes secret material and the maintainers will make best-effort determinations, if necessary consulting with the HashiCorp Security team.</li> <li>If the Attribute is within a Block, all occurrences of the Attribute value will objectively contain secret material. Some APIs (and therefore the Terraform AWS Provider resources) implement generic \"setting\" and \"value\" structures which likely will contain a mixture of secret and non-secret material. These will generally not be accepted for marking as <code>Sensitive</code>.</li> </ul> <p>If you are unsatisfied with sensitive value handling, the maintainers can recommend ensuring there is a covering issue in the Terraform CLI and/or Terraform Plugin Framework projects explaining the use case. Ultimately, Terraform Plugins including the Terraform AWS Provider cannot implement their own sensitive value abilities if the upstream projects do not implement the appropriate functionality.</p>"},{"location":"data-handling-and-conversion/#virtual-attributes","title":"Virtual Attributes","text":"<p>Attributes which only exist within Terraform and not the remote system are typically referred to as virtual attributes. Especially in the case of Destroy State Values, these attributes rely on the Implicit State Passthrough behavior of values in Terraform to be available in resource logic. A fictitious example of one of these may be a resource attribute such as a <code>skip_waiting</code> flag, which is used only in the resource logic to skip the typical behavior of waiting for operations to complete.</p> <p>If a virtual attribute has a default value that does not match the Zero Value Mapping for the type, it is recommended to explicitly call <code>d.Set()</code> with the default value in the <code>schema.Resource</code> <code>Importer</code> <code>State</code> function, for example:</p> Teraform Plugin Framework (Preferred)Teraform Plugin SDK V2 <p> <pre><code>(r *ThingResource) ImportState(ctx context.Context, req resource.ImportStateRequest, resp *resource.ImportStateResponse) {\n// ... Other import activity\n\nresp.Diagnostics.Append(resp.State.SetAttribute(ctx, path.Root(\"skip_waiting\"), true)...)\n}\n</code></pre> </p> <pre><code>&amp;schema.Resource{\n// ... other fields ...\nImporter: &amp;schema.ResourceImporter{\nState: func(d *schema.ResourceData, meta interface{}) ([]*schema.ResourceData, error) {\nd.Set(\"skip_waiting\", true)\n\nreturn []*schema.ResourceData{d}, nil\n},\n},\n}\n</code></pre> <p>This helps prevent an immediate plan difference after resource import unless the configuration has a non-default value.</p>"},{"location":"data-handling-and-conversion/#glossary","title":"Glossary","text":"<p>Below is a listing of relevant terms and descriptions for data handling and conversion in the Terraform AWS Provider to establish common conventions throughout this documentation. This list is not exhaustive of all concepts of Terraform Plugins, the Terraform AWS Provider, or the data handling that occurs during Terraform runs, but these should generally provide enough context about the topics discussed here.</p> <ul> <li>AWS Go SDK: Library that converts Go code into AWS Service API compatible operations and data types.</li> <li>AWS Go SDK Model: AWS Go SDK compatible format of AWS Service API Model.</li> <li>AWS Go SDK Service: AWS Service API Go code generated from the AWS Go SDK Model. Generated by the AWS Go SDK code.</li> <li>AWS Service API: Logical boundary of an AWS service by API endpoint. Some large AWS services may be marketed with many different product names under the same service API (e.g., VPC functionality is part of the EC2 API) and vice-versa where some services may be marketed with one product name but are split into multiple service APIs (e.g., Single Sign-On functionality is split into the Identity Store and SSO Admin APIs).</li> <li>AWS Service API Model: Declarative description of the AWS Service API operations and data types. Generated by the AWS service teams. Used to operate the API and generate API clients such as the various AWS Software Development Kits (SDKs).</li> <li>Terraform Language (\"Configuration\"): Configuration syntax interpreted by the Terraform CLI. An implementation of HCL. Full Documentation.</li> <li>Terraform Plugin Protocol: Description of Terraform Plugin operations and data types. Currently based on the Remote Procedure Call (RPC) library <code>gRPC</code>.</li> <li>Terraform Plugin Go: Low-level library that converts Go code into Terraform Plugin Protocol compatible operations and data types. Not currently implemented in the Terraform AWS Provider. Project.</li> <li>Terraform Plugin Framework: High-level library that converts Go code into Terraform Plugin Protocol compatible operations and data types. This library replaces Plugin SDK V2. See Terraform Plugin Development Packages for more information. Project.</li> <li>Terraform Plugin SDK V2: High-level library that converts Go code into Terraform Plugin Protocol compatible operations and data types. This library is replaced by Plugin Framework. See Terraform Plugin Development Packages for more information. Project.</li> <li>Terraform Plugin Schema: Declarative description of types and domain-specific behaviors for a Terraform provider, including resources and attributes. Framework Documentation. SDK V2 Documentation.</li> <li>Terraform State: Bindings between objects in a remote system (e.g., an EC2 VPC) and a Terraform configuration (e.g., an <code>aws_vpc</code> resource configuration). Full Documentation.</li> </ul> <p>AWS Service API Models use specific terminology to describe data and types:</p> <ul> <li>Enumeration: Collection of valid values for a Shape.</li> <li>Operation: An API call. Includes information about input, output, and error Shapes.</li> <li>Shape: Type description.<ul> <li>boolean: Boolean value.</li> <li>float: Fractional numeric value. May contain value validation such as maximum or minimum.</li> <li>integer: Whole numeric value. May contain value validation such as maximum or minimum.</li> <li>list: Collection that contains member Shapes. May contain value validation such as maximum or minimum keys.</li> <li>map: Grouping of key Shape to value Shape. May contain value validation such as maximum or minimum keys.</li> <li>string: Sequence of characters. May contain value validation such as an enumeration, regular expression pattern, maximum length, or minimum length.</li> <li>structure: Object that contains member Shapes. May represent an error.</li> <li>timestamp: Date and time value.</li> </ul> </li> </ul> <p>The Terraform Language uses the following terminology to describe data and types:</p> <ul> <li>Attribute (\"Argument\"): Assigns a name to a data value.</li> <li>Block (\"Configuration Block\"): Container type for Attributes or Blocks.</li> <li>null: Virtual value equivalent to the Attribute not being set.</li> <li>Types: Full Documentation.<ul> <li>any: Virtual type representing any concrete type in type declarations.</li> <li>bool: Boolean value.</li> <li>list (\"tuple\"): Ordered collection of values.</li> <li>map (\"object\"): Grouping of string keys to values.</li> <li>number: Numeric value. Can be either whole or fractional numbers.</li> <li>set: Unordered collection of values.</li> <li>string: Sequence of characters.</li> </ul> </li> </ul> <p>Terraform Plugin Framework Schemas use the following terminology to describe data and types:</p> <ul> <li>Resource Schema: Grouping of Schema that represents a Terraform Resource.</li> <li>Schema: Represents an Attribute or Block. Has a Type and Behavior(s).</li> <li>Types: Full Documentation.<ul> <li>Bool: Boolean value.</li> <li>Float64: Fractional numeric value.</li> <li>Int64: Whole numeric value.</li> <li>List: An ordered collection of values or Blocks.</li> <li>Map: Grouping of key Type to value Type.</li> <li>Set: Unordered collection of values or Blocks.</li> <li>String: Sequence of characters value.</li> </ul> </li> </ul> <p>Terraform Plugin SDK Schemas use the following terminology to describe data and types:</p> <ul> <li>Behaviors: Full Documentation.<ul> <li>Sensitive: Whether the value should be hidden from user interface output.</li> <li>StateFunc: Conversion function between the value set by the Terraform Plugin and the value seen by Terraform Plugin SDK (and ultimately the Terraform State).</li> </ul> </li> <li>Element: Underlying value type for a collection or grouping Schema.</li> <li>Resource Data: Data representation of a Resource Schema. Translation layer between the Schema and Go code of a Terraform Plugin. In the Terraform Plugin SDK, the <code>ResourceData</code> Go type.</li> <li>Resource Schema: Grouping of Schema that represents a Terraform Resource.</li> <li>Schema: Represents an Attribute or Block. Has a Type and Behavior(s).</li> <li>Types: Full Documentation.<ul> <li>TypeBool: Boolean value.</li> <li>TypeFloat: Fractional numeric value.</li> <li>TypeInt: Whole numeric value.</li> <li>TypeList: Ordered collection of values or Blocks.</li> <li>TypeMap: Grouping of key Type to value Type.</li> <li>TypeSet: Unordered collection of values or Blocks.</li> <li>TypeString: Sequence of characters value.</li> </ul> </li> </ul> <p>Some other terms that may be used:</p> <ul> <li>Block Attribute (\"Child Attribute\", \"Nested Attribute\"): Block level Attribute.</li> <li>Expand Function: Function that converts Terraform Plugin SDK data into the equivalent AWS Go SDK type.</li> <li>Flatten Function: Function that converts an AWS Go SDK type into the equivalent Terraform Plugin SDK data.</li> <li>NullableTypeBool: (SDK V2 Only) Workaround \"schema type\" created to accept a boolean value that is not configured in addition to true and false. Not implemented in the Terraform Plugin SDK, but uses <code>TypeString</code> (where <code>\"\"</code> represents not configured) and additional validation.</li> <li>NullableTypeFloat: (SDK V2 Only) Workaround \"schema type\" created to accept a fractional numeric value that is not configured in addition to <code>0.0</code>. Not implemented in the Terraform Plugin SDK, but uses <code>TypeString</code> (where <code>\"\"</code> represents not configured) and additional validation.</li> <li>NullableTypeInt: (SDK V2 Only) Workaround \"schema type\" created to accept a whole numeric value that is not configured in addition to <code>0</code>. Not implemented in the Terraform Plugin SDK, but uses <code>TypeString</code> (where <code>\"\"</code> represents not configured) and additional validation.</li> <li>**Root Attribute: Resource top-level Attribute or Block.</li> </ul> <p>For additional reference, the Terraform documentation also includes a full glossary of terminology.</p>"},{"location":"debugging/","title":"Debugging","text":"<p>This guide covers strategies we have found useful in finding runtime and logic errors in the AWS Provider. We do not cover syntax or compiler errors as these are well addressed by Go documentation and IDEs, such as Visual Studio Code (\"VS Code\").</p> <p>If you have your own debugging tricks for the provider, open a pull request to add them here!</p>"},{"location":"debugging/#1-reproduce","title":"1. Reproduce","text":"<p>One of the most crucial steps in the process of bug fixing is to reproduce the bug and create a minimal reproduction of it. In this section, we will discuss why it is so important to reproduce bugs.</p> <p>TL;DR: for Repro</p> <p>Perhaps the most important step in debugging is reproducing the bug.</p>"},{"location":"debugging/#what-is-a-bug","title":"What is a bug?","text":"<p>Before we dive into the details, let's first define what a bug is. A bug is an error or flaw that produces an incorrect or unexpected result, or causes the AWS Provider to behave in an unintended manner. For the Provider, \"bugs\" generally refer to errors that happen at runtime due to logic problems or unexpected interactions with AWS.</p>"},{"location":"debugging/#why-is-it-important-to-reproduce-bugs","title":"Why is it important to reproduce bugs?","text":"<p>Reproducing a bug is the process of intentionally triggering the error or unexpected behavior that occurs when the bug is present. It is important to reproduce bugs because it allows us to:</p> <ol> <li>Verify that the bug exists: By reproducing the bug, we can confirm that the error or unexpected behavior is real and either always happens or happens intermittently and not just a one-time occurrence. This is important because if we can't reproduce the bug, we may end up wasting time trying to fix a non-existent issue.</li> <li>Understand the cause of the bug: Reproducing the bug can help us understand what is causing the error or unexpected behavior. This is important because without understanding the root cause of the bug, we may end up fixing the symptoms of the bug rather than the underlying problem.</li> <li>Test the fix: If we know how to reproduce the bug, we also know how to verify we've fixed it.</li> </ol>"},{"location":"debugging/#2-create-a-minimal-reproduction","title":"2. Create a minimal reproduction","text":"<p>TL;DR: A complete 10-line configuration that reproduces a bug is far more (perhaps 100\u00d7 more) useful than a 1000-line configuration that reproduces the same bug.</p> <p>Creating a minimal reproduction of a bug is the process of isolating the bug to its simplest form. It is very important to create a minimal reproduction of bugs, especially with Terraform configurations, because it allows us to:</p> <ol> <li>Focus on the root cause of the bug: By eliminating any extraneous configuration or dependencies, we can focus on the specific configuration that is causing the bug. This makes it easier to understand and fix the root cause of the bug.</li> <li>Save time: By creating a minimal reproduction of the bug, we can reduce the amount of time it takes to reproduce the bug and test the fix. This is because we don't have to navigate through a large configuration or deal with unnecessary dependencies. The minimal configuration becomes the basis of a new acceptance test that verifies the bug is fixed (and stays fixed in the future).</li> <li>Make it easier for others to reproduce the bug: If we are working on a team, creating a minimal reproduction of the bug makes it easier for other team members to reproduce the bug and understand the root cause.</li> </ol>"},{"location":"debugging/#how-to-create-a-minimal-reproduction-of-a-bug","title":"How to create a minimal reproduction of a bug","text":"<p>Creating a minimal reproduction of a bug can be a time-consuming process, but it is well worth the effort. Here are some tips for creating a minimal reproduction of a bug:</p> <ol> <li>Start with a simple test case: Start with a simple configuration that causes the bug.</li> <li>Remove any extraneous configuration: Remove as many resources, dependencies, and arguments as possible to simplify, while still maintaining test independence.</li> <li>Verify that the configuration still reproduces the bug: After removing the configuration, make sure that the configuration still causes the bug. If the bug no longer occurs, you may have removed too much. In this case, add back configuration until the bug reappears.</li> </ol> <p>A minimal configuration is worth its weight in gold. If you're only able to make it this far in the debugging process, create a new issue with your minimal configuration or add it to an existing issue. A minimal configuration is a great way to give someone else a jump start in looking at the problem.</p>"},{"location":"debugging/#3-create-a-new-acceptance-test","title":"3. Create A New Acceptance Test","text":"<p>Sometimes, we tend to immediately jump into the code without creating a test. However, creating an acceptance test is something we'll need to do eventually anyway but doing it first has the added benefit of allowing us to easily trigger the bug. That makes debugging easier and allows us to use debugging tools.</p>"},{"location":"debugging/#use-the-minimal-configuration-as-the-basis-for-the-test","title":"Use the Minimal Configuration as the Basis for the Test","text":"<p>Adding a problematic minimal configuration test is just like creating an acceptance test except that when we run it, it has a problem. Starting with the end in mind focuses our efforts and gives great satisfaction when the code is fixed!</p>"},{"location":"debugging/#erroring-tests-example","title":"Erroring tests example","text":"<p>For example, if we're looking at an error in the VPC flow log resource, this is how we might add a test with a minimal configuration to trigger the error.</p> <pre><code>func TestAccVPCFlowLog_destinationError(t *testing.T) {\nctx := acctest.Context(t)\nrName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck:                 func() { acctest.PreCheck(ctx, t) },\nErrorCheck:               acctest.ErrorCheck(t, names.EC2ServiceID),\nProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories,\nCheckDestroy:             testAccCheckFlowLogDestroy(ctx),\nSteps: []resource.TestStep{\n{\nConfig:      testAccVPCFlowLogConfig_destinationError(rName),\n},\n},\n})\n}\n\n// ...\n\nfunc testAccVPCFlowLogConfig_destinationError(rName string) string {\nreturn fmt.Sprintf(`\nresource \"aws_flow_log\" \"test\" {\n  # minimal configuration causing a bug\n}\n`, rName)\n}\n</code></pre> <p>If we were to run the test on the command line, this is how the erroring output might look:</p> <pre><code>make testacc TESTS=TestAccVPCFlowLog_destinationError PKG=vpc\n</code></pre> <pre><code>==&gt; Checking that code complies with gofmt requirements...\nTF_ACC=1 go test ./internal/service/ec2/... -v -count 1 -parallel 20 -run='TestAccVPCFlowLog_destinationError'  -timeout 180m\n=== RUN   TestAccVPCFlowLog_destinationError\n=== PAUSE TestAccVPCFlowLog_destinationError\n=== CONT  TestAccVPCFlowLog_destinationError\n    vpc_flow_log_test.go:297: Step 1/1 error: Error running apply: exit status 1\n\n        Error: creating Flow Log (vpc-0c2635533cef2be79): 1 error occurred:\n            * vpc-0c2635533cef2be79: 400: Access Denied for LogDestination: does-not-exist. Please check LogDestination permission\n\n          with aws_flow_log.test,\n          on terraform_plugin_test.tf line 34, in resource \"aws_flow_log\" \"test\":\n          34: resource \"aws_flow_log\" \"test\" {\n\n--- FAIL: TestAccVPCFlowLog_destinationError (13.79s)\nFAIL\nFAIL    github.com/hashicorp/terraform-provider-aws/internal/service/ec2    15.373s\nFAIL\nmake: *** [testacc] Error 1\n</code></pre>"},{"location":"debugging/#wrong-results-example","title":"Wrong results example","text":"<p>Of course, not all bugs throw errors. For example, perhaps the bug you are looking at involves a wrong value in the <code>log_group_name</code> attribute. This is an example of adding a test that will error because the value is wrong.</p> <pre><code>func TestAccVPCFlowLog_destinationError(t *testing.T) {\nctx := acctest.Context(t)\nrName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck:                 func() { acctest.PreCheck(ctx, t) },\nErrorCheck:               acctest.ErrorCheck(t, names.EC2ServiceID),\nProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories,\nCheckDestroy:             testAccCheckFlowLogDestroy(ctx),\nSteps: []resource.TestStep{\n{\nConfig:      testAccVPCFlowLogConfig_destinationError(rName),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheckFlowLogExists(ctx, resourceName, &amp;flowLog),\nresource.TestCheckResourceAttr(resourceName, \"log_group_name\", \"\"), // this should not be empty\n),\n},\n},\n})\n}\n\n// ...\n\nfunc testAccVPCFlowLogConfig_destinationError(rName string) string {\nreturn fmt.Sprintf(`\nresource \"aws_flow_log\" \"test\" {\n  # minimal configuration causing a bug\n}\n`, rName)\n}\n</code></pre> <p>If we were to run the test on the command line, this is how the erroring output might look:</p> <pre><code>make testacc TESTS=TestAccVPCFlowLog_LogDestinationType_s3 PKG=vpc\n</code></pre> <pre><code>==&gt; Checking that code complies with gofmt requirements...\nTF_ACC=1 go test ./internal/service/ec2/... -v -count 1 -parallel 20 -run='TestAccVPCFlowLog_LogDestinationType_s3'  -timeout 180m\n=== RUN   TestAccVPCFlowLog_LogDestinationType_s3\n=== PAUSE TestAccVPCFlowLog_LogDestinationType_s3\n=== CONT  TestAccVPCFlowLog_LogDestinationType_s3\n    vpc_flow_log_test.go:269: Step 1/2 error: Check failed: Check 3/4 error: aws_flow_log.test: Attribute 'log_group_name' expected \"abc-123\", got \"\"\n--- FAIL: TestAccVPCFlowLog_LogDestinationType_s3 (15.49s)\nFAIL\nFAIL    github.com/hashicorp/terraform-provider-aws/internal/service/ec2    17.358s\nFAIL\nmake: *** [testacc] Error 1\n</code></pre>"},{"location":"debugging/#take-stock","title":"Take Stock","text":"<p>It may seem like we haven't accomplished much of anything at this point. However, we have! We've:</p> <ol> <li>Reproduced the error</li> <li>Created a minimal reproduction</li> <li>Created a test to trigger the error</li> </ol> <p>This will make the rest of debugging a lot more straightforward.</p>"},{"location":"debugging/#contributing-a-failing-test","title":"Contributing a Failing Test","text":"<p>If you aren't able to figure out a bug or delving into code is not your thing, open a pull request to contribute just the test that highlights the bug. This is a valuable starting point for future work!</p> <p>We ask that you make the test \"PASS,\" but use code comments and a GitHub issue to explain what is wrong. With nearly 7,000 acceptance tests, there is always a certain percentage that inexplicably fails. Since we know that the new test we're adding highlights a known bug, we don't want the failure to be lost in the tally of inexplicable failures.</p>"},{"location":"debugging/#failing-test-contribution-example-errors","title":"Failing Test Contribution Example: Errors","text":"<p>For example, if you want to contribute just a failing test, the example below shows how to use <code>ExpectError</code> and a code comment to reference an open GitHub issue addressing the bug. Here we're using <code>ExpectError</code> to allow the test to \"pass,\" even though it should not. When the next person comes along to debug, they can simply remove <code>ExpectError</code>.</p> <pre><code>func TestAccVPCFlowLog_destinationError(t *testing.T) {\nctx := acctest.Context(t)\nrName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck:                 func() { acctest.PreCheck(ctx, t) },\nErrorCheck:               acctest.ErrorCheck(t, names.EC2ServiceID),\nProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories,\nCheckDestroy:             testAccCheckFlowLogDestroy(ctx),\nSteps: []resource.TestStep{\n{\nConfig:      testAccVPCFlowLogConfig_destinationError(rName),\n// This error should not happen!\n// See https://github.com/hashicorp/terraform-provider-aws/issues/45912\nExpectError: regexache.MustCompile(`invalid destination`),\n},\n},\n})\n}\n\n// ...\n\nfunc testAccVPCFlowLogConfig_destinationError(rName string) string {\nreturn fmt.Sprintf(`\nresource \"aws_flow_log\" \"test\" {\n  # minimal configuration causing a bug\n}\n`, rName)\n}\n</code></pre>"},{"location":"debugging/#failing-test-contribution-example-wrong-results","title":"Failing Test Contribution Example: Wrong Results","text":"<p>There are other types of bugs besides the error thrown in the previous example. For example, with a wrong results test, you would have <code>resource.TestCheckResourceAttr()</code> highlighting what is wrong, along with a link to the GitHub issue to explain why the attribute value is wrong.</p> <pre><code>func TestAccVPCFlowLog_destinationError(t *testing.T) {\nctx := acctest.Context(t)\nrName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck:                 func() { acctest.PreCheck(ctx, t) },\nErrorCheck:               acctest.ErrorCheck(t, names.EC2ServiceID),\nProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories,\nCheckDestroy:             testAccCheckFlowLogDestroy(ctx),\nSteps: []resource.TestStep{\n{\nConfig:      testAccVPCFlowLogConfig_destinationError(rName),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheckFlowLogExists(ctx, resourceName, &amp;flowLog),\n// log_group_name should be \"xyz-123\"\n// See https://github.com/hashicorp/terraform-provider-aws/issues/45912\nresource.TestCheckResourceAttr(resourceName, \"log_group_name\", \"\"),\n),\n},\n},\n})\n}\n\n// ...\n\nfunc testAccVPCFlowLogConfig_destinationError(rName string) string {\nreturn fmt.Sprintf(`\nresource \"aws_flow_log\" \"test\" {\n  # minimal configuration causing a bug\n}\n`, rName)\n}\n</code></pre>"},{"location":"debugging/#4-find-out-why-a-bug-happens","title":"4. Find Out Why a Bug Happens","text":"<p>Now that we can easily reproduce a bug with a test and we have a minimal configuration, we can look more closely at why the bug is happening.</p> <p>There are several approaches to looking at what is happening in the AWS provider. You might find that some bugs lend themselves to one approach while others are more easily evaluated with another. Also, you might start with one approach and then find you need to move to another, e.g., starting by adding a few <code>fmt.Printf()</code> statements and then move to an IDE debugger.</p>"},{"location":"debugging/#use-fmtprintf","title":"Use <code>fmt.Printf()</code>","text":"<p>One quick and dirty approach that works for simple bugs is to have Go output information to the console (i.e., terminal). This approach is especially helpful if you've reviewed the code, found an error, and have a strong suspicion about what is going on. You can use <code>fmt.Printf()</code> to confirm what you think is happening.</p> <p>The approach is also helpful to see if the code reaches a certain point or for seeing the value of a variable or two.</p> <p>This approach does not work well for complex logic, examining many lines of code, or for looking at many different variables. Use more advanced debugging, as described below, for these situations.</p> <p>This example shows using <code>fmt.Printf()</code> to output information to the console:</p> <pre><code>func resourceLogFlowCreate(ctx context.Context, d *schema.ResourceData, meta interface{}) diag.Diagnostics {\n// other code ...\n\noutputRaw, err := tfresource.RetryWhenAWSErrMessageContains(ctx, propagationTimeout, func() (interface{}, error) {\nreturn conn.CreateFlowLogsWithContext(ctx, input)\n}, errCodeInvalidParameter, \"Unable to assume given IAM role\")\n\nfmt.Printf(\"reached point %d, with input: %+v\\n\", 3, input)\n\nif err == nil &amp;&amp; outputRaw != nil {\nfmt.Printf(\"reached point %d, with output: %+v\\n\", 4, outputRaw)\nerr = UnsuccessfulItemsError(outputRaw.(*ec2.CreateFlowLogsOutput).Unsuccessful)\n}\n\n// other code ...\n}\n</code></pre> <p>Running the test from the command line, we might see the following output:</p> <pre><code>make testacc TESTS=TestAccVPCFlowLog_LogDestinationType_s3 PKG=vpc\n</code></pre> <pre><code>==&gt; Checking that code complies with gofmt requirements...\nTF_ACC=1 go test ./internal/service/ec2/... -v -count 1 -parallel 20 -run='TestAccVPCFlowLog_LogDestinationType_s3'  -timeout 180m\n=== RUN   TestAccVPCFlowLog_LogDestinationType_s3\n=== PAUSE TestAccVPCFlowLog_LogDestinationType_s3\n=== CONT  TestAccVPCFlowLog_LogDestinationType_s3\nreached point 3, with input: {\n  ClientToken: \"terraform-20230403170214312900000001\",\n  LogDestination: \"arn:aws:s3:::tf-acc-test-4802362269206133111\",\n  LogDestinationType: \"s3\",\n  MaxAggregationInterval: 600,\n  ResourceIds: [\"vpc-093265898e824c24e\"],\n  ResourceType: \"VPC\",\n  TagSpecifications: [{\n      ResourceType: \"vpc-flow-log\",\n      Tags: [{\n          Key: \"Name\",\n          Value: \"tf-acc-test-4802362269206133111\"\n        }]\n    }],\n  TrafficType: \"ALL\"\n}\nreached point 4, with output: {\n  ClientToken: \"terraform-20230403170214312900000001\",\n  FlowLogIds: [\"fl-09861862b9f8bb3a3\"]\n}\n--- PASS: TestAccVPCFlowLog_LogDestinationType_s3 (26.45s)\n</code></pre>"},{"location":"debugging/#use-visual-studio-code-debugging","title":"Use Visual Studio Code Debugging","text":"<p>Using debugging from within VS Code provides extra benefits but also an extra challenge. The extra benefits include the ability to set breakpoints, step over and into code, and see the values of variables. The extra challenge is getting your debug environment properly set up to include access to your AWS credentials and environment variables used for testing.</p> <p>Special thanks to Drew Mullen for his work on debugging the AWS provider in VS Code.</p>"},{"location":"debugging/#set-up-launchjson","title":"Set up <code>launch.json</code>","text":"<p>VS Code uses a hidden directory called <code>.vscode</code> in the root of your project for configuration files. This directory and the files it contains are ignored by Git using the <code>.gitignore</code> file included with the AWS provider. This allows you to have your own local configuration without concerns that it will be uploaded to or overwritten by the AWS provider repository.</p> <p>As shown below, use VS Code to create two files in the <code>.vscode</code> directory: <code>launch.json</code> and <code>private.env</code>.</p> <p></p> <p>In <code>launch.json</code>, add this configuration:</p> <pre><code>{\n\"version\": \"0.2.0\",\n\"configurations\": [\n{\n\"name\": \"Debug Selected Test\",\n\"request\": \"launch\",\n\"type\": \"go\",\n\"args\": [\n\"-test.v\",\n\"-test.run\",\n\"^${selectedText}$\"\n],\n\"mode\": \"auto\",\n\"program\": \"${fileDirname}\",\n\"env\": {\"PKG_NAME\": \"${relativeFileDirname}\"},\n\"envFile\": \"${workspaceFolder}/.vscode/private.env\",\n\"showLog\": true\n}\n]\n}\n</code></pre> <p>In <code>private.env</code>, add environment variables to represent your AWS provider testing configuration, including replacing <code>aws_provider_profile</code> and <code>aws_alternate_profile</code> below with the names of profiles defined in your <code>~/.aws/config</code> file:</p> <pre><code>TF_ACC=1\nTF_LOG=info\nGOFLAGS='-mod=readonly'\nAWS_PROFILE=aws_provider_profile\nAWS_DEFAULT_REGION=us-west-2\nAWS_ALTERNATE_PROFILE=aws_alternate_profile\nAWS_ALTERNATE_REGION=us-east-1\nAWS_THIRD_REGION=us-east-2\nACM_CERTIFICATE_ROOT_DOMAIN=terraform-provider-aws-acctest-acm.com\n</code></pre> <p>Note that you can set <code>TF_LOG</code> to <code>debug</code> but, if you do, you can receive thousands of lines of additional information that can make it difficult to find useful information.</p>"},{"location":"debugging/#viewing-run-and-debug","title":"Viewing Run and Debug","text":"<p>Open the Run and Debug panel in VS Code by pressing <code>Shift-\u2318-D</code> or by clicking on the debug button icon:</p> <p></p> <p>To debug, you'll also want to open the debug console by pressing <code>Shift-\u2318-Y</code> or selecting \"Debug Console\" from the View top menu.</p>"},{"location":"debugging/#run-and-debug","title":"Run and Debug","text":"<p>With the environment and VS Code GUI set up, you are ready to run and debug.</p> <ol> <li>As shown below, select the name of a test function from a <code>*_test.go</code> file by double-clicking on the name or click-dragging over the name to highlight the entire name. (Do not include the <code>func</code> at the beginning or <code>(t *testing.T) {</code> at the end.)</li> <li>Make sure that \"Debug Selected Test\" is shown next to the debug play button.</li> <li>Click on the play button in the Run and Debug panel. Depending on your configuration, there may also be a \"Debug Selected Test\" option at the bottom of the VS Code window. Clicking on that is equivalent to clicking the play button in the Run and Debug panel.</li> </ol> <p></p>"},{"location":"debugging/#breakpoints","title":"Breakpoints","text":"<p>Depending on where you suspect the problems are occurring, you can set breakpoints on the resource's Create, Read, Update, Delete, or other functions. The debugger will stop at the first breakpoint it encounters.</p> <p>Setting breakpoints is built into VS Code and is easy. Hover to the left of a line number and you'll see a faded red dot. Click on the faded red dot to turn it into a breakpoint, as shown below.</p> <p>You can see a list of breakpoints throughout the codebase at the bottom of the Run and Debug panel, as shown below.</p> <p></p>"},{"location":"debugging/#stepping-out","title":"Stepping Out","text":"<p>Once the debugger has stopped at a breakpoint, you can control how it continues from there. VS Code will display the debugger controls, which you can reposition using the grip (i.e., six dots), at the left of the controls.</p> <p></p> <p>From left to right, the controls are as follows:</p> <ol> <li>Pause (i.e., two vertical lines) pauses execution wherever it happens to be when you click pause</li> <li>Step over (i.e., dot with a curved arrow above it) goes to the next statement but will not follow execution inside functions</li> <li>Step into (i.e., dot with an arrow pointing down) goes to the next statement, following execution inside functions</li> <li>Step out (i.e., dot with an arrow pointing up) goes to the next statement in the calling function skipping the rest of the execution in the current function</li> <li>Restart (i.e., the circular arrow) stops the current run and starts at the beginning again</li> <li>Stop (i.e., the square) stops the current run</li> </ol>"},{"location":"debugging/#use-delve","title":"Use Delve","text":"<p>Behind the scenes, VS Code and other IDEs use Delve to debug. You can also use Delve without an IDE if you prefer to work on the command line.</p> <p>Here are some resources to get you started using Delve:</p> <ul> <li>Delve on GitHub</li> <li>Golang Debugging With Delve (Step-by-Step)</li> <li>Using the Go Delve Debugger from the command line</li> <li>Stop debugging Go with Println and use Delve instead</li> </ul>"},{"location":"debugging/#5-verify-the-fix-with-a-test","title":"5. Verify the Fix with a Test","text":"<p>Verify that bugs are fixed with one or more tests. The tests used to help debug, described above, verify that the bug is fixed after debugging. In addition, the tests ensure that future changes don't undo the fix.</p>"},{"location":"dependency-updates/","title":"Dependency Updates","text":"<p>Generally, dependency updates are handled by maintainers.</p>"},{"location":"dependency-updates/#go-default-version-update","title":"Go Default Version Update","text":"<p>This project typically upgrades its Go version for development and testing shortly after release to get the latest and greatest Go functionality. Before beginning the update process, ensure that you review the new version release notes to look for any areas of possible friction when updating.</p> <p>Create an issue to cover the update noting down any areas of particular interest or friction.</p> <p>Ensure that the following steps are tracked within the issue and completed within the resulting pull request.</p> <ul> <li>Update go version in <code>go.mod</code></li> <li>Verify <code>make test lint</code> works as expected</li> <li>Verify <code>goenv</code> support for the new version</li> <li>Update <code>docs/development-environment.md</code></li> <li>Update <code>.go-version</code></li> <li>Update <code>CHANGELOG.md</code> detailing the update and mention any notes practitioners need to be aware of.</li> </ul> <p>See #9992 / #10206  for a recent example.</p>"},{"location":"dependency-updates/#aws-go-sdk-updates","title":"AWS Go SDK Updates","text":"<p>Almost exclusively, <code>github.com/aws/aws-sdk-go-v2</code> updates are additive in nature. It is generally safe to only scan through them before approving and merging. If you have any concerns about any of the service client updates such as suspicious code removals in the update, or deprecations introduced, run the acceptance testing for potentially affected resources before merging.</p>"},{"location":"dependency-updates/#authentication-changes","title":"Authentication changes","text":"<p>Occasionally, there will be changes listed in the authentication pieces of the AWS Go SDK codebase, e.g., changes to <code>aws/session</code>. The AWS Go SDK <code>CHANGELOG</code> should include a relevant description of these changes under a heading such as <code>SDK Enhancements</code> or <code>SDK Bug Fixes</code>. If they seem worthy of a callout in the Terraform AWS Provider <code>CHANGELOG</code>, then upon merging we should include a similar message prefixed with the <code>provider</code> subsystem, e.g., <code>* provider: ...</code>.</p> <p>Additionally, if a <code>CHANGELOG</code> addition seemed appropriate, this dependency and version should also be updated in the Terraform S3 Backend, which currently lives in Terraform Core. An example of this can be found at https://github.com/hashicorp/terraform-provider-aws/pull/9305 and https://github.com/hashicorp/terraform/pull/22055.</p>"},{"location":"dependency-updates/#cloudfront-changes","title":"CloudFront changes","text":"<p>CloudFront service client updates have previously caused an issue when a new field introduced in the SDK was not included with Terraform and caused all requests to error (https://github.com/hashicorp/terraform-provider-aws/issues/4091). As a precaution, if you see CloudFront updates, run all the CloudFront resource acceptance testing before merging (<code>TestAccCloudFront</code>).</p>"},{"location":"dependency-updates/#golangci-lint-updates","title":"golangci-lint Updates","text":"<p>Merge if CI passes.</p>"},{"location":"dependency-updates/#terraform-plugin-development-package-updates-sdk-v2-or-framework","title":"Terraform Plugin Development Package Updates (SDK V2 or Framework)","text":"<p>Except for trivial changes, run the full acceptance testing suite against the pull request and verify there are no new or unexpected failures.</p>"},{"location":"dependency-updates/#tfproviderdocs-updates","title":"tfproviderdocs Updates","text":"<p>Merge if CI passes.</p>"},{"location":"dependency-updates/#tfproviderlint-updates","title":"tfproviderlint Updates","text":"<p>Merge if CI passes.</p>"},{"location":"dependency-updates/#yamlv2-updates","title":"yaml.v2 Updates","text":"<p>Run the acceptance testing pattern, <code>TestAccCloudFormationStack(_dataSource)?_yaml</code>, and merge if passing.</p>"},{"location":"design-decision-log/","title":"Design Decision Log","text":"<p>This serves as an index over the various decisions we make as a maintainer team over what is considered best practice, and what we should encourage/require as a design standard. These are not necessarily fixed, and are likely to evolve and be replaced as new decisions are made. This is an evolution of an internal process and will pivot to take place in public as much as possible to allow for external feedback from the community and core contributors.</p> Decision Description Issue Link Relationship Resource Design Standards Align on design standards for relationship management resources in the Terraform AWS Provider. #9901 SecretsManager Secret Target Attachment Assess the feasibility of replicating the <code>AWS::SecretsManager::SecretTargetAttachment</code> CloudFormation function with Terraform. #9183 RDS Blue Green Deployments Assess the feasibility extending blue green deployment functionality found in <code>aws_rds_instance</code> to <code>aws_rds_cluster</code>. #28956 Exclusive Relationship Management Resources A proposal describing the use case for \"exclusive relationship management\" resources and their function within the Terraform AWS provider. #39203 Standardize Use of the <code>id</code> Attribute Define a standard for use of the \"id\" attribute given improvements to provider development and testing libraries have removed its requirement. #37628 Use <code>plancheck.ExpectResourceAction</code> with disappears acceptance tests Acceptance tests exercising out of band deletion (colloquially named \"disappears\" tests) should utilize the terraform-plugin-testing library's plancheck package to assert expected post apply actions. N/A"},{"location":"development-environment/","title":"Development Environment Setup","text":""},{"location":"development-environment/#requirements","title":"Requirements","text":"<ul> <li>Terraform 0.12.26+ (to run acceptance tests)</li> <li>Go 1.23+ (to build the provider plugin)</li> <li>Mac, Linux or WSL (to build the provider plugin)</li> </ul>"},{"location":"development-environment/#quick-start","title":"Quick Start","text":"<p>If you wish to work on the provider, you'll first need Go installed on your machine (please check the requirements before proceeding).</p> <p>Note</p> <p>This project uses Go Modules making it safe to work with it outside of your existing GOPATH. The instructions that follow assume a directory in your home directory outside of the standard GOPATH (i.e <code>$HOME/development/hashicorp/</code>).</p> <p>Begin by creating a new development directory and cloning the repository.</p> <pre><code>mkdir -p $HOME/development/hashicorp/; cd $HOME/development/hashicorp/\n</code></pre> <pre><code> git clone git@github.com:hashicorp/terraform-provider-aws\n</code></pre> <p>Enter the provider directory and run <code>make tools</code>. This will install the tools for provider development.</p> <pre><code>make tools\n</code></pre>"},{"location":"development-environment/#building-the-provider","title":"Building the Provider","text":"<p>To compile the provider, run <code>make build</code>.</p> <pre><code>make build\n</code></pre> <p>This will build the provider and put the provider binary in the <code>$GOPATH/bin</code> directory.</p> <pre><code>ls -la $GOPATH/bin/terraform-provider-aws\n</code></pre>"},{"location":"development-environment/#testing-the-provider","title":"Testing the Provider","text":"<p>In order to test the provider, you can run <code>make test</code>.</p> <p>Tip</p> <p>Make sure no <code>AWS_ACCESS_KEY_ID</code> or <code>AWS_SECRET_ACCESS_KEY</code> variables are set, and there's no <code>[default]</code> section in the AWS credentials file <code>~/.aws/credentials</code>.</p> <pre><code>make test\n</code></pre> <p>In order to run the full suite of acceptance tests, run <code>make testacc</code>.</p> <p>Warning</p> <p>Acceptance tests create real resources, and often cost money to run. Please read Running and Writing Acceptance Tests before running these tests.</p> <pre><code>make testacc\n</code></pre>"},{"location":"development-environment/#using-the-provider","title":"Using the Provider","text":"<p>With Terraform v0.14 and later, development overrides for provider developers can be leveraged in order to use the provider built from source.</p> <p>To do this, populate a Terraform CLI configuration file (<code>~/.terraformrc</code> for all platforms other than Windows; <code>terraform.rc</code> in the <code>%APPDATA%</code> directory when using Windows) with at least the following options:</p> <pre><code>provider_installation {\ndev_overrides {\n\"hashicorp/aws\" = \"[REPLACE WITH GOPATH]/bin\"\n}\ndirect {}\n}\n</code></pre>"},{"location":"documentation-changes/","title":"End User Documentation Changes","text":"<p>All practitioner-focused documentation is found in the <code>/website</code> folder of the repository.</p> <pre><code>\u251c\u2500\u2500 website/docs\n    \u251c\u2500\u2500 r/                     # Documentation for resources\n    \u251c\u2500\u2500 d/                     # Documentation for data sources\n    \u251c\u2500\u2500 guides/                # Long format guides for provider level configuration or provider upgrades.\n    \u251c\u2500\u2500 cdktf/                 # Documentation for CDKTF generated in other programming languages\n    \u2514\u2500\u2500 index.html.markdown    # Home page and all provider level documentation.\n\u2514\u2500\u2500 examples/                  # Large example configurations\n</code></pre> <p>Note</p> <p>The CDKTF documentation is generated based on resource and data source documentation. Files in the <code>cdktf/</code> folder should not be edited directly.</p> <p>For any documentation change please raise a pull request including and adhering to the following:</p> <ul> <li>Reasoning for Change: Documentation updates should include an explanation for why the update is needed. If the change is a correction that aligns with AWS behavior, please include a link to the AWS Documentation in the PR.</li> <li>Prefer AWS Documentation: Documentation about AWS service features and valid argument values that are likely to update over time should link to AWS service user guides and API references where possible.</li> <li>Large Example Configurations: Example Terraform configuration that includes multiple resource definitions should be added to the repository <code>examples</code> directory instead of an individual resource documentation page. Each directory under <code>examples</code> should be self-contained to call <code>terraform apply</code> without special configuration.</li> <li>Avoid Terraform Configuration Language Features: Individual resource documentation pages and examples should refrain from highlighting particular Terraform configuration language syntax workarounds or features such as <code>variable</code>, <code>local</code>, <code>count</code>, and built-in functions.</li> </ul>"},{"location":"enhanced-region-support/","title":"Enhanced Region Support","text":"<p>Most AWS resources are Regional \u2013 they are created and exist in a single AWS Region, and to manage these resources the Terraform AWS Provider directs API calls to endpoints in the Region. The default AWS Region used to provision a resource using the provider is defined in the provider configuration used by the resource, either implicitly via environment variables or shared configuration files, or explicitly via the <code>region</code> argument. Version 6.0.0 of the Terraform AWS Provider introduces Enhanced Region Support, an additional top-level <code>region</code> argument which allows that resource to be managed in a Region other than the one defined in the provider configuration.</p> <p>In the codebase, this feature is often referred to as \"OverrideRegion\" or \"per-resource region override\".</p> <p>Every Regional resource, data source and ephemeral resource supports this feature transparently \u2013 the new top-level <code>region</code> argument does not need to be explicitly defined in the resource\u2019s schema and the resource implementation does not need to be aware whether or not a resource-level Region override is in place.</p>"},{"location":"enhanced-region-support/#effective-region","title":"Effective Region","text":"<p>The effective Region is the value of the top-level <code>region</code> argument if configured or the Region defined in the provider configuration. The <code>Region</code> method on the provider\u2019s global state object (the provider\u2019s meta object) can be called to obtain the effective Region.</p> Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <pre><code>region := r.Meta().Region(ctx)\n</code></pre> <pre><code>region := meta.(*conns.AWSClient).Region(ctx)\n</code></pre>"},{"location":"enhanced-region-support/#model-structure","title":"Model Structure","text":"<p>When using Terraform Plugin Framework, a resource's model structure must correspond to all the attributes in the resource's schema. Although the top-level <code>region</code> argument is transparently injected into a resource's schema it must be explicitly added to the resource's model. This can be done by directly embedding the <code>framework.WithRegionModel</code> structure.</p> <pre><code>type exampleResourceModel struct {\nframework.WithRegionModel\n// Fields corresponding to attributes declared in the Schema.\n}\n</code></pre>"},{"location":"enhanced-region-support/#annotations","title":"Annotations","text":"<p>Overriding the default behavior of Enhanced Region Support is done via adding resource-level annotations.</p> <p>Most resources and data sources do not need to override the default behavior; these annotations are for advanced or special cases.</p> <p><code>make gen</code> should be run after changing any annotations.</p>"},{"location":"enhanced-region-support/#global-resources-in-regional-services","title":"Global resources in regional services","text":"<p>If a resource in a Regional service is global, i.e. the addition of a top-level <code>region</code> argument is redundant or confusing (e.g. a resource representing account-wide settings) then Enhanced Region Support can be disabled by adding the <code>@Region(global=true)</code> annotation to the resource.</p> <pre><code>// @FrameworkResource(\"aws_something_example\", name=\"Example\")\n// @Region(global=true)\nfunc newExampleResource(_ context.Context) (resource.ResourceWithConfigure, error) {\nreturn &amp;resourceExample{}, nil\n}\n\ntype exampleResourceModel struct {\n// Fields corresponding to attributes in the Schema.\n// No top-level region argument so don't embed framework.WithRegionModel.\n}\n</code></pre>"},{"location":"enhanced-region-support/#suppress-region-argument-validation","title":"Suppress <code>region</code> argument validation","text":"<p>By default any configured value of the top-level <code>region</code> is validated as being in the configured partition as AWS IAM credentials are only valid for a single partition. If the argument's value need not be validated (e.g. a data source that looks up a well-known value per-Region), adding the <code>@Region(validateOverrideInPartition=false)</code> annotation suppresses validation. This is a rare requirement and should only be used for special cases.</p> <pre><code>// @FrameworkDataSource(\"aws_something_example\", name=\"Example\")\n// @Region(validateOverrideInPartition=false)\nfunc newExampleDataSource(_ context.Context) (datasource.DataSourceWithConfigure, error) {\nreturn &amp;exampleDataSource{}, nil\n}\n</code></pre>"},{"location":"enhanced-region-support/#documentation","title":"Documentation","text":"<p>The top-level <code>region</code> argument should be added to a resource's argument reference documentation. The standard text is</p> <pre><code>* `region` - (Optional) Region where this resource will be [managed](https://docs.aws.amazon.com/general/latest/gr/rande.html#regional-endpoints). Defaults to the Region set in the [provider configuration](https://registry.terraform.io/providers/hashicorp/aws/latest/docs#aws-configuration-reference).\n</code></pre>"},{"location":"error-handling/","title":"Error Handling","text":""},{"location":"error-handling/#error-handling","title":"Error Handling","text":"<p>The Terraform AWS Provider codebase bridges the implementation of a Terraform Plugin and an AWS API client to support AWS operations as Terraform Resources. An important aspect of performing remote actions is properly handling operations which are not guaranteed to succeed. Some common examples include unstable network connections, missing permissions, incorrect Terraform configurations, or unexpected responses from the remote system. All of these situations lead to an unexpected workflow action that must be surfaced to Terraform for operators to troubleshoot. This guide is intended to document best practices for surfacing these issues properly.</p> <p>For further details about how the AWS SDK for Go and the Terraform AWS Provider handle retryable errors, see the Retries and Waiters documentation.</p>"},{"location":"error-handling/#general-guidelines-and-helpers","title":"General Guidelines and Helpers","text":""},{"location":"error-handling/#naming-and-check-style","title":"Naming and Check Style","text":"<p>Following typical Go conventions, error variables in the Terraform AWS Provider codebase should be named <code>err</code>, e.g.</p> <pre><code>result, err := strconv.Itoa(\"oh no!\")\n</code></pre> <p>The code that then checks these errors should prefer <code>if</code> conditionals that usually <code>return</code> (or in the case of looping constructs, <code>break</code>/<code>continue</code>) early, especially in the case of multiple error checks, e.g.</p> <pre><code>if /* ... something checking err first ... */ {\n// ... return, break, continue, etc. ...\n}\n\nif err != nil {\n// ... return, break, continue, etc. ...\n}\n\n// all good!\n</code></pre> <p>This is in preference to some other styles of error checking, such as <code>switch</code> conditionals without a condition.</p>"},{"location":"error-handling/#wrap-errors","title":"Wrap Errors","text":"<p>Go implements error wrapping, which means that a deeply nested function call can return a particular error type, while each function up the stack can provide additional error message context without losing the ability to determine the original error. Additional information about this concept can be found in the Go blog entry titled Working with Errors in Go 1.13.</p> <p>For most use cases in this codebase, this means if code is receiving an error and needs to return it, it should implement <code>fmt.Errorf()</code> and the <code>%w</code> verb, e.g.</p> <pre><code>return fmt.Errorf(\"adding some additional message: %w\", err)\n</code></pre> <p>This type of error wrapping should be applied to all Terraform resource logic where errors (not diagnostics) are returned.</p>"},{"location":"error-handling/#aws-sdk-for-go-errors","title":"AWS SDK for Go Errors","text":"<p>The AWS SDK for Go v2 documentation includes a section on handling errors, which is recommended reading.</p> <p>For the purposes of this documentation, the most important concepts in handling these errors are:</p> <ul> <li>The SDK wraps all errors returned by service clients with the <code>smithy.OperationError</code> type.</li> <li><code>errors.As</code> should be used to unwrap errors when inspecting for a specific error type (e.g, a <code>BucketAlreadyExists</code> error from the S3 API).</li> </ul>"},{"location":"error-handling/#aws-sdk-for-go-error-helpers","title":"AWS SDK for Go Error Helpers","text":"<p>To simplify operations with AWS SDK for Go error types, the following helpers are available via the <code>github.com/hashicorp/aws-sdk-go-base/v2/tfawserr</code> Go package:</p> <ul> <li><code>tfawserr.ErrCodeEquals</code> - Preferred when the error code is specific enough for the check condition. For example, a <code>ResourceNotFoundError</code> code provides enough information that the requested resource does not exist.</li> <li><code>tfawserr.ErrMessageContains</code>: Preferred when an error code can cover multiple failure modes, and additional parsing of the error message is required to determine if it matches a specific condition.</li> </ul> <p>The recommendation for error message checking is to be just specific enough to capture the anticipated issue, but not include too much matching as the AWS API can change over time without notice. The maintainers have observed changes in wording and capitalization cause unexpected issues in the past.</p> <p>For example, given this error code and message:</p> <pre><code>InvalidParameterValueException: IAM Role arn:aws:iam::123456789012:role/XXX cannot be assumed by AWS Backup\n</code></pre> <p>An error check for this might be:</p> <pre><code>if tfawserr.ErrMessageContains(err, backup.ErrCodeInvalidParameterValueException, \"cannot be assumed\") {\n// Special handling here\n}\n</code></pre> <p>The Amazon Resource Name in the error message will be different for every environment and does not add value to the check. The AWS Backup suffix is also extraneous and could change should the service ever rename.</p>"},{"location":"error-handling/#aws-sdk-for-go-error-constants","title":"AWS SDK for Go Error Constants","text":"<p>Each AWS SDK for Go v2 service API typically implements common error codes, which get exported as public structs in the SDK. In the AWS SDK for Go v2 API Reference, these can be found in each of the service packages <code>types</code> subpackage (typically named <code>{ErrorType}Exception</code>).</p> <p>If an AWS SDK for Go service API is missing an error code constant, an AWS Support case should be submitted and a new constant can be added to <code>internal/service/{SERVICE}/errors.go</code> file (created if not present), e.g.</p> <pre><code>const(\nErrCodeInvalidParameterException = \"InvalidParameterException\"\n)\n</code></pre> <p>Then referencing code can use it via:</p> <pre><code>// imports\ntf{SERVICE} \"github.com/hashicorp/terraform-provider-aws/internal/service/{SERVICE}\"\n\n// logic\ntfawserr.ErrCodeEquals(err, tf{SERVICE}.ErrCodeInvalidParameterException)\n</code></pre>"},{"location":"error-handling/#terraform-plugin-types-and-helpers","title":"Terraform Plugin Types and Helpers","text":"<p>The Terraform Plugin SDK includes some error types which are used in certain operations and typically preferred over implementing new types:</p> <ul> <li><code>retry.NotFoundError</code></li> </ul> <p>Note</p> <p>While these helpers currently reside in the Terraform Plugin SDK V2 package, they can be used with Plugin Framework based resources. In the future these functions will likely be migrated into the provider itself, or a standalone library as there is no direct dependency on Plugin SDK functionality.</p> <p>The Terraform AWS Provider codebase implements some additional helpers for working with these in the <code>internal/tfresource</code> package:</p> <ul> <li><code>tfresource.NotFound(err)</code>: Returns true if the error is a <code>retry.NotFoundError</code>.</li> <li><code>tfresource.TimedOut(err)</code>: Returns true if the error is a <code>retry.TimeoutError</code> and contains no <code>LastError</code>. This typically signifies that the retry logic was never signaled for a retry, which can happen when AWS API operations are automatically retrying before returning.</li> </ul>"},{"location":"error-handling/#resource-lifecycle-guidelines","title":"Resource Lifecycle Guidelines","text":"<p>Terraform CLI and the Terraform Plugin libraries have certain expectations and automatic behaviors depending on the lifecycle operation of a resource. This section highlights some common issues that can occur and their expected resolution.</p>"},{"location":"error-handling/#resource-creation","title":"Resource Creation","text":"<p>For Terraform Plugin Framework based resources, creation is implemented on the <code>Create</code> method of the resource struct. For Terraform Plugin SDK V2 based resources, creation is implemented on the <code>CreateWithoutTimeout</code> function of the resource schema definition.</p>"},{"location":"error-handling/#creation-error-message-context","title":"Creation Error Message Context","text":"Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <p>Errors encountered during creation should include additional messaging about the location or cause of the error for operators and code maintainers. Plugin Framework based resources will append error diagnostics to the <code>resource.CreateResponse</code> <code>Diagnostics</code> field. The <code>create.ProblemStandardMessage</code> helper function can be used for constructing the appropriate context to accompany the error.</p> <pre><code>if err != nil {\nresp.Diagnostics.AddError(\ncreate.ProblemStandardMessage(names.{{ .Service }}, create.ErrActionCreating, ResName{{ .Resource }}, plan.Name.String(), err),\nerr.Error(),\n)\nreturn\n}\n</code></pre> <p>e.g.</p> <pre><code>if err != nil {\nresp.Diagnostics.AddError(\ncreate.ProblemStandardMessage(names.QuickSight, create.ErrActionCreating, ResNameVPCConnection, plan.Name.String(), err),\nerr.Error(),\n)\nreturn\n}\n</code></pre> <p>Resources that use other operations that return errors (e.g. waiters) should follow a similar pattern, including the resource identifier since it has typically been set before this execution:</p> <pre><code>createTimeout := r.CreateTimeout(ctx, plan.Timeouts)\nwaitOut, err := waitVPCConnectionCreated(ctx, conn, plan.ID.ValueString(), createTimeout)\nif err != nil {\nresp.Diagnostics.AddError(\ncreate.ProblemStandardMessage(names.QuickSight, create.ErrActionWaitingForCreation, ResNameVPCConnection, plan.ID.String(), err),\nerr.Error(),\n)\nreturn\n}\n</code></pre> <p>Errors encountered during creation should include additional messaging about the location or cause of the error for operators and code maintainers. The <code>create.AppendDiagError</code> helper function can be used to convert a native error into a diagnostic, which is the method for surfacing warnings and errors in Terraform providers.</p> <pre><code>if err != nil {\nreturn create.AppendDiagError(diags, names.{{ .Service }}, create.ErrActionCreating, ResName{{ .Resource }}, d.Get(\"name\").(string), err)...)\n}\n</code></pre> <p>e.g.</p> <pre><code>if err != nil {\nreturn create.AppendDiagError(diags, names.IVS, create.ErrActionCreating, ResNameRecordingConfiguration, d.Id(), err)\n}\n</code></pre> <p>Resources that use other operations that return errors (e.g. waiters) should follow a similar pattern, including the resource identifier since it has typically been set before this execution:</p> <pre><code>if _, err := waitRecordingConfigurationCreated(ctx, conn, d.Id(), d.Timeout(schema.TimeoutCreate)); err != nil {\nreturn create.AppendDiagError(diags, names.IVS, create.ErrActionWaitingForCreation, ResNameRecordingConfiguration, d.Id(), err)\n}\n</code></pre>"},{"location":"error-handling/#disnewresource-checks","title":"d.IsNewResource() Checks","text":"<p>Note</p> <p>This type of check only applies to Plugin SDK V2 based resources. Plugin Framework based resources rely solely on retries and waiters to handle eventually consistent resources.</p> <p>During resource creation, Terraform CLI expects either a properly applied state for the new resource or an error. To signal proper resource existence, the Terraform Plugin SDK uses an underlying resource identifier (set via <code>d.SetId(/* some value */)</code>). If for some reason the resource creation is returned without an error, but also without the resource identifier being set, Terraform CLI will return an error such as:</p> <pre><code>Error: Provider produced inconsistent result after apply\n\nWhen applying changes to aws_sns_topic_subscription.sqs,\nprovider \"registry.terraform.io/hashicorp/aws\" produced an unexpected new\nvalue: Root resource was present, but now absent.\n\nThis is a bug in the provider, which should be reported in the provider's own\nissue tracker.\n</code></pre> <p>A typical pattern in resource implementations in the <code>CreateWithoutTimeout</code> function is to <code>return</code> the <code>ReadWithoutTimeout</code> function at the end to fill in the Terraform State for all attributes. Another typical pattern in resource implementations in the <code>ReadWithoutTimeout</code> function is to remove the resource from the Terraform State if the remote system returns an error or status that indicates the remote resource no longer exists by explicitly calling <code>d.SetId(\"\")</code> and returning no error. If the remote system is not strongly read-after-write consistent (eventually consistent), this means the resource creation can return no error and also return no resource state.</p> <p>To prevent this type of Terraform CLI error, the resource implementation should also check against <code>d.IsNewResource()</code> before removing from the Terraform State and returning no error. If that check is <code>true</code>, then remote operation error (or one synthesized from the non-existent status) should be returned instead. While adding this check will not fix the resource implementation to handle the eventually consistent nature of the remote system, the error being returned will be less opaque for operators and code maintainers to troubleshoot.</p> <p>In the Terraform AWS Provider, an initial fix for the Terraform CLI error will typically look like:</p> <pre><code>func resourceServiceThingCreate(ctx context.Context, d *schema.ResourceData, meta interface{}) diag.Diagnostics {\nvar diags diag.Diagnostics\n\n/* ... */\n\nreturn append(diags, resourceServiceThingRead(ctx, d, meta)...)\n}\n\nfunc resourceServiceThingRead(ctx context.Context, d *schema.ResourceData, meta interface{}) diag.Diagnostics {\nvar diags diag.Diagnostics\n\n/* ... */\n\noutput, err := conn.DescribeServiceThing(input)\n\nif !d.IsNewResource() &amp;&amp; tfawserr.ErrCodeEquals(err, \"ResourceNotFoundException\") {\nlog.Printf(\"[WARN] {Service} {Thing} (%s) not found, removing from state\", d.Id())\nd.SetId(\"\")\nreturn diags\n}\n\nif err != nil {\nreturn create.AppendDiagError(diags, names.{{ .Service }}, create.ErrActionReading, ResName{{ .Resource }}, d.Id(), err)\n}\n\n/* ... */\n}\n</code></pre> <p>If the remote system is eventually consistent, see the Retries and Waiters documentation on Resource Lifecycle Retries for how to prevent consistency-type errors.</p>"},{"location":"error-handling/#resource-read","title":"Resource Read","text":"<p>For Terraform Plugin Framework based resources, read is implemented on the <code>Read</code> method of the resource struct. For Terraform Plugin SDK V2 based resources, read is implemented on the <code>ReadWithoutTimeout</code> function of the resource schema definition.</p>"},{"location":"error-handling/#read-error-message-context","title":"Read Error Message Context","text":"Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <p>Errors encountered during read should include the resource identifier (for managed resources) and additional messaging about the location or cause of the error for operators and code maintainers. Plugin Framework based resources will append error diagnostics to the <code>resource.ReadResponse</code> <code>Diagnostics</code> field. The <code>create.ProblemStandardMessage</code> helper function can be used for constructing the appropriate context to accompany the error.</p> <pre><code>if err != nil {\nresp.Diagnostics.AddError(\ncreate.ProblemStandardMessage(names.{{ .Service }}, create.ErrActionReading, ResName{{ .Resource }}, state.ID.String(), err),\nerr.Error(),\n)\nreturn\n}\n</code></pre> <p>e.g.</p> <pre><code>if err != nil {\nresp.Diagnostics.AddError(\ncreate.ProblemStandardMessage(names.QuickSight, create.ErrActionReading, ResNameVPCConnection, state.ID.String(), err),\nerr.Error(),\n)\nreturn\n}\n</code></pre> <p>Errors encountered during read should include the resource identifier (for managed resources) and additional messaging about the location or cause of the error for operators and code maintainers. The <code>create.AppendDiagError</code> helper function can be used to convert a native error into a diagnostic, which is the method for surfacing warnings and errors in Terraform providers.</p> <pre><code>if err != nil {\nreturn create.AppendDiagError(diags, names.{{ .Service }}, create.ErrActionReading, ResName{{ .Resource }}, d.Id(), err)\n}\n</code></pre> <p>e.g.</p> <pre><code>if err != nil {\nreturn create.AppendDiagError(diags, names.IVS, create.ErrActionReading, ResNameRecordingConfiguration, d.Id(), err)\n}\n</code></pre>"},{"location":"error-handling/#singular-data-source-errors","title":"Singular Data Source Errors","text":"<p>A data source which is expected to return Terraform State about a single remote resource is commonly referred to as a \"singular\" data source. Implementation-wise, it may use any available describe or list functionality from the remote system to retrieve the information. In addition to remote operation and data handling errors, errors should also be returned if:</p> <ul> <li>Zero results are found.</li> <li>Multiple results are found.</li> </ul> <p>For remote operations that are designed to return an error when the remote resource is not found, this error is typically just passed through similar to other remote operation errors. For remote operations that are designed to return a successful result whether there are zero, one, or multiple results, the error must be generated.</p> <p>For example in pseudo-code:</p> <pre><code>output, err := conn.ListServiceThings(input)\n\nif err != nil {\n// Return error diagnostic wrapping remote error\n}\n\nif output == nil || len(output.Results) == 0 {\n// Return custom error diagnostic indicating empty results\n}\n\nif len(output.Results) &gt; 1 {\n// Return custom error diagnostic indicating multiple results\n}\n</code></pre>"},{"location":"error-handling/#plural-data-source-errors","title":"Plural Data Source Errors","text":"<p>An emergent concept is a data source that returns multiple results, acting similarly to listing functionality from the remote system. These types of data sources should return not return errors if:</p> <ul> <li>Zero results are found.</li> <li>Multiple results are found.</li> </ul> <p>Remote operation and other data handling errors should still be returned.</p>"},{"location":"error-handling/#resource-update","title":"Resource Update","text":"<p>For Terraform Plugin Framework based resources, update is implemented on the <code>Update</code> method of the resource struct. For Terraform Plugin SDK V2 based resources, update is implemented on the <code>UpdateWithoutTimeout</code> function of the resource schema definition.</p>"},{"location":"error-handling/#update-error-message-context","title":"Update Error Message Context","text":"Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <p>Errors ecnountered during update should include the resource identifier and additional messaging about the location or cause of the error for operators and code maintainers. Plugin Framework based resources will append error diagnostics to the <code>resource.UpdateResponse</code> <code>Diagnostics</code> field. The <code>create.ProblemStandardMessage</code> helper function can be used for constructing the appropriate context to accompany the error.</p> <pre><code>if err != nil {\nresp.Diagnostics.AddError(\ncreate.ProblemStandardMessage(names.{{ .Service }}, create.ErrActionUpdating, ResName{{ .Resource }}, plan.ID.String(), err),\nerr.Error(),\n)\nreturn\n}\n</code></pre> <p>e.g.</p> <pre><code>if err != nil {\nresp.Diagnostics.AddError(\ncreate.ProblemStandardMessage(names.QuickSight, create.ErrActionUpdating, ResNameVPCConnection, plan.ID.String(), err),\nerr.Error(),\n)\nreturn\n}\n</code></pre> <p>Resources that use other operations that return errors (e.g. waiters) should follow a similar pattern:</p> <pre><code>updateTimeout := r.UpdateTimeout(ctx, plan.Timeouts)\nwaitOut, err := waitVPCConnectionUpdated(ctx, conn, plan.ID.ValueString(), updateTimeout)\nif err != nil {\nresp.Diagnostics.AddError(\ncreate.ProblemStandardMessage(names.QuickSight, create.ErrActionWaitingForUpdate, ResNameVPCConnection, plan.ID.String(), err),\nerr.Error(),\n)\nreturn\n}\n</code></pre> <p>Errors encountered during update should include the resource identifier and additional messaging about the location or cause of the error for operators and code maintainers. The <code>create.AppendDiagError</code> helper function can be used to convert a native error into a diagnostic, which is the method for surfacing warnings and errors in Terraform providers.</p> <pre><code>if err != nil {\nreturn create.AppendDiagError(diags, names.{{ .Service }}, create.ErrActionUpdating, ResName{{ .Resource }}, d.Id(), err)\n}\n</code></pre> <p>e.g.</p> <pre><code>if err != nil {\nreturn create.AppendDiagError(diags, names.IVS, create.ErrActionUpdating, ResNameRecordingConfiguration, d.Id(), err)\n}\n</code></pre> <p>Resources that use other operations that return errors (e.g. waiters) should follow a similar pattern:</p> <pre><code>if _, err := waitRecordingConfigurationUpdated(ctx, conn, d.Id(), d.Timeout(schema.TimeoutUpdate)); err != nil {\nreturn create.AppendDiagError(diags, names.IVS, create.ErrActionWaitingForUpdate, ResNameRecordingConfiguration, d.Id(), err)\n}\n</code></pre>"},{"location":"error-handling/#resource-deletion","title":"Resource Deletion","text":"<p>For Terraform Plugin Framework based resources, deletion is implemented on the <code>Delete</code> method of the resource struct. For Terraform Plugin SDK V2 based resources, deletion is implemented on the <code>DeleteWithoutTimeout</code> function of the resource schema definition.</p>"},{"location":"error-handling/#deletion-error-message-context","title":"Deletion Error Message Context","text":"Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <p>Errors encountered during deletion should include the resource identifier and additional messaging about the location or cause of the error for operators and code maintainers. Plugin Framework based resources will append error diagnostics to the <code>resource.DeleteResponse</code> <code>Diagnostics</code> field. The <code>create.ProblemStandardMessage</code> helper function can be used for constructing the appropriate context to accompany the error.</p> <pre><code>if err != nil {\nresp.Diagnostics.AddError(\ncreate.ProblemStandardMessage(names.{{ .Service }}, create.ErrActionDeleting, ResName{{ .Resource }}, state.ID.String(), err),\nerr.Error(),\n)\nreturn\n}\n</code></pre> <p>e.g.</p> <pre><code>if err != nil {\nresp.Diagnostics.AddError(\ncreate.ProblemStandardMessage(names.QuickSight, create.ErrActionDeleting, ResNameVPCConnection, state.ID.String(), err),\nerr.Error(),\n)\nreturn\n}\n</code></pre> <p>Resources that use other operations that return errors (e.g. waiters) should follow a similar pattern:</p> <pre><code>deleteTimeout := r.DeleteTimeout(ctx, plan.Timeouts)\nwaitOut, err := waitVPCConnectionDeleted(ctx, conn, state.ID.ValueString(), deleteTimeout)\nif err != nil {\nresp.Diagnostics.AddError(\ncreate.ProblemStandardMessage(names.QuickSight, create.ErrActionWaitingForDeletion, ResNameVPCConnection, state.ID.String(), err),\nerr.Error(),\n)\nreturn\n}\n</code></pre> <p>Errors encountered during deletion should include the resource identifier and additional messaging about the location or cause of the error for operators and code maintainers. The <code>create.AppendDiagError</code> helper function can be used to convert a native error into a diagnostic, which is the method for surfacing warnings and errors in Terraform providers.</p> <pre><code>if err != nil {\nreturn create.AppendDiagError(diags, names.{{ .Service }}, create.ErrActionDeleting, ResName{{ .Resource }}, d.Id(), err)\n}\n</code></pre> <p>e.g.</p> <pre><code>if err != nil {\nreturn create.AppendDiagError(diags, names.IVS, create.ErrActionDeleting, ResNameRecordingConfiguration, d.Id(), err)\n}\n</code></pre> <p>Resources that use other operations that return errors (e.g. waiters) should follow a similar pattern:</p> <pre><code>if _, err := waitRecordingConfigurationDeleted(ctx, conn, d.Id(), d.Timeout(schema.TimeoutDelete)); err != nil {\nreturn create.AppendDiagError(diags, names.IVS, create.ErrActionWaitingForDeletion, ResNameRecordingConfiguration, d.Id(), err)\n}\n</code></pre>"},{"location":"error-handling/#resource-already-deleted","title":"Resource Already Deleted","text":"<p>A typical pattern for resource deletion is to immediately perform the remote system deletion operation without checking existence. This is generally acceptable as operators are encouraged to always refresh their Terraform State prior to performing changes. However, in certain scenarios, such as external systems modifying the remote system prior to the Terraform execution, it is still possible that the remote system will return an error signifying that the resource does not exist. In these cases, resources should implement logic that skips returning the error.</p> Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <p>Note</p> <p>The Terraform Plugin Framework automatically handles the equivalent of <code>resp.State.RemoveResource()</code> on deletion, so it is not necessary to include it.</p> <pre><code>output, err := conn.DeleteServiceThing(input)\n\nif tfawserr.ErrCodeEquals(err, \"ResourceNotFoundException\") {\nreturn\n}\n\nif err != nil {\nresp.Diagnostics.AddError(\ncreate.ProblemStandardMessage(names.{{ .Service }}, create.ErrActionDeleting, ResName{{ .Resource }}, state.ID.String(), err),\nerr.Error(),\n)\nreturn\n}\n</code></pre> <p>Note</p> <p>The Terraform Plugin SDK V2 automatically handles the equivalent of <code>d.SetId(\"\")</code> on deletion, so it is not necessary to include it.</p> <pre><code>output, err := conn.DeleteServiceThing(input)\n\nif tfawserr.ErrCodeEquals(err, \"ResourceNotFoundException\") {\nreturn diags\n}\n\nif err != nil {\nreturn create.AppendDiagError(diags, names.{{ .Service }}, create.ErrActionDeleting, ResName{{ .Resource }}, d.Id(), err)\n}\n</code></pre>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#who-are-the-maintainers","title":"Who are the maintainers?","text":"<p>The HashiCorp Terraform AWS provider team is :</p> <ul> <li>Marc Cosentino, Product Manager - GitHub @marcosentino</li> <li>Simon Davis, Engineering Manager - GitHub @breathingdust</li> <li>Justin Retzolk, Ecosystem Engineer - GitHub @justinretzolk</li> <li>Adrian Johnson, Engineer - GitHub @johnsonaj</li> <li>Dirk Avery, Engineer - GitHub @YakDriver</li> <li>Graham Davison, Engineer - GitHub @gdavison</li> <li>Jared Baker, Engineer - GitHub @jar-b</li> <li>Kit Ewbank, Engineer - GitHub @ewbankkit</li> </ul>"},{"location":"faq/#why-isnt-my-pr-merged-yet","title":"Why isn\u2019t my PR merged yet?","text":"<p>Unfortunately, due to the volume of issues and new pull requests we receive, we are unable to give each one the full attention that we would like. We always focus on the contributions that provide the greatest value to the most community members. For more information on how we prioritize pull requests, see the prioritization guide.</p>"},{"location":"faq/#how-do-you-decide-what-gets-merged-for-each-release","title":"How do you decide what gets merged for each release?","text":"<p>We have a large backlog of pull requests to get through and the team is moving through them as quickly as we can. All pull requests must be reviewed by a HashiCorp engineer before inclusion. This is to ensure that the design of the addition fits with what provider users have come to expect, and to ensure that testing and best practices are adhered to. This is particularly important for such a large codebase, to ensure that we sustain its maintainability as it grows.</p> <p>The number one factor we look at when deciding what issues to look at are your \ud83d\udc4d reactions to the original issue/PR description as these can be easily discovered. Comments that further explain desired use cases or poor user experience are also heavily factored in. The items with the most support are always on our radar, and we commit to keeping the community updated on their status and potential timelines.</p> <p>We publish a roadmap every quarter which describes major themes or specific product areas of focus. What is excluded from the public roadmap is work performed under NDA with AWS on new services, and any ad-hoc work we pick up during the quarter. This ad-hoc work can be responding to bugs, gardening day activity, customer prioritization, and technical debt items.</p> <p>We also are investing time to improve the contributing experience by improving documentation, adding more linter coverage to ensure that incoming PRs can be in as good shape as possible. This will allow us to get through them quicker.</p>"},{"location":"faq/#my-pr-hasnt-been-merged-and-it-now-has-merge-conflictsfailed-checks-should-i-keep-it-up-to-date","title":"My PR hasn't been merged and it now has merge conflicts/failed checks, should I keep it up to date?","text":"<p>We realize that sometimes pull requests sit for a considerable amount of time without being addressed. During this time period they may accumulate merge conflicts and failed linter checks as the provider codebase moves forward. As maintainers we have no expectation that you keep your PR up to date, these issues will be addressed at review time most often by the maintainers themselves. Obviously we would hope that your PR is mergeable when first raised! The mergeability of the PR does not affect its prioritization for review.</p>"},{"location":"faq/#how-often-do-you-release","title":"How often do you release?","text":"<p>We release weekly on Thursday. We release often to ensure we can bring value to the community at a frequent cadence and to ensure we are in a good place to react to AWS region launches and service announcements.</p>"},{"location":"faq/#backward-compatibility-promise","title":"Backward Compatibility Promise","text":"<p>Our policy is described on the Terraform website here. While we do our best to prevent breaking changes until major version releases of the provider, it is generally recommended to pin the provider version in your configuration.</p> <p>Due to the constant release pace of AWS and the relatively infrequent major version releases of the provider, there can be cases where a minor version update may contain unexpected changes depending on your configuration or environment. These may include items such as a resource requiring additional IAM permissions to support newer functionality. We typically base these decisions on a pragmatic compromise between introducing a relatively minor one-time inconvenience for a subset of the community versus a better overall user experience for the entire community.</p>"},{"location":"faq/#once-a-major-release-is-published-will-new-features-and-fixes-be-backported-to-previous-versions","title":"Once a major release is published, will new features and fixes be backported to previous versions?","text":"<p>Generally, new features and fixes will only be added to the most recent major version. Due to the high-touch nature of provider development and the extensive regression testing required to ensure stability, maintaining multiple versions of the provider is not sustainable at this time. An exception to this could be a discovered security vulnerability for which backporting may be the most reasonable course of action. These would be reviewed on a case-by-case basis.</p>"},{"location":"faq/#aws-just-announced-a-new-region-when-will-i-see-it-in-the-provider","title":"AWS just announced a new region, when will I see it in the provider?","text":"<p>Normally pretty quickly. We usually see the region appear within the <code>aws-go-sdk</code> within a couple of days of the announcement. Depending on when it lands, we can often get it out within the current or following weekly release. Comparatively, adding support for a new region in the S3 backend can take a little longer, as it is shipped as part of Terraform Core and not via the AWS Provider.</p> <p>Please note that this new region requires a manual process to enable in your account. Once enabled in the console, it takes a few minutes for everything to work properly.</p> <p>If the region is not enabled properly, or the enablement process is still in progress, you may receive errors like these:</p> <pre><code>$ terraform apply\n\nError: error validating provider credentials: error calling sts:GetCallerIdentity: InvalidClientTokenId: The security token included in the request is invalid.\n    status code: 403, request id: 142f947b-b2c3-11e9-9959-c11ab17bcc63\n\n  on main.tf line 1, in provider \"aws\":\n   1: provider \"aws\" {\n</code></pre> <p>To use this new region before support has been added to the Terraform AWS Provider, you can disable the provider's automatic region validation via:</p> <pre><code>provider \"aws\" {\n  # ... potentially other configuration ...\n\nregion                 = \"af-south-1\"\nskip_region_validation = true\n}\n</code></pre>"},{"location":"faq/#how-can-i-help","title":"How can I help?","text":"<p>Great question, if you have contributed before check out issues with the <code>help-wanted</code> label. These are normally enhancement issues that will have a great impact, but the maintainers are unable to develop them in the near future. If you are just getting started, take a look at issues with the <code>good-first-issue</code> label. Items with these labels will always be given priority for response.</p> <p>Check out the Contributing Guide for additional information.</p>"},{"location":"faq/#how-can-i-become-a-maintainer","title":"How can I become a maintainer?","text":"<p>This is an area under active research. Stay tuned!</p>"},{"location":"go-vcr/","title":"Go-VCR","text":"<p>The Terraform AWS provider utilizes <code>go-vcr</code> to improve acceptance test performance and reduce costs.</p> <p><code>go-vcr</code> is a Go library for recording and replaying HTTP requests. In the context of Terraform provider acceptance testing, replaying recorded interactions allows core provider logic to be exercised without provisioning real infrastructure. The benefits are more pronounced for long-running tests1 as the built-in polling mechanisms which would typically wait for resource creation or modification can be by-passed, resulting in quicker feedback loops for developers.</p> <p>Note</p> <p>Maintainers are actively rolling out <code>go-vcr</code> support across service packages. Not all service will support recording and replaying interactions, and those that do may still have gaps for certain styles of tests. Subscribe to this meta issue for progress updates.</p>"},{"location":"go-vcr/#using-go-vcr","title":"Using <code>go-vcr</code>","text":"<p>The AWS provider supports two VCR modes - record and replay.</p> <p>To enable <code>go-vcr</code>, the <code>VCR_MODE</code> and <code>VCR_PATH</code> environment variables must both be set. The valid values for <code>VCR_MODE</code> are <code>RECORD_ONLY</code> and <code>REPLAY_ONLY</code>. <code>VCR_PATH</code> can point to any path on the local filesystem.</p> <p>Tip</p> <p>Always use the same directory for recording and replaying acceptance tests. This will maximize re-use of recorded interactions and the corresponding cost savings.</p>"},{"location":"go-vcr/#recording-tests","title":"Recording Tests","text":"<p><code>RECORD_ONLY</code> mode will intercept HTTP interactions made by the provider and write request and response data to a YAML file at the configured path. A randomness seed is also stored in a separate file, allowing for replayed interactions to generate the same resource names and appropriately match recorded interaction payloads. The file names will match the test case with a <code>.yaml</code> and <code>.seed</code> extension, respectively.</p> <p>To record tests, set <code>VCR_MODE</code> to <code>RECORD_ONLY</code> and <code>VCR_PATH</code> to the test recording directory. For example, to record Log Group resource tests in the <code>logs</code> package:</p> <pre><code>make testacc PKG=logs TESTS=TestAccLogsLogGroup_ VCR_MODE=RECORD_ONLY VCR_PATH=/path/to/testdata/ </code></pre>"},{"location":"go-vcr/#replaying-tests","title":"Replaying Tests","text":"<p><code>REPLAY_ONLY</code> mode replays recorded HTTP interactions by reading the local interaction and seed files. Each outbound request is matched with a recorded interaction based on the request headers and body. When a matching request is found, the recorded response is sent back. If no matching interaction can be found, an error is thrown and the test will fail.</p> <p>Tip</p> <p>A missing interaction likely represents a gap in <code>go-vcr</code> support. If the underlying cause is not already being tracked (check the open tasks in the meta issue) a new issue should be opened.</p> <p>To replay tests, set <code>VCR_MODE</code> to <code>REPLAY_ONLY</code> and <code>VCR_PATH</code> to the test recording directory. For example, to replay Log Group resource tests in the <code>logs</code> package:</p> <pre><code>make testacc PKG=logs TESTS=TestAccLogsLogGroup_ VCR_MODE=REPLAY_ONLY VCR_PATH=/path/to/testdata/ </code></pre>"},{"location":"go-vcr/#enabling-go-vcr","title":"Enabling <code>go-vcr</code>","text":"<p>Enabling <code>go-vcr</code> support for a service primarily involves replacing certain functions and data structures with \"VCR-aware\" equivalents. Broadly this includes service clients, acceptance test data structures, status check functionality (waiters), and any functionality which generates names.</p> <p>Semgrep rules have been written to automate the majority of these changes. The <code>vcr-enable</code> Make target will apply semgrep rules and then format code and imports for a given package.</p> <pre><code>make vcr-enable PKG=logs\n</code></pre>"},{"location":"go-vcr/#additional-changes","title":"Additional Changes","text":"<p>The changes made by semgrep may leave the code in a state which will not compile or conflicts with code generation. When this occurs some manual intervention may be required before running acceptance tests.</p>"},{"location":"go-vcr/#test-check-helper-functions","title":"Test Check Helper Functions","text":"<p>The most common manual changes required are to acceptance test check helper functions (similar to \"check exists\" or \"check destroy\", but not covered via semgrep), which might now reference a <code>*testing.T</code> argument within the function body. Adding a <code>*testing.T</code> argument to the function signature will resolve the missing reference.</p> <p>For example, this was the change applied to the <code>testAccCheckMetricFilterManyExists</code> helper function in the <code>logs</code> package:</p> <pre><code>-func testAccCheckMetricFilterManyExists(ctx context.Context, basename string, n int) resource.TestCheckFunc {\n+func testAccCheckMetricFilterManyExists(ctx context.Context, t *testing.T, basename string, n int) resource.TestCheckFunc {\n</code></pre>"},{"location":"go-vcr/#generated-tagging-tests","title":"Generated Tagging Tests","text":"<p>If the service includes resources with generated tags tests, two additional <code>@Tags</code> annotations will be required to ensure the generator does not replace the <code>*testing.T</code> argument added to the \"check exists\" and \"check destroy\" functions by semgrep. Add the following annotations to the resource definition:</p> <pre><code>// @Testing(existsTakesT=true)\n// @Testing(destroyTakesT=true)\n</code></pre>"},{"location":"go-vcr/#validating-changes","title":"Validating Changes","text":"<p>The most time consuming part of enabling <code>go-vcr</code> for a service is validating acceptance test results. The full acceptance test suite should run in <code>RECORD_ONLY</code> mode with no errors.</p> <p>There are known support gaps which may result in test failures when running in <code>REPLAY_ONLY</code> mode. This is not a blocker for enabling <code>go-vcr</code> in the service, though it is worth verifying the failures are caused by known gaps already documented in the meta-issue. A new issue should be opened for any failures that appear unrelated to those already being tracked.</p> <p>Once test validation is complete, a pull request can be opened with the changes and test results.</p> <ol> <li> <p>The full acceptance test suite for certain resources can take upwards of 4 hours to complete. These are typically resources which need to provision compute as part of their lifecycle, such as an RDS database or ElastiCache cluster.\u00a0\u21a9</p> </li> </ol>"},{"location":"id-attributes/","title":"<code>id</code> Attributes","text":""},{"location":"id-attributes/#background","title":"Background","text":"<p>Historically all resources in the Terraform AWS provider have included a read-only <code>id</code> attribute, as Terraform Plugin SDK V2 and its associated acceptance testing library require it. In most cases, this attribute corresponds to a unique identifier generated by AWS during resource creation. However, for some resources the identifier is a value provided by the user and the resulting <code>id</code> attribute inherently duplicates the value of some other required argument.</p> <p>With the general availability of Terraform Plugin Framework, the separation of the provider testing functionality into its own standalone library (<code>terraform-plugin-testing</code>), and some corresponding enhancements made to this library, resources are no longer required to have an <code>id</code> attribute.</p>"},{"location":"id-attributes/#standard-usage","title":"Standard Usage","text":"<p>As net-new resources must be implemented with Terraform Plugin Framework, the following standard should be applied.</p> <p><code>id</code> Attribute Standard</p> <p>For all net-new resources, the <code>id</code> attribute should be omitted if it is redundant with an existing argument or is a combination of multiple arguments. When the unique identifier is a combination of arguments, these should be delimited with a comma (<code>,</code>), and rely on the internal <code>ExpandResourceID</code> function to handle splitting values within the <code>ImportState</code> method. In all other cases the <code>id</code> attribute should continue to be used as it has been historically.</p>"},{"location":"id-attributes/#examples","title":"Examples","text":"<p>For resources omitting an <code>id</code> argument, minor changes are required to customize the import method and acceptance test the import operation. For testing, the import verification <code>TestStep</code> will now require the <code>ImportStateVerifyIdentifierAttribute</code> and one of <code>ImportStateID</code> or <code>ImportStateIdFunc</code> be configured. Examples for a single, non-<code>id</code> identifier and a multi-part identifier are included below.</p>"},{"location":"id-attributes/#non-id-identifier","title":"Non-<code>id</code> Identifier","text":"<p>In this example, the <code>vpc_endpoint_id</code> argument is used as the resource identifier.</p> <p><code>ImportState</code> method:</p> <pre><code>func (r *resourceEndpointPrivateDNS) ImportState(ctx context.Context, req resource.ImportStateRequest, resp *resource.ImportStateResponse) {\nresource.ImportStatePassthroughID(ctx, path.Root(\"vpc_endpoint_id\"), req, resp)\n}\n</code></pre> <p><code>TestStep</code>:</p> <pre><code>{\nResourceName:                         resourceName,\nImportState:                          true,\nImportStateIdFunc:                    testAccVPCEndpointPrivateDNSImportStateIdFunc(resourceName),\nImportStateVerify:                    true,\nImportStateVerifyIdentifierAttribute: \"vpc_endpoint_id\",\n},\n</code></pre> <p><code>ImportStateIdFunc</code>:</p> <pre><code>func testAccVPCEndpointPrivateDNSImportStateIdFunc(resourceName string) resource.ImportStateIdFunc {\nreturn func(s *terraform.State) (string, error) {\nrs, ok := s.RootModule().Resources[resourceName]\nif !ok {\nreturn \"\", fmt.Errorf(\"Not found: %s\", resourceName)\n}\n\nreturn rs.Primary.Attributes[\"vpc_endpoint_id\"], nil\n}\n}\n</code></pre>"},{"location":"id-attributes/#multi-part-identifier","title":"Multi-part Identifier","text":"<p>In this example, the resource identifier is a combination of the <code>function_name</code> and <code>qualifier</code> arguments.</p> <p><code>ImportState</code> Method:</p> <pre><code>func (r *resourceRuntimeManagementConfig) ImportState(ctx context.Context, req resource.ImportStateRequest, resp *resource.ImportStateResponse) {\nparts, err := intflex.ExpandResourceId(req.ID, runtimeManagementConfigIDParts, true)\nif err != nil {\nresp.Diagnostics.AddError(\n\"Unexpected Import Identifier\",\nfmt.Sprintf(\"Expected import identifier with format: function_name,qualifier. Got: %q\", req.ID),\n)\nreturn\n}\n\nresp.Diagnostics.Append(resp.State.SetAttribute(ctx, path.Root(\"function_name\"), parts[0])...)\nresp.Diagnostics.Append(resp.State.SetAttribute(ctx, path.Root(\"qualifier\"), parts[1])...)\n}\n</code></pre> <p><code>TestStep</code>:</p> <pre><code>{\nResourceName:                         resourceName,\nImportState:                          true,\nImportStateIdFunc:                    testAccRuntimeManagementConfigImportStateIdFunc(resourceName),\nImportStateVerify:                    true,\nImportStateVerifyIdentifierAttribute: \"function_name\",\n},\n</code></pre> <p><code>ImportStateIdFunc</code>:</p> <pre><code>func testAccRuntimeManagementConfigImportStateIdFunc(resourceName string) resource.ImportStateIdFunc {\nreturn func(s *terraform.State) (string, error) {\nrs, ok := s.RootModule().Resources[resourceName]\nif !ok {\nreturn \"\", fmt.Errorf(\"Not found: %s\", resourceName)\n}\n\nreturn fmt.Sprintf(\"%s,%s\", rs.Primary.Attributes[\"function_name\"], rs.Primary.Attributes[\"qualifier\"]), nil\n}\n}\n</code></pre>"},{"location":"issue-reporting-and-lifecycle/","title":"Issue Reporting and Lifecycle","text":""},{"location":"issue-reporting-and-lifecycle/#issue-reporting-checklists","title":"Issue Reporting Checklists","text":"<p>We welcome issues of all kinds including feature requests, bug reports, and general questions. Below you'll find checklists with guidelines for well-formed issues of each type.</p> <p>We encourage opening new issues rather than commenting on closed issues if a problem has not been completely solved or causes a regression. This ensures we are able to triage it effectively.</p>"},{"location":"issue-reporting-and-lifecycle/#bug-reports","title":"Bug Reports","text":"<ul> <li> <p>Test against the latest release: Make sure you test against the latest    released version. It is possible we already fixed the bug you're experiencing.</p> </li> <li> <p>Search for possible duplicate reports: It's helpful to keep bug    reports consolidated to one thread, so do a quick search on existing bug    reports to check if anybody else has reported the same thing. You can scope       searches by the label \"bug\" to help narrow things down.</p> </li> <li> <p>Include steps to reproduce: Provide steps to reproduce the issue,    along with your <code>.tf</code> files, with secrets removed, so we can try to    reproduce it. Without this, it makes it much harder to fix the issue.</p> </li> <li> <p>For panics, include <code>crash.log</code>: If you experienced a panic, please    create a gist of the entire generated crash log    for us to look at. Double-check check no sensitive items were in the log.</p> </li> </ul>"},{"location":"issue-reporting-and-lifecycle/#feature-requests","title":"Feature Requests","text":"<ul> <li> <p>Search for possible duplicate requests: It's helpful to keep requests    consolidated to one thread, so do a quick search on existing requests to    check if anybody else has reported the same thing. You can scope searches by       the label \"enhancement\" to help narrow things down.</p> </li> <li> <p>Include a use case description: In addition to describing the    behavior of the feature you'd like to see added, it's helpful to also lay    out the reason why the feature would be important and how it would benefit    Terraform users.</p> </li> </ul>"},{"location":"issue-reporting-and-lifecycle/#questions","title":"Questions","text":"<ul> <li>Search for answers in Terraform documentation: We're happy to answer    questions in GitHub Issues, but it helps reduce issue churn and maintainer    workload if you work to find answers to common questions in the    documentation. Oftentimes Question issues result in documentation updates    to help future users, so if you don't find an answer, you can give us    pointers for where you'd expect to see it in the docs.</li> </ul>"},{"location":"issue-reporting-and-lifecycle/#issue-lifecycle","title":"Issue Lifecycle","text":"<p>Tip</p> <p>For detailed information on how issues are prioritized, see the prioritization guide.</p> <ol> <li> <p>The issue is reported.</p> </li> <li> <p>The issue is verified and categorized by a Terraform collaborator.    Categorization is done via GitHub labels. We generally use a two-label    system of (1) issue/PR type, and (2) section of the codebase. Type is    one of \"bug\", \"enhancement\", \"documentation\", or \"question\", and section    is usually the AWS service name.</p> </li> <li> <p>An initial triage process determines whether the issue is critical and must    be addressed immediately, or can be left open for community discussion.</p> </li> <li> <p>The issue is addressed in a pull request or commit. The issue number will be    referenced in the commit message so that the code that fixes it is clearly    linked.</p> </li> <li> <p>The issue is closed. Sometimes, valid issues will be closed because they are    tracked elsewhere or non-actionable. The issue is still indexed and    available for future viewers, or can be re-opened if necessary.</p> </li> <li> <p>30 days after the issue has been closed it is locked, preventing further comments.</p> </li> </ol>"},{"location":"makefile-cheat-sheet/","title":"Makefile Cheat Sheet","text":"<p>The Terraform AWS Provider Makefile includes a lot of functionality to make working on the provider easier and more efficient. Many contributors are familiar with using the Makefile for running acceptance tests, but there is a lot more functionality hidden in this humble file.</p> <p>Tip</p> <p>See Continuous Integration for more information about the CI-focused parts of the Makefile.</p>"},{"location":"makefile-cheat-sheet/#basics","title":"Basics","text":"<p>If you're new to our Makefile, this section will bring you up to speed.</p>"},{"location":"makefile-cheat-sheet/#location","title":"Location","text":"<p>The Makefile is located in the root of the provider repository and is called GNUmakefile.</p>"},{"location":"makefile-cheat-sheet/#phony-targets","title":"Phony Targets","text":"<p>Historically, Makefiles were used to help with the complexities of compiling and linking software projects, managing dependencies, and enabling the creation of various target files. <code>make</code> would create a \"target\" file as determined by the command line:</p> <pre><code>% make &lt;target&gt;\n</code></pre> <p>Today, we use phony targets in the Makefile to automate many tasks. \"Phony\" simply means that the target doesn't define the file we're trying to make but rather the recipe we want to perform, such as running tests.</p> <p>For example, <code>testacc</code> is a phony target to simplify the command for running acceptance tests:</p> <pre><code>make testacc TESTS=TestAccIAMRole_basic PKG=iam\n</code></pre>"},{"location":"makefile-cheat-sheet/#meta-targets-and-dependent-targets","title":"Meta Targets and Dependent Targets","text":"<p>Meta targets are <code>make</code> targets that only run other targets. They aggregate the functionality of other targets for convenience. In the Cheat Sheet, meta targets are marked with M.</p> <p>Dependent targets also run other targets but, in addition, have their own functionality. A dependent target generally runs the other targets first before executing its own functionality. In the Cheat Sheet, dependent targets are marked with D.</p> <p>For example, in the cheat sheet, the <code>ci</code>, <code>clean</code>, and <code>misspell</code> targets are meta targets that only run other targets.</p> <p>On the other hand, examples of dependent targets are <code>deps-check</code>, <code>gen-check</code>, and <code>semgrep-code-quality</code>.</p> <p>When you call a meta or dependent target and it runs other targets, those targets must complete successfully in order for the target you called to succeed.</p>"},{"location":"makefile-cheat-sheet/#variables","title":"Variables","text":"<p>In the Cheat Sheet, you can see which variables affect which targets. This section describes the variables in more detail.</p> <p>Variables are often defined before the <code>make</code> call on the same line, such as <code>MY_VAR=42 make my-target</code>. However, they can also be set on the same line after the <code>make</code> call or in your environment, using, for example, <code>export MY_VAR=42</code>.</p> <p>Note</p> <p>Targets that meta and dependent targets run may not all respect the same set of variables.</p> <ul> <li><code>ACCTEST_PARALLELISM</code> - (Default: <code>20</code>) Number of concurrent acceptance tests to run. Overridden if <code>P</code> is set.</li> <li><code>ACCTEST_TIMEOUT</code> - (Default: <code>360m</code>) Timeout before acceptance tests panic.</li> <li><code>BASE_REF</code> - (Default: <code>main</code>) Origin reference to use for Git <code>diff</code> comparison, as in <code>origin/BASE_REF</code>.</li> <li><code>CURDIR</code> - (Default: Value of <code>$PWD</code>) Root path to use for <code>/.ci/scripts/</code>.</li> <li><code>GO_VER</code> - (Default: Value in <code>.go-version</code> file) Version of Go to use. To use the default version on your system, use <code>GO_VER=go</code>.</li> <li><code>K</code> - (Default: None) Name of the service package you want to use, such as <code>ec2</code>, <code>iam</code>, or <code>lambda</code>, limiting Go processing to that package and dependencies. Equivalent to <code>PKG</code> variable. Assigns values to <code>PKG_NAME</code>, <code>SVC_DIR</code>, and <code>TEST</code> overridding any values set.</li> <li><code>P</code> - (Default: <code>20</code>) Number of concurrent acceptance tests to run. Assigns a value to <code>ACCTEST_PARALLELISM</code> overridding any value set.</li> <li><code>PKG</code> - (Default: None) Name of the service package you want to use, such as <code>ec2</code>, <code>iam</code>, or <code>lambda</code>, limiting Go processing to that package and dependencies. Equivalent to <code>K</code> variable. Assigns values to <code>PKG_NAME</code>, <code>SVC_DIR</code>, and <code>TEST</code> overridding any values set.</li> <li><code>PKG_NAME</code> - (Default: <code>internal</code>) Subdirectory (Go package) to use as the basis for Go processing. Overridden if <code>PKG</code> or <code>K</code> is set.</li> <li><code>RUNARGS</code> - (Default: None) Raw arguments passed to Go when running acceptance tests. For example, <code>RUNARGS=-run=TestMyTest</code>. Overridden if <code>TESTS</code> or <code>T</code> is set.</li> <li><code>SEMGREP_ARGS</code> - (Default: <code>--error</code>) Semgrep arguments. See the Semgrep reference.</li> <li><code>SEMGREP_ENABLE_VERSION_CHECK</code> - (Default: <code>false</code>) Whether to check Semgrep servers to verify you are running the latest Semgrep version.</li> <li><code>SEMGREP_SEND_METRICS</code> - (Default: <code>off</code>) When Semgrep usage metrics are sent to Semgrep.</li> <li><code>SEMGREP_TIMEOUT</code> - (Default: <code>900</code>) Maximum time to spend running a rule on a single file, in seconds.</li> <li><code>SVC_DIR</code> - (Default: <code>./internal/service</code>) Directory to as the base for recursive processing. Overridden if <code>PKG</code> or <code>K</code> is set.</li> <li><code>SWEEP_DIR</code> - (Default: <code>./internal/sweep</code>) Location of the sweep directory.</li> <li><code>SWEEP</code> - (Default: <code>us-west-2,us-east-1,us-east-2,us-west-1</code>) Comma-separated list of AWS regions to sweep.</li> <li><code>SWEEP_TIMEOUT</code> - (Default: <code>360m</code>) Time Go will spend sweeping resources before panicking.</li> <li><code>SWEEPARGS</code> - (Default: None) Raw arguments that define what to sweep, including dependencies. Similar to <code>SWEEPERS</code>. For example, <code>SWEEPARGS=-sweep-run=aws_example_thing</code>.</li> <li><code>SWEEPERS</code> - (Default: None) Resources to sweep, including dependencies. Similar to <code>SWEEPARGS</code>. For example, <code>SWEEPERS=aws_example_thing</code>. Assigns a value to <code>SWEEPARGS</code> overridding any value set.</li> <li><code>T</code> - (Default: None) Names of tests to run. Equivalent to <code>TESTS</code>. Assigns a value to <code>RUNARGS</code> overridding any value set.</li> <li><code>TEST</code> - (Default: <code>./...</code>) Limit tests to this directory and dependencies. Overridden if <code>PKG</code> or <code>K</code> is set.</li> <li><code>TEST_COUNT</code> - (Default: <code>1</code>) Number of times to run each acceptance or unit test.</li> <li><code>TESTS</code> - (Default: None) Names of tests to run. Equivalent to <code>T</code>. Assigns a value to <code>RUNARGS</code> overridding any value set.</li> <li><code>TESTARGS</code> - (Default: None) Raw arguments passed to Go when running tests. Unlike <code>RUNARGS</code>, this is not overridden if <code>TESTS</code> or <code>T</code> is set.</li> </ul>"},{"location":"makefile-cheat-sheet/#cheat-sheet","title":"Cheat Sheet","text":"<ul> <li>Target: Use as a subcommand to <code>make</code>, such as <code>make gen</code>. Meta and dependent targets are marked with M and D, respectively.</li> <li>Description: When CI-related, this aligns with the name of the check as seen on GitHub.</li> <li>CI?: Indicates whether the target is equivalent or largely equivalent to a check run on the GitHub repository for a pull request. See continuous integration for more details.</li> <li>Legacy?: Indicates whether the target is a legacy holdover. Use caution with a legacy target! It may not work, or it may perform checks or fixes that do not align with current practices. In the future, this target should be removed, modernized, or verified to still have value.</li> <li>Vars: Variables that you can set when using the target, such as <code>MY_VAR=42 make my-target</code>. Meta and dependent targets run other targets that may not respect the same variables.</li> </ul> <p>Tip</p> <p>Makefile autocompletion works out of the box on Zsh (the default shell for Terminal on macOS) and Fish shells. For Bash, the <code>bash-completion</code> package, among others, provides Makefile autocompletion. Using autocompletion allows you, for example, to type <code>make ac</code>, press tab, and the shell autocompletes <code>make acctest-lint</code>.</p> Target Description CI? Legacy? Vars <code>acctest-lint</code>M Run all CI acceptance test checks \u2714\ufe0f <code>K</code>, <code>PKG</code>, <code>SVC_DIR</code> <code>build</code>D Build the provider <code>GO_VER</code> <code>changelog-misspell</code> CHANGELOG Misspell / misspell \u2714\ufe0f <code>ci</code>M Run all CI checks \u2714\ufe0f <code>BASE_REF</code>, <code>GO_VER</code>, <code>K</code>, <code>PKG</code>, <code>SEMGREP_ARGS</code>, <code>SVC_DIR</code>, <code>TEST</code>, <code>TESTARGS</code> <code>ci-quick</code>M Run quicker CI checks \u2714\ufe0f <code>BASE_REF</code>, <code>GO_VER</code>, <code>K</code>, <code>PKG</code>, <code>SEMGREP_ARGS</code>, <code>SVC_DIR</code>, <code>TEST</code>, <code>TESTARGS</code> <code>clean</code>M Clean up Go cache, tidy and re-install tools <code>GO_VER</code> <code>clean-go</code>D Clean up Go cache <code>GO_VER</code> <code>clean-make-tests</code> Clean up artifacts from make tests <code>clean-tidy</code>D Clean up tidy <code>GO_VER</code> <code>copyright</code> Copyright Checks / add headers check \u2714\ufe0f default = <code>build</code> <code>GO_VER</code> <code>deps-check</code>D Dependency Checks / go_mod \u2714\ufe0f <code>GO_VER</code> <code>docs</code>M Run all CI documentation checks \u2714\ufe0f <code>docs-check</code> Check provider documentation \u2714\ufe0f <code>docs-link-check</code> Documentation Checks / markdown-link-check \u2714\ufe0f <code>docs-lint</code> Lint documentation \u2714\ufe0f <code>docs-lint-fix</code> Fix documentation linter findings \u2714\ufe0f <code>docs-markdown-lint</code> Documentation Checks / markdown-lint \u2714\ufe0f <code>docs-misspell</code> Documentation Checks / misspell \u2714\ufe0f <code>examples-tflint</code> Examples Checks / tflint \u2714\ufe0f <code>fix-constants</code>M Use Semgrep to fix constants <code>K</code>, <code>PKG</code>, <code>PKG_NAME</code>, <code>SEMGREP_ARGS</code> <code>fix-imports</code> Fixing source code imports with goimports <code>fmt</code> Fix Go source formatting <code>K</code>, <code>PKG</code>, <code>PKG_NAME</code> <code>fmt-check</code> Verify Go source is formatted <code>CURDIR</code> <code>fumpt</code> Run gofumpt <code>K</code>, <code>PKG</code>, <code>PKG_NAME</code> <code>gen</code>D Run all Go generators <code>GO_VER</code> <code>gen-check</code>D Provider Checks / go_generate \u2714\ufe0f <code>generate-changelog</code> Generate changelog <code>CURDIR</code> <code>gh-workflow-lint</code> Workflow Linting / actionlint \u2714\ufe0f <code>go-build</code> Provider Checks / go-build \u2714\ufe0f <code>go-misspell</code> Provider Checks / misspell \u2714\ufe0f <code>golangci-lint</code>M All golangci-lint Checks \u2714\ufe0f <code>K</code>, <code>PKG</code>, <code>TEST</code> <code>golangci-lint1</code> golangci-lint Checks / 1 of 5 \u2714\ufe0f <code>K</code>, <code>PKG</code>, <code>TEST</code> <code>golangci-lint2</code> golangci-lint Checks / 2 of 5 \u2714\ufe0f <code>K</code>, <code>PKG</code>, <code>TEST</code> <code>golangci-lint3</code> golangci-lint Checks / 3 of 5 \u2714\ufe0f <code>K</code>, <code>PKG</code>, <code>TEST</code> <code>golangci-lint4</code> golangci-lint Checks / 4 of 5 \u2714\ufe0f <code>K</code>, <code>PKG</code>, <code>TEST</code> <code>golangci-lint5</code> golangci-lint Checks / 5 of 5 \u2714\ufe0f <code>K</code>, <code>PKG</code>, <code>TEST</code> <code>help</code> Display help <code>import-lint</code> Provider Checks / import-lint \u2714\ufe0f <code>K</code>, <code>PKG</code>, <code>TEST</code> <code>install</code>M = <code>build</code> <code>GO_VER</code> <code>lint</code>M Legacy target, use caution \u2714\ufe0f <code>lint-fix</code>M Fix acceptance test, website, and docs linter findings \u2714\ufe0f <code>misspell</code>M Run all CI misspell checks \u2714\ufe0f <code>modern-check</code> Check for modern Go \u2714\ufe0f <code>TEST</code> <code>modern-fix</code> Fix checks for modern Go \u2714\ufe0f <code>TEST</code> <code>pr-target-check</code> Pull Request Target Check \u2714\ufe0f <code>prereq-go</code> Install the project's Go version <code>GO_VER</code> <code>provider-lint</code> ProviderLint Checks / providerlint \u2714\ufe0f <code>K</code>, <code>PKG</code>, <code>SVC_DIR</code> <code>provider-markdown-lint</code> Provider Check / markdown-lint \u2714\ufe0f <code>sane</code>D Run sane check <code>ACCTEST_PARALLELISM</code>, <code>ACCTEST_TIMEOUT</code>, <code>GO_VER</code>, <code>TEST_COUNT</code> <code>sanity</code>D Run sanity check (failures allowed) <code>ACCTEST_PARALLELISM</code>, <code>ACCTEST_TIMEOUT</code>, <code>GO_VER</code>, <code>TEST_COUNT</code> <code>semgrep</code>M Run all CI Semgrep checks \u2714\ufe0f <code>K</code>, <code>PKG</code>, <code>PKG_NAME</code>, <code>SEMGREP_ARGS</code> <code>semgrep-all</code>D Run semgrep on all files <code>K</code>, <code>PKG</code>, <code>PKG_NAME</code>, <code>SEMGREP_ARGS</code> <code>semgrep-code-quality</code>D Semgrep Checks / Code Quality Scan \u2714\ufe0f <code>K</code>, <code>PKG</code>, <code>PKG_NAME</code>, <code>SEMGREP_ARGS</code> <code>semgrep-constants</code>D Fix constants with Semgrep --autofix <code>K</code>, <code>PKG</code>, <code>PKG_NAME</code>, <code>SEMGREP_ARGS</code> <code>semgrep-docker</code>D Run Semgrep \u2714\ufe0f <code>semgrep-fix</code>D Fix Semgrep issues that have fixes <code>K</code>, <code>PKG</code>, <code>PKG_NAME</code>, <code>SEMGREP_ARGS</code> <code>semgrep-naming</code>D Semgrep Checks / Test Configs Scan \u2714\ufe0f <code>K</code>, <code>PKG</code>, <code>PKG_NAME</code>, <code>SEMGREP_ARGS</code> <code>semgrep-naming-cae</code>D Semgrep Checks / Naming Scan Caps/<code>AWS</code>/EC2 \u2714\ufe0f <code>K</code>, <code>PKG</code>, <code>PKG_NAME</code>, <code>SEMGREP_ARGS</code> <code>semgrep-service-naming</code>D Semgrep Checks / Service Name Scan A-Z \u2714\ufe0f <code>K</code>, <code>PKG</code>, <code>PKG_NAME</code>, <code>SEMGREP_ARGS</code> <code>semgrep-validate</code> Validate Semgrep configuration files <code>semgrep-vcr</code> Enable VCR support with Semgrep --autofix <code>K</code>, <code>PKG</code>, <code>PKG_NAME</code>, <code>SEMGREP_ARGS</code> <code>skaff</code>D Install skaff <code>GO_VER</code> <code>skaff-check-compile</code> Skaff Checks / Compile skaff \u2714\ufe0f <code>sweep</code>D Run sweepers <code>GO_VER</code>, <code>SWEEP_DIR</code>, <code>SWEEP_TIMEOUT</code>, <code>SWEEP</code>, <code>SWEEPARGS</code> <code>sweeper</code>D Run sweepers with failures allowed <code>GO_VER</code>, <code>SWEEP_DIR</code>, <code>SWEEP_TIMEOUT</code>, <code>SWEEP</code> <code>sweeper-check</code>M Provider Checks / Sweeper Linked, Unlinked \u2714\ufe0f <code>sweeper-linked</code> Provider Checks / Sweeper Functions Linked \u2714\ufe0f <code>sweeper-unlinked</code>D Provider Checks / Sweeper Functions Not Linked \u2714\ufe0f <code>t</code>D Run acceptance tests  (similar to <code>testacc</code>) <code>ACCTEST_PARALLELISM</code>, <code>ACCTEST_TIMEOUT</code>, <code>GO_VER</code>, <code>K</code>, <code>PKG</code>, <code>PKG_NAME</code>, <code>RUNARGS</code>, <code>TEST_COUNT</code>, <code>TESTARGS</code> <code>test</code>D Run unit tests <code>GO_VER</code>, <code>K</code>, <code>PKG</code>, <code>TEST</code>, <code>TESTARGS</code> <code>test-compile</code>D Test package compilation <code>GO_VER</code>, <code>K</code>, <code>PKG</code>, <code>PKG_NAME</code>, <code>TEST</code>, <code>TESTARGS</code> <code>testacc</code>D Run acceptance tests <code>ACCTEST_PARALLELISM</code>, <code>ACCTEST_TIMEOUT</code>, <code>GO_VER</code>, <code>K</code>, <code>PKG</code>, <code>PKG_NAME</code>, <code>RUNARGS</code>, <code>TEST_COUNT</code>, <code>TESTARGS</code> <code>testacc-lint</code> Acceptance Test Linting / terrafmt \u2714\ufe0f <code>K</code>, <code>PKG</code>, <code>SVC_DIR</code> <code>testacc-lint-fix</code> Fix acceptance test linter findings <code>K</code>, <code>PKG</code>, <code>SVC_DIR</code> <code>testacc-short</code>D Run acceptace tests with the -short flag <code>ACCTEST_PARALLELISM</code>, <code>ACCTEST_TIMEOUT</code>, <code>GO_VER</code>, <code>K</code>, <code>PKG</code>, <code>PKG_NAME</code>, <code>RUNARGS</code>, <code>TEST_COUNT</code>, <code>TESTARGS</code> <code>testacc-tflint</code> Acceptance Test Linting / tflint \u2714\ufe0f <code>K</code>, <code>PKG</code>, <code>SVC_DIR</code> <code>testacc-tflint-dir</code> Run <code>tflint</code> on Terraform acceptance test directories \u2714\ufe0f <code>K</code>, <code>PKG</code>, <code>SVC_DIR</code> <code>testacc-tflint-dir-fix</code> Fix <code>tflint</code> issues in Terraform acceptance test directories \u2714\ufe0f <code>K</code>, <code>PKG</code>, <code>SVC_DIR</code> <code>testacc-tflint-embedded</code> Run <code>tflint</code> on embedded Terraform configurations \u2714\ufe0f <code>K</code>, <code>PKG</code>, <code>SVC_DIR</code> <code>tfproviderdocs</code>D Provider Checks / tfproviderdocs \u2714\ufe0f <code>tfsdk2fw</code>D Install tfsdk2fw <code>GO_VER</code> <code>tools</code>D Install tools <code>GO_VER</code> <code>ts</code>M Alias to <code>testacc-short</code> <code>vcr-enable</code> Enable Go-VCR support <code>K</code>, <code>PKG</code>, <code>PKG_NAME</code>, <code>SEMGREP_ARGS</code> <code>website</code>M Run all CI website checks \u2714\ufe0f <code>website-link-check</code> Check website links \u2714\ufe0f <code>website-link-check-ghrc</code> Check website links with ghrc \u2714\ufe0f <code>website-link-check-markdown</code> Website Checks / markdown-link-check-a-z-markdown \u2714\ufe0f <code>website-link-check-md</code> Website Checks / markdown-link-check-md \u2714\ufe0f <code>website-lint</code> Lint website files \u2714\ufe0f <code>website-lint-fix</code> Fix website linter findings \u2714\ufe0f <code>website-markdown-lint</code> Website Checks / markdown-lint \u2714\ufe0f <code>website-misspell</code> Website Checks / misspell \u2714\ufe0f <code>website-terrafmt</code> Website Checks / terrafmt \u2714\ufe0f <code>website-tflint</code> Website Checks / tflint \u2714\ufe0f <code>yamllint</code> <code>YAML</code> Linting / yamllint \u2714\ufe0f"},{"location":"naming/","title":"Naming Conventions for the AWS Provider","text":""},{"location":"naming/#service-identifier","title":"Service Identifier","text":"<p>In the AWS Provider, a service identifier should consistently identify an AWS service from code to documentation to provider used by a practitioner. Prominent places you will see service identifiers:</p> <ul> <li>The package name (e.g., <code>internal/service/&lt;serviceidentifier&gt;</code>)</li> <li>In resource and data source names (e.g., <code>aws_&lt;serviceidentifier&gt;_thing</code>)</li> <li>Documentation file names (e.g., <code>website/docs/r/&lt;serviceidentifier&gt;_thing</code>)</li> </ul> <p>Typically, choosing the AWS Provider identifier for a service is simple. AWS consistently uses one name and we use that name as the identifier. However, some services are not simple. To provide consistency, and to help contributors and practitioners know what to expect, we provide this rule for defining a service identifier:</p>"},{"location":"naming/#rule","title":"Rule","text":"<ol> <li>Determine the service package name for AWS Go SDK v2.</li> <li>Determine the AWS CLI v2 command corresponding to the service (i.e., the word following <code>aws</code> in CLI commands; e.g., for <code>aws sts get-caller-identity</code>, <code>sts</code> is the command, <code>get-caller-identity</code> is the subcommand).</li> <li>If the SDK and CLI agree, use that. If the service only exists in one, use that.</li> <li>If they differ, use the shorter of the two.</li> <li>Use lowercase letters and do not include any underscores (<code>_</code>).</li> </ol>"},{"location":"naming/#how-well-is-it-followed","title":"How Well Is It Followed?","text":"<p>With 156+ services having some level of implementation, the following is a summary of how well this rule is currently followed.</p> <p>For AWS provider service package names, only five packages violate this rule: <code>appautoscaling</code> should be <code>applicationautoscaling</code>, <code>codedeploy</code> should be <code>deploy</code>, <code>elasticsearch</code> should be <code>es</code>, <code>cloudwatchlogs</code> should be <code>logs</code>, and <code>simpledb</code> should be <code>sdb</code>.</p> <p>For the service identifiers used in resource and data source configuration names (e.g., <code>aws_acmpca_certificate_authority</code>), 32 wholly or partially violate the rule.</p> <ul> <li>EC2, ELB, ELBv2, and RDS have legacy but heavily used resources and data sources that do not or inconsistently use service identifiers.</li> <li>The remaining 28 services violate the rule in a consistent way: <code>appautoscaling</code> should be <code>applicationautoscaling</code>, <code>codedeploy</code> should be <code>deploy</code>, <code>elasticsearch</code> should be <code>es</code>, <code>cloudwatch_log</code> should be <code>logs</code>, <code>simpledb</code> should be <code>sdb</code>, <code>prometheus</code> should be <code>amp</code>, <code>api_gateway</code> should be <code>apigateway</code>, <code>cloudcontrolapi</code> should be <code>cloudcontrol</code>, <code>cognito_identity</code> should be <code>cognitoidentity</code>, <code>cognito</code> should be <code>cognitoidp</code>, <code>config</code> should be <code>configservice</code>, <code>dx</code> should be <code>directconnect</code>, <code>directory_service</code> should be <code>ds</code>, <code>elastic_beanstalk</code> should be <code>elasticbeanstalk</code>, <code>cloudwatch_event</code> should be <code>events</code>, <code>kinesis_firehose</code> should be <code>firehose</code>, <code>msk</code> should be <code>kafka</code>, <code>mskconnect</code> should be <code>kafkaconnect</code>, <code>kinesis_analytics</code> should be <code>kinesisanalytics</code>, <code>kinesis_video</code> should be <code>kinesisvideo</code>, <code>lex</code> should be <code>lexmodels</code>, <code>media_convert</code> should be <code>mediaconvert</code>, <code>media_package</code> should be <code>mediapackage</code>, <code>media_store</code> should be <code>mediastore</code>, <code>route53_resolver</code> should be <code>route53resolver</code>, relevant <code>s3</code> should be <code>s3control</code>, <code>serverlessapplicationrepository</code> should be <code>serverlessrepo</code>, and <code>service_discovery</code> should be <code>servicediscovery</code>.</li> </ul>"},{"location":"naming/#packages","title":"Packages","text":"<p>Package names are not seen or used by practitioners. However, they should still be carefully considered.</p>"},{"location":"naming/#rule_1","title":"Rule","text":"<ol> <li>For service packages (i.e., packages under <code>internal/service</code>), use the AWS Provider service identifier as the package name.</li> <li>For other packages, use a short name for the package. Common Go lengths are 3-9 characters.</li> <li>Use a descriptive name. The name should capture the key purpose of the package.</li> <li>Use lowercase letters and do not include any underscores (<code>_</code>).</li> <li>Avoid useless names like <code>helper</code>. These names convey zero information. Everything in the AWS Provider is helping something or someone do something so the name <code>helper</code> doesn't narrow down the purpose of the package within the codebase.</li> <li>Use a name that is not too narrow or too broad as Go packages should not be too big or too small. Tiny packages can be combined using a broader name encompassing both. For example, <code>verify</code> is a good name because it tells you what the package does and allows a broad set of validation, comparison, and checking functionality.</li> </ol>"},{"location":"naming/#resources-and-data-sources","title":"Resources and Data Sources","text":"<p>When creating a new resource or data source, it is important to get names right. Once practitioners rely on names, we can only change them through breaking changes. If you are unsure about what to call a resource or data source, discuss it with the community and maintainers.</p>"},{"location":"naming/#rule_2","title":"Rule","text":"<ol> <li>Follow the AWS SDK for Go v2. Almost always, the API operations make determining the name simple. For example, the Amazon CloudWatch Evidently service includes <code>CreateExperiment</code>, <code>GetExperiment</code>, <code>UpdateExperiment</code>, and <code>DeleteExperiment</code>. Thus, the resource (or data source) name is \"Experiment.\"</li> <li>Give a resource its Terraform configuration (i.e., HCL) name (e.g., <code>aws_imagebuilder_image_pipeline</code>) by joining these three parts with underscores:<ul> <li><code>aws</code> prefix</li> <li>Service identifier (service identifiers do not include underscores), all lowercase (e.g., <code>imagebuilder</code>)</li> <li>Resource (or data source) name in snake case (spaces replaced with underscores, if any), all lowercase (e.g., <code>image_pipeline</code>)</li> </ul> </li> <li>Name the main resource function <code>Resource&lt;ResourceName&gt;()</code>, with the resource name in Mixed Caps. Do not include the service name or identifier. For example, define <code>ResourceImagePipeline()</code> in a file called <code>internal/service/imagebuilder/image_pipeline.go</code>.</li> <li>Similarly, name the main data source function <code>DataSource&lt;ResourceName&gt;()</code>, with the data source name in Mixed Caps. Do not include the service name or identifier. For example, define <code>DataSourceImagePipeline()</code> in a file called <code>internal/service/imagebuilder/image_pipeline_data_source.go</code>.</li> </ol>"},{"location":"naming/#files","title":"Files","text":"<p>File names should follow Go and Markdown conventions with these additional points.</p>"},{"location":"naming/#resource-and-data-source-documentation-rule","title":"Resource and Data Source Documentation Rule","text":"<ol> <li>Resource markdown goes in the <code>website/docs/r</code> directory. Data source markdown goes in the <code>website/docs/d</code> directory.</li> <li>Use the service identifier and resource or data source name, separated by an underscore (<code>_</code>).</li> <li>All letters are lowercase.</li> <li>Use <code>.html.markdown</code> as the extension.</li> <li>Do not include \"aws\" in the name.</li> </ol> <p>A correct example is <code>accessanalyzer_analyzer.html.markdown</code>. An incorrect example is <code>service_discovery_instance.html.markdown</code> because the service identifier should not include an underscore.</p>"},{"location":"naming/#go-file-rule","title":"Go File Rule","text":"<ol> <li>Resource and data source files are in the <code>internal/service/&lt;service&gt;</code> directory.</li> <li>Do not include the service as part of the file name.</li> <li>Data sources should include <code>_data_source</code> after the data source name (e.g., <code>application_data_source.go</code>).</li> <li>Put unit and acceptance tests in a file ending with <code>_test.go</code> (e.g., <code>custom_domain_association_test.go</code>).</li> <li>Use snake case for multiword names (i.e., all letters are lowercase, words separated by underscores).</li> <li>Use the <code>.go</code> extension.</li> <li>Idiomatic names for common non-resource, non-data-source files include <code>consts.go</code> (service-wide constants), <code>find.go</code> (finders), <code>flex.go</code> (FLatteners and EXpanders), <code>generate.go</code> (directives for code generation), <code>id.go</code> (ID creators and parsers), <code>status.go</code> (status functions), <code>sweep.go</code> (sweepers), <code>tags_gen.go</code> (generated tag code), <code>validate.go</code> (validators), and <code>wait.go</code> (waiters).</li> </ol>"},{"location":"naming/#mixed-caps","title":"Mixed Caps","text":"<p>Note</p> <p>Mixed Caps is different than camel case, Pascal case, or snake case!</p> <p>Idiomatic Go uses Mixed Caps for multiword names in code. Mixed caps is similar to camel case except initialisms and abbreviations in mixed caps should be the correct, human-readable case, such as <code>VPCEndpoint</code> not <code>VpcEndpoint</code>. After all, names in code are for humans.</p> <p>An acronym such as \"VPC\" should either be all capitalized (\"VPC\") or all lowercase (\"vpc\"), never \"Vpc\" or \"vPC.\" Similarly, in mixedCaps, \"DynamoDB\" should either be \"DynamoDB\" or \"dynamoDB\", depending on whether an initial cap is needed or not, and never \"dynamoDb\" or \"DynamoDb.\"</p> <p>For more details on capitalizations we enforce with CI Semgrep tests, see the Caps List.</p>"},{"location":"naming/#rule_3","title":"Rule","text":"<ol> <li>Use mixedCaps for names (such as for functions, types, methods, variables, and constants) in the Terraform AWS Provider Go code.</li> </ol>"},{"location":"naming/#functions","title":"Functions","text":"<p>In general, follow Go best practices for good function naming. This rule is for functions defined outside of the test context (i.e., not in a file ending with <code>_test.go</code>). For test functions, see Test Support Functions or Acceptance Test Configurations below.</p>"},{"location":"naming/#rule_4","title":"Rule","text":"<ol> <li>Only export functions (capitalize) when necessary, i.e., when the function is used outside the current package, including in the <code>_test</code> (<code>.test</code>) package.</li> <li>Use MixedCaps (exported) or mixedCaps (not exported). Do not use underscores for multiwords.</li> <li>Do not include the service name in the function name. (If functions are used outside the current package, the import package clarifies a function's origin. For example, the EC2 function <code>FindVPCEndpointByID()</code> is used outside the <code>internal/service/ec2</code> package but where it is used, the call is <code>tfec2.FindVPCEndpointByID()</code>.)</li> <li>For CRUD functions for resources, use this format: <code>resource&lt;ResourceName&gt;&lt;CRUDFunction&gt;</code>. For example, <code>resourceImageRecipeUpdate()</code>, <code>resourceBaiduChannelRead()</code>.</li> <li>For data sources, for Read functions, use this format: <code>dataSource&lt;DataSourceName&gt;Read</code>. For example, <code>dataSourceBrokerRead()</code>, <code>dataSourceEngineVersionRead()</code>.</li> <li>To improve readability, consider including the resource name in helper function names that pertain only to that resource. For example, for an expander function for an \"App\" resource and a \"Campaign Hook\" expander, use <code>expandAppCampaignHook()</code>.</li> <li>Do not include \"AWS\" or \"Aws\" in the name.</li> </ol>"},{"location":"naming/#variables-and-constants","title":"Variables and Constants","text":"<p>In general, follow Go best practices for good variable and constant naming.</p>"},{"location":"naming/#rule_5","title":"Rule","text":"<ol> <li>Only export variables and constants (capitalize) when necessary, i.e., the variable or constant is used outside the current package, including in the <code>_test</code> (<code>.test</code>) package.</li> <li>Use MixedCaps (exported) or mixedCaps (not exported). Do not use underscores for multiwords.</li> <li>Do not include the service name in variable or constant names. (If variables or constants are used outside the current package, the import package clarifies its origin. For example, IAM's <code>PropagationTimeout</code> is widely used outside of IAM but each instance is through the package import alias, <code>tfiam.PropagationTimeout</code>. \"IAM\" is unnecessary in the constant name.)</li> <li>To improve readability, consider including the resource name in variable and constant names that pertain only to that resource. For example, for a string constant for a \"Role\" resource and a \"not found\" status, use <code>roleStatusNotFound</code> or <code>RoleStatusNotFound</code>, if used outside the service's package.</li> <li>Do not include \"AWS\" or \"Aws\" in the name.</li> </ol> <p>Note</p> <p>Give priority to constants from the AWS SDK for Go rather than defining new constants for the same values.</p>"},{"location":"naming/#acceptance-and-unit-tests","title":"Acceptance and Unit Tests","text":"<p>With about 6000 acceptance and unit tests, following these naming conventions is essential to organization and (human) context switching between services.</p> <p>There are three types of tests in the AWS Provider: (regular) acceptance tests, serialized acceptance tests, and unit tests. All are functions that take a variable of type <code>*testing.T</code>. Acceptance tests and unit tests have exported (i.e., capitalized) names while serialized tests do not. Serialized tests are called by another exported acceptance test, often ending with <code>_serial</code>. The majority of tests in the AWS provider are acceptance tests.</p>"},{"location":"naming/#acceptance-test-rule","title":"Acceptance Test Rule","text":"<p>Acceptance test names have a minimum of two (e.g., <code>TestAccBackupPlan_tags</code>) or a maximum of three (e.g., <code>TestAccDynamoDBTable_Replica_multiple</code>) parts, joined with underscores:</p> <ol> <li>First part: All have a prefix (i.e., <code>TestAcc</code>), service name (e.g., <code>Backup</code>, <code>DynamoDB</code>), and resource name (e.g., <code>Plan</code>, <code>Table</code>), Mixed Caps without underscores between. Do not include \"AWS\" or \"Aws\" in the name.</li> <li>Middle part (Optional): Test group (e.g., <code>Replica</code>), uppercase, Mixed Caps. Consider a metaphor where tests are chapters in a book. If it is helpful, tests can be grouped together like chapters in a book that are sometimes grouped into parts or sections of the book.</li> <li>Last part: Test identifier (e.g., <code>basic</code>, <code>tags</code>, or <code>multiple</code>), lowercase, mixedCaps). The identifier should make the test's purpose clear but be concise. For example, the identifier <code>conflictsWithCloudFrontDefaultCertificate</code> (41 characters) conveys no more information than <code>conflictDefaultCertificate</code> (26 characters), since \"CloudFront\" is implied and \"with\" is always implicit. Avoid words that convey no meaning or whose meaning is implied. For example, \"with\" (e.g., <code>_withTags</code>) is not needed because we imply the name is telling us what the test is with. <code>withTags</code> can be simplified to <code>tags</code>.</li> </ol>"},{"location":"naming/#serialized-acceptance-test-rule","title":"Serialized Acceptance Test Rule","text":"<p>The names of serialized acceptance tests follow the regular acceptance test name rule except for serialized acceptance test names:</p> <ol> <li>Start with <code>testAcc</code> instead of <code>TestAcc</code></li> <li>Do not include the name of the service (e.g., a serialized acceptance test would be called <code>testAccApp_basic</code> not <code>testAccAmplifyApp_basic</code>).</li> </ol>"},{"location":"naming/#unit-test-rule","title":"Unit Test Rule","text":"<p>Unit test names follow the same rule as acceptance test names except for unit test names:</p> <ol> <li>Start with <code>Test</code>, not <code>TestAcc</code></li> <li>Do not include the name of the service</li> <li>Usually do not have any underscores</li> <li>If they test a function, should include the function name (e.g., a unit test of <code>ExpandListener()</code> should be called <code>TestExpandListener()</code>)</li> </ol>"},{"location":"naming/#test-support-functions","title":"Test Support Functions","text":"<p>This rule is for functions defined in the test context (i.e., in a file ending with <code>_test.go</code>) that do not return a string with Terraform configuration. For non-test functions, see Functions above. Or, see Acceptance Test Configurations below.</p>"},{"location":"naming/#rule_6","title":"Rule","text":"<ol> <li>Only export functions (capitalize) when necessary, i.e., when the function is used outside the current package. This is very rare.</li> <li>Use MixedCaps (exported) or mixedCaps (not exported). Do not use underscores for multiwords.</li> <li>Do not include the service name in the function name. For example, <code>testAccCheckAMPWorkspaceExists()</code> should be named <code>testAccCheckWorkspaceExists()</code> instead, dropping the service name.</li> <li>Several types of support functions occur commonly and should follow these patterns:<ul> <li>Destroy: <code>testAccCheck&lt;Resource&gt;Destroy</code></li> <li>Disappears: <code>testAccCheck&lt;Resource&gt;Disappears</code></li> <li>Exists: <code>testAccCheck&lt;Resource&gt;Exists</code></li> <li>Not Recreated: <code>testAccCheck&lt;Resource&gt;NotRecreated</code></li> <li>PreCheck: <code>testAccPreCheck</code> (often, only one PreCheck is needed per service so no resource name is needed)</li> <li>Recreated: <code>testAccCheck&lt;Resource&gt;Recreated</code></li> </ul> </li> <li>Do not include \"AWS\" or \"Aws\" in the name.</li> </ol>"},{"location":"naming/#acceptance-test-configurations","title":"Acceptance Test Configurations","text":"<p>This rule is for functions defined in the test context (i.e., in a file ending with <code>_test.go</code>) that return a string with Terraform configuration. For test support functions, see Test Support Functions above. Or, for non-test functions, see Functions above.</p> <p>Note</p> <p>This rule is not widely used currently. However, new functions and functions you change should follow it.</p>"},{"location":"naming/#rule_7","title":"Rule","text":"<ol> <li>Only export functions (capitalize) when necessary, i.e., when the function is used outside the current package. This is very rare.</li> <li>Use MixedCaps (exported) or mixedCaps (not exported). Do not use underscores for multiwords.</li> <li>Do not include the service name in the function name.</li> <li>Follow this pattern: <code>testAccConfig&lt;Resource&gt;_&lt;TestGroup&gt;_&lt;configDescription&gt;</code><ul> <li><code>_&lt;TestGroup&gt;</code> is optional. Refer to the Acceptance Test Rule test group discussion.</li> <li>Especially when an acceptance test only uses one configuration, the <code>&lt;configDescription&gt;</code> should be the same as the test identifier discussed in the Acceptance Test Rule.</li> </ul> </li> <li>Do not include \"AWS\" or \"Aws\" in the name.</li> </ol>"},{"location":"prioritization/","title":"How We Prioritize","text":""},{"location":"prioritization/#intro","title":"Intro","text":""},{"location":"prioritization/#what-this-document-is","title":"What this document is","text":"<p>This document describes how we handle prioritization of work from a variety of input sources. Our focus is always to deliver tangible value to the practitioner on a predictable and frequent schedule, and we feel it is important to be transparent in how we weigh input in order to deliver on this goal.</p>"},{"location":"prioritization/#what-this-document-is-not","title":"What this document is not","text":"<p>Due to the variety of input sources, the scale of the provider, and resource constraints, it is impossible to give a hard number on how each of the factors outlined in this document is weighted. Instead, the goal of the document is to give a transparent, but generalized assessment of each of the sources of input so that the community has a better idea of why things are prioritized the way they are. Additional information may be found in the FAQ.</p>"},{"location":"prioritization/#prioritization","title":"Prioritization","text":"<p>We prioritize work based on a number of factors, including community feedback, issue/PR reactions, as well as the source of the request. While community feedback is heavily weighted, there are times when other factors take precedence. By their nature, some factors are less visible to the community, and so are outlined here as a way to be as transparent as possible. Each of the sources of input is detailed below.</p>"},{"location":"prioritization/#community","title":"Community","text":"<p>Our large community of practitioners is vocal and immensely productive in contributing to the provider codebase. Unfortunately, our current team capacity means that we are unable to give every issue or pull request the same level of attention. This means we need to prioritize the issues that provide the most value to the greatest number of practitioners.</p> <p>We will always focus on the issues which have the most attention. The main rubric we have for assessing community wants is GitHub reactions. In addition to reactions, we look at comments, reactions to comments, and links to additional issues and PRs to help get a more holistic view of where the community stands. We try to ensure that for the issues where we have the most community support, we are responsive to that support and attempt to give timelines wherever possible.</p>"},{"location":"prioritization/#customer","title":"Customer","text":"<p>Another source of work that must be weighted is escalations around particular feature requests and bugs from HashiCorp and AWS customers. Escalations typically come via several routes:</p> <ul> <li>Customer Support</li> <li>Sales Engineering</li> <li>AWS Solutions Architects contacting us on behalf of their clients.</li> </ul> <p>These reports flow into an internal board and are triaged weekly to determine whether the escalation request should be prioritized for an upcoming release or added to the backlog to monitor for additional community support. During triage, we verify whether a GitHub issue or PR exists for the request and will create one if it does not exist. In this way, these requests are visible to the community to some degree. An escalation coming from a customer does not necessarily guarantee that it will be prioritized over requests made by the community. Instead, we assess them based on the following rubric:</p> <ul> <li>Does the issue have considerable community support?</li> <li>Does the issue pertain to one of our Core Services?</li> </ul> <p>By weighing these factors, we can make a call to determine whether, and how it is to be prioritized.</p>"},{"location":"prioritization/#partner","title":"Partner","text":"<p>AWS Service Teams and Partner representatives regularly contact us to discuss upcoming features or new services. This work is often done under an NDA, so usually needs to be done in private. Often the ask is to enable Terraform support or an upcoming feature or service.</p> <p>As with customer escalations, a request from a partner does not necessarily mean that it will be prioritized over other efforts; capacity restraints require us to prioritize major releases or prefer offerings in line with our core services.</p>"},{"location":"prioritization/#internal","title":"Internal","text":""},{"location":"prioritization/#sdkcore-updates","title":"SDK/Core Updates","text":"<p>We endeavor to keep in step with all minor SDK releases, so these are automatically pulled in by our GitHub automation. Major releases normally include breaking changes and usually require us to bump the provider itself to a major version. We plan to make one major version change a year and try to avoid any more than that.</p>"},{"location":"prioritization/#technical-debt","title":"Technical Debt","text":"<p>We always include capacity for technical debt work in every iteration, but engineers are free to include minor tech debt work on their own recognizance. Larger items are discussed and prioritized in an internal meeting aimed at reviewing technical debt.</p>"},{"location":"prioritization/#adverse-user-experience-or-security-vulnerabilities","title":"Adverse User Experience or Security Vulnerabilities","text":"<p>Issues with the provider that provide a poor user experience (bugs, crashes), or involve a threat to security are always prioritized for inclusion. The severity of these will determine how soon they are included for release.</p>"},{"location":"provider-design/","title":"Provider Design","text":"<p>The Terraform AWS Provider follows the guidelines established in the HashiCorp Provider Design Principles. That general documentation provides many high-level design points gleaned from years of experience with Terraform's design and implementation concepts. Sections below will expand on specific design details between that documentation and this provider, while others will capture other pertinent information that may not be covered there. Other pages of the contributing guide cover implementation details such as code, testing, and documentation specifics.</p>"},{"location":"provider-design/#api-and-sdk-boundary","title":"API and SDK Boundary","text":"<p>The AWS provider implements support for the AWS service APIs using the AWS Go SDK. The API and SDK limits extend to the provider. In general, SDK operations manage the lifecycle of AWS components, such as creating, describing, updating, and deleting a database. Operations do not usually handle functionality within those components, such as executing a query on a database. If you are interested in other APIs/SDKs, we invite you to view the many Terraform Providers available, as each has a community of domain expertise.</p> <p>Some examples of functionality that is not expected in this provider:</p> <ul> <li>Raw HTTP(S) handling. See the Terraform HTTP Provider and Terraform TLS Provider instead.</li> <li>Kubernetes resource management beyond the EKS service APIs. See the Terraform Kubernetes Provider instead.</li> <li>Active Directory or other protocol clients. See the Terraform Active Directory Provider and other available providers instead.</li> <li>Functionality that requires additional software beyond the Terraform AWS Provider to be installed on the host executing Terraform. This currently includes the AWS CLI. See the Terraform External Provider and other available providers instead.</li> </ul>"},{"location":"provider-design/#infrastructure-as-code-suitability","title":"Infrastructure as Code Suitability","text":"<p>The provider maintainers' design goal is to cover as much of the AWS API as pragmatically possible. However, not every aspect of the API is compatible with an infrastructure-as-code (IaC) conception. Specifically: IaC is best suited for immutable rather than mutable infrastructure -- i.e. for resources with a single desired state described in its entirety, as opposed to resources defined via a dynamic process.</p> <p>If such limits affect you, we recommend that you open an AWS Support case and encourage others to do the same. Request that AWS components be made more self-contained and compatible with IaC. These AWS Support cases can also yield insights into the AWS service and API that are not well documented.</p>"},{"location":"provider-design/#resource-type-considerations","title":"Resource Type Considerations","text":"<p>Terraform resources work best as the smallest infrastructure blocks on which practitioners can build more complex configurations and abstractions, such as Terraform Modules. The general heuristic guiding when to implement a new Terraform resource for an aspect of AWS is whether the AWS service API provides create, read, update, and delete (CRUD) operations. However, not all AWS service API functionality falls cleanly into CRUD lifecycle management. In these situations, there is extra consideration necessary for properly mapping API operations to Terraform resources.</p> <p>This section highlights design patterns when considering an implementation within a singular Terraform resource or as separate Terraform resources.</p> <p>Please note: the overall design and implementation across all AWS functionality is federated: individual services may implement concepts and use terminology differently. As such, this guide is not exhaustive. The aim is to provide general concepts and basic terminology that point contributors in the right direction, especially in understanding previous implementations.</p>"},{"location":"provider-design/#authorization-and-acceptance-resources","title":"Authorization and Acceptance Resources","text":"<p>Some AWS services use an authorization-acceptance model for cross-account associations or access. Examples include:</p> <ul> <li>Direct Connect Association Proposals</li> <li>GuardDuty Member Invitations</li> <li>RAM Resource Share Associations</li> <li>Route 53 VPC Associations</li> <li>Security Hub Member Invitations</li> </ul> <p>Depending on the API and components, AWS uses two basic ways of creating cross-region and cross-account associations. One way is to generate an invitation (or proposal) identifier from one AWS account to another. Then in the other AWS account, that identifier is used to accept the invitation. The second way is configuring a reference to another AWS account identifier. These may not require explicit acceptance on the receiving account to finish creating the association or begin working.</p> <p>To model creating an association using an invitation or proposal, follow these guidelines.</p> <ul> <li>Follow the naming in the AWS service API to determine whether to use the term \"invitation\" or \"proposal.\"</li> <li>For the originating account, create an \"invitation\" or \"proposal\" resource. Make sure that the AWS service API has operations for creating and reading invitations.</li> <li>For the responding account, create an \"accepter\" resource. Ensure that the API has operations for accepting, reading, and rejecting invitations in the responding account. Map the operations as follows:<ul> <li>Create: Accepts the invitation.</li> <li>Read: Reads the invitation to determine its status. Note that in some APIs, invitations expire and disappear, complicating associations. If a resource does not find an invitation, the developer should implement a fallback to read the API resource associated with the invitation/proposal.</li> <li>Delete: Rejects or otherwise deletes the invitation.</li> </ul> </li> </ul> <p>To model the second type of association, implicit associations, create an \"association\" resource and, optionally, an \"authorization\" resource. Map create, read, and delete to the corresponding operations in the AWS service API.</p>"},{"location":"provider-design/#cross-service-functionality","title":"Cross-Service Functionality","text":"<p>Many AWS service APIs build on top of other AWS services. Some examples of these include:</p> <ul> <li>EKS Node Groups managing Auto Scaling Groups</li> <li>Lambda Functions managing EC2 ENIs</li> <li>Transfer Servers managing EC2 VPC Endpoints</li> </ul> <p>Some cross-service API implementations lack the management or description capabilities of the other service. The lack can make the Terraform resource implementation seem incomplete or unsuccessful in end-to-end configurations. Given the overall \u201cresources should represent a single API object\u201d goal from the HashiCorp Provider Design Principles, a resource must only communicate with a single AWS service API. As such, maintainers will not approve cross-service resources.</p> <p>The rationale behind this design decision includes the following:</p> <ul> <li>Unexpected IAM permissions being necessary for the resource. In high-security environments, all the service permissions may not be available or acceptable.</li> <li>Unexpected services generating CloudTrail logs for the resource.</li> <li>Needing extra and unexpected API endpoints configuration for organizations using custom endpoints, such as VPC endpoints.</li> <li>Unexpected changes to the AWS service internals for the cross-service implementations. Given that this functionality is not part of the primary service API, these details can change over time and may not be considered as a breaking change by the service team for an API upgrade.</li> </ul> <p>A poignant real-world example of the last point involved a Lambda resource. The resource helped clean up extra resources (ENIs) due to a common misconfiguration. Practitioners found the functionality helpful since the issue was hard to diagnose. Years later, AWS updated the Lambda API. Immediately, practitioners reported that Terraform executions were failing. Downgrading the provider was not possible since many configurations depended on recent releases. For environments running many versions behind, forcing an upgrade with the fix would likely cause unrelated and unexpected changes. In the end, HashiCorp and AWS performed a large-scale outreach to help upgrade and fix the misconfigurations. Provider maintainers and practitioners lost considerable time.</p>"},{"location":"provider-design/#data-sources","title":"Data Sources","text":"<p>A separate class of Terraform resource types are data sources. These are typically intended as a configuration method to lookup or fetch data in a read-only manner. Data sources should not have side effects on the remote system.</p> <p>When discussing data sources, they are typically classified by the intended number of return objects or data. Singular data sources represent a one-to-one lookup or data operation. Plural data sources represent a one-to-many operation.</p>"},{"location":"provider-design/#plural-data-sources","title":"Plural Data Sources","text":"<p>These data sources are intended to return zero, one, or many results, usually associated with a managed resource type. Typically results are a set unless ordering guarantees are provided by the remote system. These should be named with a plural suffix (e.g., <code>s</code> or <code>es</code>) and should not include any specific attribute in the naming (e.g., prefer <code>aws_ec2_transit_gateways</code> instead of <code>aws_ec2_transit_gateway_ids</code>).</p>"},{"location":"provider-design/#singular-data-sources","title":"Singular Data Sources","text":"<p>These data sources are intended to return one result or an error. These should not include any specific attribute in the naming (e.g., prefer <code>aws_ec2_transit_gateway</code> instead of <code>aws_ec2_transit_gateway_id</code>).</p>"},{"location":"provider-design/#iam-resource-based-policy-resources","title":"IAM Resource-Based Policy Resources","text":"<p>For some AWS components, the AWS API allows specifying an IAM resource-based policy, the IAM policy to associate with a component. Some examples include:</p> <ul> <li>ECR Repository Policies</li> <li>EFS File System Policies</li> <li>SNS Topic Policies</li> </ul> <p>Provider developers should implement this capability in a new resource rather than adding it to the associated resource. Reasons for this include:</p> <ul> <li>Many of the policies must include the ARN of the resource. Working around this requirement with custom difference handling within a self-contained resource is unnecessarily cumbersome.</li> <li>Some policies involving multiple resources need to cross-reference each other's ARNs. Without a separate resource, this introduces a configuration cycle.</li> <li>Splitting the resources allows operators to logically split their configurations into purely operational and security boundaries. This allows environments to have distinct practitioner roles and permissions for IAM versus infrastructure changes.</li> </ul> <p>One rare exception to this guideline is where the policy is required during resource creation.</p>"},{"location":"provider-design/#managing-resource-running-state","title":"Managing Resource Running State","text":"<p>The AWS API provides the ability to start, stop, enable, or disable some AWS components. Some examples include:</p> <ul> <li>Batch Job Queues</li> <li>CloudFront Distributions</li> <li>RDS DB Event Subscriptions</li> </ul> <p>In this situation, provider developers should implement this ability within the resource instead of creating a separate resource. Since a practitioner cannot practically manage interaction with a resource's states in Terraform's declarative configuration, developers should implement the state management in the resource. This design provides consistency and future-proofing even where updating a resource in the current API is not problematic.</p>"},{"location":"provider-design/#task-execution-and-waiter-resources","title":"Task Execution and Waiter Resources","text":"<p>Some AWS operations are asynchronous. Terraform requests that AWS perform a task. Initially, AWS only notifies Terraform that it received the request. Terraform then requests the status while awaiting completion. Examples of this include:</p> <ul> <li>ACM Certificate validation</li> <li>EC2 AMI copying</li> <li>RDS DB Cluster Snapshot management</li> </ul> <p>In this situation, provider developers should create a separate resource representing the task, assuming that the AWS service API provides operations to start the task and read its status. Adding the task functionality to the parent resource muddies its infrastructure-management purpose. The maintainers prefer this approach even though there is some duplication of an existing resource. For example, the provider has a resource for copying an EC2 AMI in addition to the EC2 AMI resource itself. This modularity allows practitioners to manage the result of the task resource with another resource.</p> <p>For a related consideration, see the Managing Resource Running State section.</p>"},{"location":"provider-design/#versioned-resources","title":"Versioned Resources","text":"<p>AWS supports having multiple versions of some components. Examples of this include:</p> <ul> <li>ECS Task Definitions</li> <li>Lambda Functions</li> <li>Secrets Manager Secrets</li> </ul> <p>In general, provider developers should create a separate resource to represent a single version. For example, the provider has both the <code>aws_secretsmanager_secret</code> and <code>aws_secretsmanager_secret_version</code> resources. However, in some cases, developers should handle versioning in the main resource.</p> <p>In deciding when to create a separate resource, follow these guidelines:</p> <ul> <li>If AWS necessarily creates a version when you make a new AWS component, include version handling in the same Terraform resource. Creating an AWS component with one Terraform resource and later using a different resource for updates is confusing.</li> <li>If the AWS service API allows deleting versions and practitioners want to delete versions, provider developers should implement a separate version resource.</li> <li>If the API only supports publishing new versions, either method is acceptable, however most current implementations are self-contained. Terraform's current configuration language does not natively support triggering resource updates or recreation across resources without a state value change. This can make the implementation more difficult for practitioners without special resource and configuration workarounds, such as a <code>triggers</code> attribute. If this changes in the future, then this guidance may be updated towards separate resources, following the Task Execution and Waiter Resources guidance.</li> </ul>"},{"location":"provider-design/#other-considerations","title":"Other Considerations","text":""},{"location":"provider-design/#aws-credential-exfiltration","title":"AWS Credential Exfiltration","text":"<p>In the interest of security, the maintainers will not approve data sources that provide the ability to reference or export the AWS credentials of the running provider. There are valid use cases for this information, such as executing AWS CLI calls as part of the same Terraform configuration. However, this mechanism may allow credentials to be discovered and used outside of Terraform. Some specific concerns include:</p> <ul> <li>The values may be visible in Terraform user interface output or logging, allowing anyone with a user interface or log access to see the credentials.</li> <li>The values are currently stored in plaintext in the Terraform state, allowing anyone with access to the state file or another Terraform configuration that references the state access to the credentials.</li> <li>Any new related functionality, while opt-in to implement, is also opt-in to prevent via security controls or policies. Adopting a weaker default security posture requires advance notice and prevents organizations that implement those controls from updating to a version with any such functionality.</li> </ul>"},{"location":"raising-a-pull-request/","title":"Raising a Pull Request","text":"<ol> <li> <p>Fork the GitHub repository allowing you to make the changes in your own copy of the repository.</p> </li> <li> <p>Create a branch using the following naming prefixes:</p> <ul> <li>f = feature</li> <li>b = bug fix</li> <li>d = documentation</li> <li>t = tests</li> <li>td = technical debt</li> <li>v = dependencies (\"vendoring\" previously)</li> </ul> <p>Some indicative example branch names would be <code>f-aws_emr_instance_group-refactor</code> or <code>td-staticcheck-st1008</code></p> </li> <li> <p>Make the changes you would like to include in the provider, add new tests as required, and make sure that all relevant existing tests are passing.</p> </li> <li> <p>Create a pull request. Please ensure (if possible) that the 'Allow edits from maintainers' checkbox is checked. This will allow the maintainers to make changes and merge the PR without requiring action from the contributor.    You are welcome to submit your pull request for commentary or review before    it is fully completed by creating a draft pull request.    Please include specific questions or items you'd like feedback on.</p> </li> <li> <p>Pull Request Best Practices</p> <ul> <li>Descriptive Titles:  When creating a pull request (PR), use a clear and descriptive title that highlights the primary change. If the change pertains to a specific resource or data source, include its name in the title.</li> <li>Detailed Descriptions:  Provide a comprehensive description that explains the reasoning behind the change, what was modified, and any expected changes to the user experience (if applicable).</li> <li>Focused and Manageable Scope:<ul> <li>Keep pull requests small and focused on a single change.</li> <li>For resource or data source additions, each PR should contain only one item and its corresponding tests.</li> <li>Avoid bundling multiple resources or combining service client additions with resource changes in a single PR. Such combinations are significantly harder and more time-consuming for maintainers to review.</li> </ul> </li> </ul> </li> <li> <p>Create a changelog entry following the process outlined here</p> </li> <li> <p>Once you believe your pull request is ready to be reviewed, ensure the    pull request is not a draft pull request by marking it ready for review    or removing <code>[WIP]</code> from the pull request title if necessary, and a    maintainer will review it. Follow the checklists below    to help ensure that your contribution can be easily reviewed and potentially    merged.</p> </li> <li> <p>One of Terraform's provider team members will look over your contribution and    either approve it or provide comments letting you know if there is anything    left to do. We'll try to give you the opportunity to make the required changes yourself, but in some cases, we may perform the changes ourselves if it makes sense to (minor changes, or for urgent issues).  We do our best to keep up with the volume of PRs waiting for review, but it may take some time depending on the complexity of the work.</p> </li> <li> <p>Once all outstanding comments and checklist items have been addressed, your    contribution will be merged! Merged PRs will be included in the next    Terraform release.</p> </li> <li> <p>In some cases, we might decide that a PR should be closed without merging.    We'll make sure to provide clear reasoning when this happens.</p> </li> </ol>"},{"location":"raising-a-pull-request/#go-coding-style","title":"Go Coding Style","text":"<p>All Go code is automatically checked for compliance with various linters, such as <code>gofmt</code>. These tools can be installed using the <code>GNUMakefile</code> in this repository.</p> <pre><code>make tools\n</code></pre> <p>Check your code with the linters:</p> <pre><code>make lint\n</code></pre> <p>We use Semgrep to check for other code standards. This can be run directly on the command line, i.e.,</p> <pre><code>semgrep\n</code></pre> <p>or it can be run using Docker via the Makefile, i.e.,</p> <pre><code>make semgrep\n</code></pre> <p><code>gofmt</code> will also fix many simple formatting issues for you. The Makefile includes a target for this:</p> <pre><code>make fmt\n</code></pre> <p>The import statement in a Go file follows these rules (see #15903):</p> <ol> <li>Import declarations are grouped into a maximum of three groups in the following order:<ul> <li>Standard packages (also called short import path or built-in packages)</li> <li>Third-party packages (also called long import path packages)</li> <li>Local packages</li> </ul> </li> <li>Groups are separated by a single blank line</li> <li>Packages within each group are alphabetized</li> </ol> <p>Check your imports:</p> <pre><code>make import-lint\n</code></pre> <p>For greater detail, the following Go language resources provide common coding preferences that may be referenced during review, if not automatically handled by the project's linting tools.</p> <ul> <li>Effective Go</li> <li>Go Code Review Comments</li> </ul>"},{"location":"raising-a-pull-request/#resource-contribution-guidelines","title":"Resource Contribution Guidelines","text":"<p>The following resource checks need to be addressed before your contribution can be merged. The exclusion of any applicable check may result in a delayed time to merge. Some of these are not handled by the automated code testing that occurs during submission, so reviewers (even those outside the maintainers) are encouraged to reach out to contributors about any issues to save time.</p> <p>This Contribution Guide also includes separate sections on topics such as Error Handling, which also applies to contributions.</p> <ul> <li>Passes Testing: All code and documentation changes must pass unit testing, code linting, and website link testing. Resource code changes must pass all acceptance testing for the resource.</li> <li>Avoids API Calls Across Account, Region, and Service Boundaries: Resources should not implement cross-account, cross-region, or cross-service API calls.</li> <li>Does Not Set Optional or Required for Non-Configurable Attributes: Resource schema definitions for read-only attributes must not include <code>Optional: true</code> or <code>Required: true</code>.</li> <li>Avoids tfresource.Retry() without tfresource.RetryableError(): Resource logic should only implement <code>tfresource.Retry()</code> if there is a retryable condition (e.g., <code>return tfresource.RetryableError(err)</code>).</li> <li>Avoids Reusing Resource Read Function in Data Source Read Function: Data sources should fully implement their own resource <code>Read</code> functionality.</li> <li>Avoids Reading Schema Structure in Resource Code: The resource <code>Schema</code> should not be read in resource <code>Create</code>/<code>Read</code>/<code>Update</code>/<code>Delete</code> functions to perform looping or otherwise complex attribute logic. Use <code>d.Get()</code> and <code>d.Set()</code> directly with individual attributes instead.</li> <li>Avoids ResourceData.GetOkExists(): Resource logic should avoid using <code>ResourceData.GetOkExists()</code> as its expected functionality is not guaranteed in all scenarios.</li> <li>Calls Read After Create and Update: Except where API eventual consistency prohibits immediate reading of resources or updated attributes,  resource <code>Create</code> and <code>Update</code> functions should return the resource <code>Read</code> function.</li> <li>Implements Immediate Resource ID Set During Create: Immediately after calling the API creation function, the resource ID should be set with <code>d.SetId()</code> before other API operations or returning the <code>Read</code> function.</li> <li>Implements Attribute Refreshes During Read: All attributes available in the API should have <code>d.Set()</code> called their values in the Terraform state during the <code>Read</code> function.</li> <li>Performs Error Checks with Non-Primitive Attribute Refreshes: When using <code>d.Set()</code> with non-primitive types (<code>schema.TypeList</code>, <code>schema.TypeSet</code>, or <code>schema.TypeMap</code>), perform error checking to prevent issues where the code is not properly able to refresh the Terraform state.</li> <li>Implements Import Acceptance Testing and Documentation: Support for resource import (<code>Importer</code> in resource schema) must include <code>ImportState</code> acceptance testing (see also the Acceptance Testing Guidelines) and <code>## Import</code> section in resource documentation.</li> <li>Implements Customizable Timeouts Documentation: Support for customizable timeouts (<code>Timeouts</code> in resource schema) must include <code>## Timeouts</code> section in resource documentation.</li> <li>Implements State Migration When Adding New Virtual Attribute: For new \"virtual\" attributes (those only in Terraform and not in the API), the schema should implement State Migration to prevent differences for existing configurations that upgrade.</li> <li>Uses AWS Go SDK Constants: Many AWS services provide string constants for value enumerations, error codes, and status types. See also the \"Constants\" sections under each of the service packages in the AWS Go SDK documentation.</li> <li>Uses AWS Go SDK Pointer Conversion Functions: Many APIs return pointer types and these functions return the zero value for the type if the pointer is <code>nil</code>. This prevents potential panics from unchecked <code>*</code> pointer dereferences and can eliminate boilerplate <code>nil</code> checking in many cases. See also the <code>aws</code> package in the AWS Go SDK documentation.</li> <li>Uses AWS Go SDK Types: Use available SDK structs instead of implementing custom types with indirection.</li> <li>Uses Existing Validation Functions: Schema definitions including <code>ValidateFunc</code> for attribute validation should use available Terraform <code>helper/validation</code> package functions. <code>All()</code>/<code>Any()</code> can be used for combining multiple validation function behaviors.</li> <li>Uses tfresource.TimedOut() with retry.Retry(): Resource logic implementing <code>retry.Retry()</code> should error check with <code>tfresource.TimedOut(err error)</code> and potentially unset the error before returning the error. For example:</li> </ul> <pre><code>var output *kms.CreateKeyOutput\nerr := retry.Retry(1*time.Minute, func() *retry.RetryError {\nvar err error\n\noutput, err = conn.CreateKey(input)\n\n/* ... */\n\nreturn nil\n})\n\nif tfresource.TimedOut(err) {\noutput, err = conn.CreateKey(input)\n}\n\nif err != nil {\nreturn fmt.Errorf(\"creating KMS External Key: %s\", err)\n}\n</code></pre> <ul> <li>Uses id.UniqueId(): API fields for concurrency protection such as <code>CallerReference</code> and <code>IdempotencyToken</code> should use <code>id.UniqueId()</code>. The implementation includes a monotonic counter which is safer for concurrent operations than solutions such as <code>time.Now()</code>.</li> <li>Skips id Attribute: The <code>id</code> attribute is implicit for all Terraform resources and does not need to be defined in the schema.</li> </ul> <p>The below are style-based items that may be noted during review and are recommended for simplicity, consistency, and quality assurance:</p> <ul> <li>Implements arn Attribute: APIs that return an ARN should implement <code>arn</code> as an attribute. Alternatively, the ARN can be synthesized using the AWS Go SDK <code>arn.ARN</code> structure. For example:</li> </ul> <pre><code>// Direct Connect Virtual Interface ARN.\n// See https://docs.aws.amazon.com/directconnect/latest/UserGuide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-resources.\narn := arn.ARN{\nPartition: meta.(*conns.AWSClient).Partition,\nRegion:    meta.(*conns.AWSClient).Region,\nService:   \"directconnect\",\nAccountID: meta.(*conns.AWSClient).AccountID,\nResource:  fmt.Sprintf(\"dxvif/%s\", d.Id()),\n}.String()\nd.Set(\"arn\", arn)\n</code></pre> <p>When the <code>arn</code> attribute is synthesized this way, add the resource to the list of those affected by the provider's <code>skip_requesting_account_id</code> attribute.</p> <ul> <li>Implements Warning Logging With Resource State Removal: If a resource is removed outside of Terraform (e.g., via a different tool, API, or web UI), <code>d.SetId(\"\")</code> and <code>return nil</code> can be used in the resource <code>Read</code> function to trigger resource recreation. When this occurs, a warning log message should be printed beforehand: <code>log.Printf(\"[WARN] {SERVICE} {THING} (%s) not found, removing from state\", d.Id())</code></li> <li>Uses American English for Attribute Naming: For any ambiguity with attribute naming, prefer American English over British English. e.g., <code>color</code> without the British <code>u</code>.</li> <li>Skips Timestamp Attributes: Generally, creation and modification dates from the API should be omitted from the schema.</li> <li>Uses Paginated AWS Go SDK Functions When Iterating Over a Collection of Objects: When the API for listing a collection of objects provides a paginated function, use it instead of looping until the next page token is not set. For example, with the EC2 API, <code>DescribeInstancesPages</code> should be used instead of <code>DescribeInstances</code> when more than one result is expected.</li> <li>Adds Paginated Functions Missing from the AWS Go SDK to Internal Service Package: If the AWS Go SDK does not define a paginated equivalent for a function to list a collection of objects, it should be added to a per-service internal package using the <code>listpages</code> generator. A support case should also be opened with AWS to have the paginated functions added to the AWS Go SDK.</li> </ul>"},{"location":"regular-expressions/","title":"Using Regular Expressions","text":"<p>Regular expressions are a powerful tool. However, they are also very expensive in terms of memory. Ensuring correct and useful functionality is the priority but we have a few tips to minimize impact without affecting capabilities.</p> <ul> <li>Consider non-regular expressions options. <code>strings.Contains()</code>, <code>strings.Replace()</code>, and <code>strings.ReplaceAll()</code> are dramatically faster and less memory intensive than regular expressions. If one of these will work equally well, use the non-regular expression option.</li> <li> <p>Order character classes consistently. We use regular expression caching to reduce our memory footprint. This is more effective if character classes are consistently ordered. Since a character class is a set, order does not affect functionality. We have many equivalent regular expressions that only differ by character class order. Below is the order we recommend for consistency:</p> <ol> <li>Numeric range, i.e., digits (e.g., <code>0-9</code>)</li> <li>Uppercase alphabetic range (e.g., <code>A-Z</code>, <code>A-F</code>)</li> <li>Lowercase alphabetic range (e.g., <code>a-z</code>, <code>a-f</code>)</li> <li>Underscore (<code>_</code>)</li> <li>Everything else (except dash, <code>-</code>) in ASCII order: <code>\\t\\n\\r !\"#$%&amp;()*+,./0123456789:;&lt;=&gt;?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^abcdefghijklmnopqrstuvwxyz{|}~</code></li> <li> <p>Last, dash (<code>-</code>)</p> <p>For example, consider the following expressions which are equivalent but vary character class ordering:</p> <pre><code>`[_a-zA-Z0-9-,.]` // wrong ordering\n`[0-9A-Za-z_,.-]` // correct\n</code></pre> <pre><code>`[;a-z0-9]` // wrong ordering\n`[0-9a-z;]` // correct\n</code></pre> </li> </ol> </li> <li> <p>Inside character classes, avoid unnecessary character escaping. Go does not complain about extra character escaping but avoid it to improve cache performance. Inside a character class, most characters do not need to be escaped, as Go assumes you mean the literal character.</p> <ul> <li>These characters which normally have special meaning in regular expressions, inside character classes do not need to be escaped: <code>$</code>, <code>(</code>, <code>)</code>, <code>*</code>, <code>+</code>, <code>.</code>, <code>?</code>, <code>^</code>, <code>{</code>, <code>|</code>, <code>}</code>.</li> <li>Dash (<code>-</code>), when it is last in the character class or otherwise unambiguously not part of a range, does not need to be escaped. If in doubt, place the dash last in the character class (e.g., <code>[a-c-]</code>) or escape the dash (e.g., <code>\\-</code>).</li> <li> <p>Angle brackets (<code>[</code>, <code>]</code>) always need to be escaped in a character class.</p> <p>For example, consider the following expressions which are equivalent but include unnecessary character escapes:</p> <pre><code>`[\\$\\(\\.\\?\\|]` // unnecessary escapes\n`[$(.?|]`      // correct\n</code></pre> <pre><code>`[a-z\\-0-9_A-Z\\.]` // unnecessary escapes, wrong order\n`[0-9A-Za-z_.-]`   // correct\n</code></pre> </li> </ul> </li> </ul>"},{"location":"resource-filtering/","title":"Resource Filtering","text":""},{"location":"resource-filtering/#adding-resource-filtering-support","title":"Adding Resource Filtering Support","text":"<p>AWS provides server-side filtering across many services and resources, which can be used when listing resources of that type, for example in the implementation of a data source. See the EC2 Listing and filtering your resources page for information about how server-side filtering can be used with EC2 resources.</p> <p>To determine if the supporting AWS API supports this functionality:</p> <ul> <li>Open the AWS Go SDK documentation for the service, e.g., for <code>service/rds</code>. Note: there can be a delay between the AWS announcement and the updated AWS Go SDK documentation.</li> <li>Determine if the service API includes functionality for filtering resources (usually a <code>Filters</code> argument to a <code>DescribeThing</code> API call).</li> </ul> <p>Implementing server-side filtering support for Terraform AWS Provider resources requires the following, each with its own section below:</p> <ul> <li>Generated Service Filtering Code: In the internal code generators (e.g., <code>internal/generate/namevaluesfilters</code>), implementation and customization of how a service handles filtering, which is standardized for the resources.</li> <li>Resource Filtering Code Implementation: In the resource's equivalent data source code (e.g., <code>internal/service/{servicename}/thing_data_source.go</code>), implementation of <code>filter</code> schema attribute, along with handling in the <code>Read</code> function.</li> <li>Resource Filtering Documentation Implementation: In the resource's equivalent data source documentation (e.g., <code>website/docs/d/service_thing.html.markdown</code>), addition of <code>filter</code> argument</li> </ul>"},{"location":"resource-filtering/#adding-service-to-filter-generating-code","title":"Adding Service to Filter Generating Code","text":"<p>This step is only necessary for the first implementation and may have been previously completed. If so, move on to the next section.</p> <p>More details about this code generation can be found in the namevaluesfilters documentation.</p> <p>Add the AWS Go SDK service name (e.g., <code>rds</code>) to <code>sliceServiceNames</code> in <code>internal/generate/namevaluesfilters/generators/servicefilters/main.go</code>.</p> <ul> <li>Run <code>make gen</code> (<code>go generate ./...</code>) and ensure there are no errors via <code>make test</code> (<code>go test ./...</code>)</li> </ul>"},{"location":"resource-filtering/#resource-filter-code-implementation","title":"Resource Filter Code Implementation","text":"<ul> <li>In the resource's equivalent data source Go file (e.g., <code>internal/service/ec2/internet_gateway_data_source.go</code>), add the following Go import: <code>\"github.com/hashicorp/terraform-provider-aws/internal/generate/namevaluesfilters\"</code></li> <li>In the resource schema, add <code>\"filter\": namevaluesfilters.Schema(),</code></li> <li>Implement the logic to build the list of filters:</li> </ul> Terraform Plugin SDK V2 <pre><code>input := ec2.DescribeInternetGatewaysInput{}\n\n// Filters based on attributes.\nfilters := namevaluesfilters.New(map[string]string{\n\"internet-gateway-id\": d.Get(\"internet_gateway_id\").(string),\n})\n// Add filters based on key-value tags (N.B. Not applicable to all AWS services that support filtering)\nfilters.Add(namevaluesfilters.EC2Tags(keyvaluetags.New(d.Get(\"tags\").(map[string]interface{})).IgnoreAWS().IgnoreConfig(ignoreTagsConfig).Map()))\n// Add filters based on the custom filtering \"filter\" attribute.\nfilters.Add(d.Get(\"filter\").(*schema.Set))\n\ninput.Filters = filters.EC2Filters()\n</code></pre>"},{"location":"resource-filtering/#resource-filtering-documentation-implementation","title":"Resource Filtering Documentation Implementation","text":"<ul> <li>In the resource's equivalent data source documentation (e.g., <code>website/docs/d/internet_gateway.html.markdown</code>), add the following to the arguments reference:</li> </ul> <pre><code>* `filter` - (Optional) Custom filter block as described below.\n\nMore complex filters can be expressed using one or more `filter` sub-blocks, which take the following arguments:\n\n* `name` - (Required) Name of the field to filter by, as defined by\n  [the underlying AWS API](https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeInternetGateways.html).\n\n* `values` - (Required) Set of values that are accepted for the given field.\n  An Internet Gateway will be selected if any one of the given values matches.\n</code></pre>"},{"location":"resource-name-generation/","title":"Resource Name Generation","text":""},{"location":"resource-name-generation/#adding-resource-name-generation-support","title":"Adding Resource Name Generation Support","text":"<p>Terraform AWS Provider resources can use shared logic to support and test name generation, where the operator can choose between an expected naming value, a generated naming value with a prefix, or a fully generated name.</p> <p>Implementing name generation requires modifying the following:</p> <ul> <li>Resource Code: In the resource code, add a <code>name_prefix</code> attribute, along with handling in the <code>Create</code> function.</li> <li>Resource Acceptance Tests: In the resource acceptance tests, add new acceptance test functions and configurations to exercise the naming logic.</li> <li>Resource Documentation: In the resource documentation, add the <code>name_prefix</code> argument and update the <code>name</code> argument description.</li> </ul>"},{"location":"resource-name-generation/#resource-code","title":"Resource Code","text":"<ul> <li>In the resource file (e.g., <code>internal/service/{service}/{thing}.go</code>), add the following import: <code>\"github.com/hashicorp/terraform-provider-aws/internal/create\"</code>.</li> <li>Inside the resource schema, add the new <code>name_prefix</code> attribute and adjust the <code>name</code> attribute to be <code>Optional</code>, <code>Computed</code>, and conflict with the <code>name_prefix</code> attribute. Be sure to keep any existing validation functions already present on the <code>name</code>.</li> </ul> Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <pre><code>\"name\": schema.StringAttribute{\nOptional: true\nComputed: true,\nPlanModifiers: []planmodifier.String{\nstringplanmodifier.UseStateForUnknown(),\nstringplanmodifier.RequiresReplace(),\n},\nValidators: append(\nstringvalidator.ExactlyOneOf(\npath.MatchRelative().AtParent().AtName(\"name\"),\npath.MatchRelative().AtParent().AtName(\"name_prefix\"),\n),\n),\n},\n\"name_prefix\": schema.StringAttribute{\nOptional:   true,\nComputed:   true,\nPlanModifiers: []planmodifier.String{\nstringplanmodifier.UseStateForUnknown(),\nstringplanmodifier.RequiresReplace(),\n},\n},\n</code></pre> <pre><code>\"name\": {\nType:          schema.TypeString,\nOptional:      true,\nComputed:      true,\nForceNew:      true,\nConflictsWith: []string{\"name_prefix\"},\n},\n\"name_prefix\": {\nType:          schema.TypeString,\nOptional:      true,\nComputed:      true,\nForceNew:      true,\nConflictsWith: []string{\"name\"},\n},\n</code></pre> <ul> <li>In the resource <code>Create</code> function, make use of the <code>create.Name()</code> function.</li> </ul> Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <pre><code>name := create.Name(plan.Name.ValueString(), plan.NamePrefix.ValueString())\n\n// ... in AWS Go SDK V2 Input types, etc. use aws.ToString(name)\n</code></pre> <pre><code>name := create.Name(d.Get(\"name\").(string), d.Get(\"name_prefix\").(string))\n\n// ... in AWS Go SDK V2 Input types, etc. use aws.ToString(name)\n</code></pre> <ul> <li>If the resource supports import, set both <code>name</code> and <code>name_prefix</code> in the resource <code>Read</code> function.</li> </ul> Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <pre><code>state.Name = flex.StringToFramework(ctx, resp.Name)\nstate.NamePrefix = create.NamePrefixFromName(flex.StringToFramework(ctx, resp.Name))\n</code></pre> <pre><code>d.Set(\"name\", resp.Name)\nd.Set(\"name_prefix\", create.NamePrefixFromName(aws.StringValue(resp.Name)))\n</code></pre>"},{"location":"resource-name-generation/#resource-acceptance-tests","title":"Resource Acceptance Tests","text":"<ul> <li>In the resource test file (e.g., <code>internal/service/{service}/{thing}_test.go</code>), add the following import: <code>\"github.com/hashicorp/terraform-provider-aws/internal/create\"</code>.</li> <li>Implement two new tests named <code>_nameGenerated</code> and <code>_namePrefix</code> which verify the creation of the resource without <code>name</code> and <code>name_prefix</code> arguments, and with only the <code>name_prefix</code> argument, respectively.</li> </ul> <pre><code>func TestAccServiceThing_nameGenerated(t *testing.T) {\nctx := acctest.Context(t)\nvar thing service.ServiceThing\nresourceName := \"aws_service_thing.test\"\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck:                 func() { acctest.PreCheck(ctx, t) },\nErrorCheck:               acctest.ErrorCheck(t, names.ServiceServiceID),\nProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories,\nCheckDestroy:             testAccCheckThingDestroy(ctx),\nSteps: []resource.TestStep{\n{\nConfig: testAccThingConfig_nameGenerated(),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheckThingExists(ctx, resourceName, &amp;thing),\nacctest.CheckResourceAttrNameGenerated(resourceName, \"name\"),\nresource.TestCheckResourceAttr(resourceName, \"name_prefix\", id.UniqueIdPrefix),\n),\n},\n// If the resource supports import:\n{\nResourceName:      resourceName,\nImportState:       true,\nImportStateVerify: true,\n},\n},\n})\n}\n\nfunc TestAccServiceThing_namePrefix(t *testing.T) {\nctx := acctest.Context(t)\nvar thing service.ServiceThing\nresourceName := \"aws_service_thing.test\"\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck:                 func() { acctest.PreCheck(ctx, t) },\nErrorCheck:               acctest.ErrorCheck(t, names.ServiceServiceID),\nProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories,\nCheckDestroy:             testAccCheckThingDestroy(ctx),\nSteps: []resource.TestStep{\n{\nConfig: testAccThingConfig_namePrefix(\"tf-acc-test-prefix-\"),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheckThingExists(ctx, resourceName, &amp;thing),\nacctest.CheckResourceAttrNameFromPrefix(resourceName, \"name\", \"tf-acc-test-prefix-\"),\nresource.TestCheckResourceAttr(resourceName, \"name_prefix\", \"tf-acc-test-prefix-\"),\n),\n},\n// If the resource supports import:\n{\nResourceName:      resourceName,\nImportState:       true,\nImportStateVerify: true,\n},\n},\n})\n}\n\nfunc testAccThingConfig_nameGenerated() string {\nreturn fmt.Sprintf(`\nresource \"aws_service_thing\" \"test\" {\n  # ... other configuration ...\n}\n`)\n}\n\nfunc testAccThingConfig_namePrefix(namePrefix string) string {\nreturn fmt.Sprintf(`\nresource \"aws_service_thing\" \"test\" {\n  # ... other configuration ...\n\n  name_prefix = %[1]q\n}\n`, namePrefix)\n}\n</code></pre>"},{"location":"resource-name-generation/#resource-documentation","title":"Resource Documentation","text":"<ul> <li>In the resource documentation (e.g., <code>website/docs/r/{service}_{thing}.html.markdown</code>), add the following to the arguments reference.</li> </ul> <pre><code>* `name_prefix` - (Optional) Creates a unique name beginning with the specified prefix. Conflicts with `name`.\n</code></pre> <ul> <li>Adjust the existing <code>name</code> argument description to ensure it is denoted as <code>Optional</code>, mention that it can be generated and that it conflicts with <code>name_prefix</code>.</li> </ul> <pre><code>* `name` - (Optional) Name of the thing. If omitted, Terraform will assign a random, unique name. Conflicts with `name_prefix`.\n</code></pre>"},{"location":"resource-tagging/","title":"Resource Tagging","text":""},{"location":"resource-tagging/#adding-resource-tagging-support","title":"Adding Resource Tagging Support","text":"<p>AWS provides key-value metadata across many services and resources, which can be used for a variety of use cases including billing, ownership, and more. See the AWS Tagging Strategy page for more information about tagging at a high level.</p> <p>The Terraform AWS Provider supports default tags configured on the provider in addition to tags configured on the resource. Implementing tagging support for Terraform AWS Provider resources requires the following, each with its own section below:</p> <ul> <li>Generated Service Tagging Code: Each service has a <code>generate.go</code> file where generator directives live.   Through these directives and their flags, you can customize code generation for the service.   You can find the code that the tagging generator generates in a <code>tags_gen.go</code> file in a service, such as <code>internal/service/ec2/tags_gen.go</code>.   You should generally not need to edit the generator code itself (i.e., in <code>internal/generate/tags</code>).</li> <li>Resource Code: In the resource code, add the <code>tags</code> and <code>tags_all</code> schema attributes,   along with a plan modification in the resource definition, and handling in <code>Create</code>, <code>Read</code>, and <code>Update</code> functions.</li> <li>Resource Acceptance Tests: In the resource acceptance tests, add new acceptance test functions and configurations to exercise the new tagging logic.</li> <li>Resource Documentation: In the resource documentation, add the <code>tags</code> argument and <code>tags_all</code> attribute.</li> </ul>"},{"location":"resource-tagging/#generating-tag-code-for-a-service","title":"Generating Tag Code for a Service","text":"<p>This step is generally only necessary for the first implementation and may have been previously completed.</p> <p>More details about this code generation, including fixes for potential error messages in this process, can be found in the <code>generate</code> package documentation.</p> <p>The generator will create several types of tagging-related code. All services that support tagging will generate the function <code>keyValueTags</code>, which converts from service-specific structs returned by the AWS SDK into a common format used by the provider, and the function <code>svcTags</code>, which converts from the common format back to the service-specific structs. In addition, many services have separate functions to list or update tags, so the corresponding <code>listTags</code> and <code>updateTags</code> can be generated. Optionally, to retrieve a specific tag, you can generate the <code>GetTag</code> function.</p> <p>If the service directory does not contain a <code>generate.go</code> file, create one. This file must only contain generate directives and a package declaration (e.g., <code>package eks</code>). For examples of the <code>generate.go</code> file, many service directories contain one, e.g., <code>internal/service/eks/generate.go</code>.</p> <p>If the <code>generate.go</code> file does not contain a generate directive for tagging code, i.e., <code>//go:generate go run ../../generate/tags/main.go</code>, add it. Note that without flags, the directive itself will not do anything useful. You must not include more than one <code>generate/tags/main.go</code> directive, as subsequent directives will overwrite previous directives. To generate multiple types of tag code, use multiple flags with the directive.</p>"},{"location":"resource-tagging/#generating-tagging-types","title":"Generating Tagging Types","text":"<p>Determine how the service implements tagging: Some services will use a simple map style (<code>map[string]*string</code> in Go), while others will have a separate structure, often a <code>[]service.Tag</code> struct with <code>Key</code> and <code>Value</code> fields.</p> <p>If the service uses the simple map style, pass the flag <code>-ServiceTagsMap</code>.</p> <p>If the service uses a slice of structs, pass the flag <code>-ServiceTagsSlice</code>. If the name of the tag struct is not <code>Tag</code>, pass the flag <code>-TagType=&lt;struct name&gt;</code>. Note that the struct name is used without the package name. For example, the AppMesh service uses the struct <code>TagRef</code>, so the flag is <code>-TagType=TagRef</code>. If the key and value fields on the struct are not <code>Key</code> and <code>Value</code>, specify the names using the flags <code>-TagTypeKeyElem</code> and <code>-TagTypeValElem</code> respectively. For example, the KMS service uses the struct <code>Tag</code>, but the key and value fields are <code>TagKey</code> and <code>TagValue</code>, so the flags are <code>-TagTypeKeyElem=TagKey</code> and <code>-TagTypeValElem=TagValue</code>.</p> <p>Some services, such as EC2 and Auto Scaling, return a different type depending on the API call used to retrieve the tag. To indicate the additional type, include the flag <code>-TagType2=&lt;struct name&gt;</code>. For example, the Auto Scaling uses the struct <code>Tag</code> as part of resource calls, but returns the struct <code>TagDescription</code> from the <code>DescribeTags</code> API call. The flag used is <code>-TagType2=TagDescription</code>.</p> <p>For more details on flags for generating service keys, see the documentation for the tag generator</p>"},{"location":"resource-tagging/#generating-standalone-tag-listing-functions","title":"Generating Standalone Tag Listing Functions","text":"<p>If the service API uses a standalone function to retrieve tags instead of including them with the resource (usually a <code>ListTags</code> or <code>ListTagsForResource</code> API call), pass the flag <code>-ListTags</code>.</p> <p>If the API call is not <code>ListTagsForResource</code>, pass the flag <code>-ListTagsOp=&lt;API call name&gt;</code>. Note that this does not include the package name. For example, the Auto Scaling service uses the API call <code>DescribeTags</code>, so the flag is <code>-ListTagsOp=DescribeTags</code>.</p> <p>If the API call uses a field other than <code>ResourceArn</code> to identify the resource, pass the flag <code>-ListTagsInIDElem=&lt;field name&gt;</code>. For example, the CloudWatch service uses the field <code>ResourceARN</code>, so the flag is <code>-ListTagsInIDElem=ResourceARN</code>. Some API calls take a slice of identifiers instead of a single identifier. In this case, pass the flag <code>-ListTagsInIDNeedSlice=yes</code>.</p> <p>If the field containing the tags in the result of the API call is not named <code>Tags</code>, pass the flag <code>-ListTagsOutTagsElem=&lt;struct name&gt;</code>. For example, the CloudTrail service returns a nested structure, where the resulting flag is <code>-ListTagsOutTagsElem=ResourceTagList[0].TagsList</code>.</p> <p>In some cases, it can be useful to retrieve single tags. Pass the flag <code>-GetTag</code> to generate a function to do so.</p> <p>For more details on flags for generating tag listing functions, see the documentation for the tag generator</p>"},{"location":"resource-tagging/#generating-standalone-tag-updating-functions","title":"Generating Standalone Tag Updating Functions","text":"<p>If the service API uses a standalone function to update tags instead of including them when updating the resource (usually a <code>TagResource</code> and <code>UntagResource</code> API call), pass the flag <code>-UpdateTags</code>.</p> <p>If the API call to add tags is not <code>TagResource</code>, pass the flag <code>-TagOp=&lt;API call name&gt;</code>. Note that this does not include the package name. For example, the ElastiCache service uses the API call <code>AddTagsToResource</code>, so the flag is <code>-TagOp=AddTagsToResource</code>.</p> <p>If the API call to add tags uses a field other than <code>ResourceArn</code> to identify the resource, pass the flag <code>-TagInIDElem=&lt;field name&gt;</code>. For example, the EC2 service uses the field <code>Resources</code>, so the flag is <code>-TagInIDElem=Resources</code>. Some API calls take a slice of identifiers instead of a single identifier. In this case, pass the flag <code>-TagInIDNeedSlice=yes</code>.</p> <p>If the API call to remove tags is not <code>UntagResource</code>, pass the flag <code>-UntagOp=&lt;API call name&gt;</code>. Note that this does not include the package name. For example, the ElastiCache service uses the API call <code>RemoveTagsFromResource</code>, so the flag is <code>-UntagOp=RemoveTagsFromResource</code>.</p> <p>If the API call to remove tags uses a field other than <code>ResourceArn</code> to identify the resource, pass the flag <code>-UntagInTagsElem=&lt;field name&gt;</code>. For example, the Route 53 service uses the field <code>Keys</code>, so the flag is <code>-UntagInTagsElem=Keys</code>.</p> <p>For more details on flags for generating tag updating functions, see the documentation for the tag generator</p>"},{"location":"resource-tagging/#generating-standalone-post-creation-tag-updating-functions","title":"Generating Standalone Post-Creation Tag Updating Functions","text":"<p>When creating a resource, some AWS APIs support passing tags in the Create call while others require setting the tags after the initial creation. If the API does not support tagging on creation, pass the <code>-CreateTags</code> flag to generate a <code>createTags</code> function that can be called from the resource Create handler function.</p>"},{"location":"resource-tagging/#running-code-generation","title":"Running Code generation","text":"<p>Run the command <code>make gen</code> to run the code generators for the project. To ensure that the code compiles, run <code>make test</code>.</p>"},{"location":"resource-tagging/#resource-code","title":"Resource Code","text":""},{"location":"resource-tagging/#resource-schema","title":"Resource Schema","text":"<p>Add the following imports to the resource's Go source file:</p> <pre><code>imports (\n/* ... other imports ... */\ntftags \"github.com/hashicorp/terraform-provider-aws/internal/tags\"\n\"github.com/hashicorp/terraform-provider-aws/internal/verify\"\n\"github.com/hashicorp/terraform-provider-aws/names\"\n)\n</code></pre> <p>Add the <code>tags</code> parameter and <code>tags_all</code> attribute to the schema, using constants defined in the <code>names</code> package. The <code>tags</code> parameter contains the tags set directly on the resource. The <code>tags_all</code> attribute contains a union of the tags set directly on the resource and default tags configured on the provider.</p> Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <pre><code>func (r *resourceExample) Schema(ctx context.Context, req resource.SchemaRequest, resp *resource.SchemaResponse) {\nresp.Schema = schema.Schema{\nAttributes: map[string]schema.Attribute{\n/* ... other configuration ... */\nnames.AttrTags:    tftags.TagsAttribute(),\nnames.AttrTagsAll: tftags.TagsAttributeComputedOnly(),\n},\n}\n}\n</code></pre> <pre><code>func ResourceExample() *schema.Resource {\nreturn &amp;schema.Resource{\n/* ... other configuration ... */\nSchema: map[string]*schema.Schema{\n/* ... other configuration ... */\nnames.AttrTags:    tftags.TagsSchema(),\nnames.AttrTagsAll: tftags.TagsSchemaComputed(),\n},\n}\n}\n</code></pre>"},{"location":"resource-tagging/#transparent-tagging","title":"Transparent Tagging","text":"<p>All service that support tagging use a facility we call transparent (or implicit) tagging, where the majority of resource tagging functionality is implemented using code located in the provider's runtime packages (see <code>internal/provider/intercept.go</code> and <code>internal/provider/fwprovider/intercept.go</code> for details) and not in the resource's CRUD handler functions. Resource implementers opt-in to transparent tagging by adding an annotation (a specially formatted Go comment) to the resource's factory function (similar to the resource self-registration mechanism).</p> Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <pre><code>// @FrameworkResource(\"aws_service_example\", name=\"Example\")\n// @Tags(identifierAttribute=\"arn\")\nfunc newResourceExample(_ context.Context) (resource.ResourceWithConfigure, error) {\nreturn &amp;resourceExample{}, nil\n}\n</code></pre> <pre><code>// @SDKResource(\"aws_service_example\", name=\"Example\")\n// @Tags(identifierAttribute=\"arn\")\nfunc ResourceExample() *schema.Resource {\nreturn &amp;schema.Resource{\n...\n}\n}\n</code></pre> <p>The <code>identifierAttribute</code> argument to the <code>@Tags</code> annotation identifies the attribute in the resource type's schema whose value is used in tag listing and updating API calls. Common values are <code>\"arn\"</code> and <code>\"id\"</code>. If the resource type does not need separate <code>createTags</code>, <code>listTags</code>, or <code>updateTags</code> functions, do not specify an <code>identifierAttribute</code>.</p> <p>Once the annotation has been added to the resource's code, run <code>make gen</code> to register the resource for transparent tagging. This will add an entry to the <code>service_package_gen.go</code> file located in the service package folder.</p>"},{"location":"resource-tagging/#resource-create-operation","title":"Resource Create Operation","text":"<p>When creating a resource, some AWS APIs support passing tags in the Create call while others require setting the tags after the initial creation.</p> <p>If the API supports tagging on creation (e.g., the <code>Input</code> struct accepts a <code>Tags</code> field), use the <code>getTagsIn</code> function to get any configured tags.</p> Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <pre><code>input := service.CreateExampleInput{\n/* ... other configuration ... */\nTags: getTagsIn(ctx),\n}\n</code></pre> <pre><code>input := service.CreateExampleInput{\n/* ... other configuration ... */\nTags: getTagsIn(ctx),\n}\n</code></pre> <p>Otherwise, if the API does not support tagging on creation, call <code>createTags</code> after the resource has been created.</p> Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <pre><code>if err := createTags(ctx, conn, plan.ID.ValueString(), getTagsIn(ctx)); err != nil {\nresp.Diagnostics.AddError(\ncreate.ProblemStandardMessage(names.Service, create.ErrActionCreating, ResNameExample, plan.ID.String(), nil),\nerr.Error(),\n)\nreturn\n}\n</code></pre> <pre><code>if err := createTags(ctx, conn, d.Id(), getTagsIn(ctx)); err != nil {\nreturn sdkdiag.AppendErrorf(diags, \"setting Service Example (%s) tags: %s\", d.Id(), err)\n}\n</code></pre>"},{"location":"resource-tagging/#resource-read-operation","title":"Resource Read Operation","text":"<p>In the resource <code>Read</code> operation, use the <code>setTagsOut</code> function to signal to the transparent tagging mechanism that the resource has tags that should be saved into Terraform state.</p> Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <pre><code>setTagsOut(ctx, out.Tags)\n</code></pre> <pre><code>setTagsOut(ctx, out.Tags)\n</code></pre> <p>If the service API does not return the tags directly from reading the resource and requires use of the generated <code>listTags</code> function, do nothing and the transparent tagging mechanism will make the <code>listTags</code> call and save any tags into the Terraform state.</p>"},{"location":"resource-tagging/#resource-update-operation","title":"Resource Update Operation","text":"<p>In the resource <code>Update</code> operation, only non-<code>tags</code> updates need to be done as the transparent tagging mechanism makes the <code>updateTags</code> call.</p> Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <pre><code>if !plan.Name.Equal(state.Name) ||\n!plan.Description.Equal(state.Description) ||\n// etc.\n// Do NOT check for tags changes here.\n!plan.OtherField.Equal(state.OtherField) {\n...\n}\n</code></pre> <pre><code>if d.HasChangesExcept(names.AttrTags, names.AttrTagsAll) {\n...\n}\n</code></pre> <p>For Terraform Plugin SDK V2 based resources, ensure that the <code>Update</code> operation always calls the resource <code>Read</code> operation before returning so that the transparent tagging mechanism correctly saves any tags into the Terraform state.</p> Terraform Plugin SDK V2 <pre><code>func resourceAnalyzerUpdate(ctx context.Context, d *schema.ResourceData, meta interface{}) diag.Diagnostics {\nvar diags diag.Diagnostics\n// Tags only.\nreturn append(diags, resourceAnalyzerRead(ctx, d, meta)...)\n}\n</code></pre>"},{"location":"resource-tagging/#explicit-tagging","title":"Explicit Tagging","text":"<p>If the resource cannot opt-in to transparent tagging, more boilerplate code must be explicitly added to the resource CRUD handler functions. This section describes how to do this.</p> <p>Note</p> <p>There are currently no Terraform Plugin Framework based resources which use explicit tagging. As such, the remaining examples in this section will reference legacy Terraform Plugin SDK V2 patterns.</p>"},{"location":"resource-tagging/#resource-create-operation_1","title":"Resource Create Operation","text":"<p>When creating a resource, some AWS APIs support passing tags in the Create call while others require setting the tags after the initial creation.</p> <p>If the API supports tagging on creation (e.g., the <code>Input</code> struct accepts a <code>Tags</code> field), implement the logic to convert the configuration tags into the service tags, e.g., with EKS Clusters:</p> Terraform Plugin SDK V2 <pre><code>// Typically declared near conn := /*...*/\ndefaultTagsConfig := meta.(*conns.AWSClient).DefaultTagsConfig(ctx)\ntags := defaultTagsConfig.MergeTags(tftags.New(ctx, d.Get(\"tags\").(map[string]interface{})))\n\ninput := eks.CreateClusterInput{\n/* ... other configuration ... */\nTags: svcTags(tags.IgnoreAWS()),\n}\n</code></pre> <p>If the service API does not allow passing an empty list, the logic can be adjusted similarly to:</p> Terraform Plugin SDK V2 <pre><code>// Typically declared near conn := /*...*/\ndefaultTagsConfig := meta.(*conns.AWSClient).DefaultTagsConfig(ctx)\ntags := defaultTagsConfig.MergeTags(tftags.New(ctx, d.Get(\"tags\").(map[string]interface{})))\n\ninput := eks.CreateClusterInput{\n/* ...other configuration... */\n}\n\nif len(tags) &gt; 0 {\ninput.Tags = svcTags(tags.IgnoreAWS())\n}\n</code></pre> <p>Otherwise, if the API does not support tagging on creation, implement the logic to convert the configuration tags into the service API call to tag a resource, e.g., with Device Farm device pools:</p> Terraform Plugin SDK V2 <pre><code>// Typically declared near conn := /*...*/\ndefaultTagsConfig := meta.(*conns.AWSClient).DefaultTagsConfig(ctx)\ntags := defaultTagsConfig.MergeTags(tftags.New(ctx, d.Get(\"tags\").(map[string]interface{})))\n\n/* ... creation steps ... */\n\nif len(tags) &gt; 0 {\nif err := updateTags(ctx, conn, d.Id(), nil, tags); err != nil {\nreturn fmt.Errorf(\"adding DeviceFarm Device Pool (%s) tags: %w\", d.Id(), err)\n}\n}\n</code></pre> <p>Some EC2 resources (e.g., <code>aws_ec2_fleet</code>) have a <code>TagSpecifications</code> field in the <code>InputStruct</code> instead of a <code>Tags</code> field. In these cases the <code>tagSpecificationsFromKeyValue()</code> helper function should be used. This example shows using <code>TagSpecifications</code>:</p> Terraform Plugin SDK V2 <pre><code>// Typically declared near conn := /*...*/\ndefaultTagsConfig := meta.(*conns.AWSClient).DefaultTagsConfig(ctx)\ntags := defaultTagsConfig.MergeTags(tftags.New(ctx, d.Get(\"tags\").(map[string]interface{})))\n\ninput := ec2.CreateFleetInput{\n/* ... other configuration ... */\nTagSpecifications: tagSpecificationsFromKeyValue(tags, ec2.ResourceTypeFleet),\n}\n</code></pre>"},{"location":"resource-tagging/#resource-read-operation_1","title":"Resource Read Operation","text":"<p>In the resource <code>Read</code> operation, implement the logic to convert the service tags to save them into the Terraform state for drift detection, e.g., with EKS Clusters:</p> Terraform Plugin SDK V2 <pre><code>// Typically declared near conn := /*...*/\ndefaultTagsConfig := meta.(*conns.AWSClient).DefaultTagsConfig(ctx)\nignoreTagsConfig := meta.(*conns.AWSClient).IgnoreTagsConfig(ctx)\n\n/* ... other d.Set(...) logic ... */\n\ntags := keyValueTags(ctx, cluster.Tags).IgnoreAWS().IgnoreConfig(ignoreTagsConfig)\n\nif err := d.Set(\"tags\", tags.RemoveDefaultConfig(defaultTagsConfig).Map()); err != nil {\nreturn fmt.Errorf(\"setting tags: %w\", err)\n}\n\nif err := d.Set(\"tags_all\", tags.Map()); err != nil {\nreturn fmt.Errorf(\"setting tags_all: %w\", err)\n}\n</code></pre> <p>If the service API does not return the tags directly from reading the resource and requires a separate API call, use the generated <code>listTags</code> function, e.g., with Athena Workgroups:</p> Terraform Plugin SDK V2 <pre><code>// Typically declared near conn := /*...*/\ndefaultTagsConfig := meta.(*conns.AWSClient).DefaultTagsConfig(ctx)\nignoreTagsConfig := meta.(*conns.AWSClient).IgnoreTagsConfig(ctx)\n\n/* ... other d.Set(...) logic ... */\n\ntags, err := listTags(ctx, conn, arn.String())\n\nif err != nil {\nreturn fmt.Errorf(\"listing tags for resource (%s): %w\", arn, err)\n}\n\ntags = tags.IgnoreAWS().IgnoreConfig(ignoreTagsConfig)\n\nif err := d.Set(\"tags\", tags.RemoveDefaultConfig(defaultTagsConfig).Map()); err != nil {\nreturn fmt.Errorf(\"setting tags: %w\", err)\n}\n\nif err := d.Set(\"tags_all\", tags.Map()); err != nil {\nreturn fmt.Errorf(\"setting tags_all: %w\", err)\n}\n</code></pre>"},{"location":"resource-tagging/#resource-update-operation_1","title":"Resource Update Operation","text":"<p>In the resource <code>Update</code> operation, implement the logic to handle tagging updates, e.g., with EKS Clusters:</p> Terraform Plugin SDK V2 <pre><code>if d.HasChange(\"tags_all\") {\no, n := d.GetChange(\"tags_all\")\nif err := updateTags(ctx, conn, d.Get(\"arn\").(string), o, n); err != nil {\nreturn fmt.Errorf(\"updating tags: %w\", err)\n}\n}\n</code></pre> <p>If the resource <code>Update</code> function applies specific updates to attributes regardless of changes to tags, implement the following e.g., with IAM Policy:</p> Terraform Plugin SDK V2 <pre><code>if d.HasChangesExcept(\"tags\", \"tags_all\") {\n/* ... other logic ...*/\nrequest := iam.CreatePolicyVersionInput{\nPolicyArn:      aws.String(d.Id()),\nPolicyDocument: aws.String(d.Get(\"policy\").(string)),\nSetAsDefault:   aws.Bool(true),\n}\n\nif _, err := conn.CreatePolicyVersionWithContext(ctx, request); err != nil {\nreturn fmt.Errorf(\"updating IAM policy (%s): %w\", d.Id(), err)\n}\n}\n</code></pre>"},{"location":"resource-tagging/#resource-acceptance-tests","title":"Resource Acceptance Tests","text":"<p>Some services, and some resource or data source types within services, have generated acceptance tests for tagging support. These tests cover a broad set of tagging behaviors. New services should use the generated acceptance tests.</p>"},{"location":"resource-tagging/#generated-acceptance-tests","title":"Generated Acceptance Tests","text":"<p>To enable generated acceptance tests for a service, add the following line to the service's <code>generate.go</code> file:</p> <pre><code>//go:generate go run ../../generate/tagstests/main.go\n</code></pre>"},{"location":"resource-tagging/#controlling-test-generation","title":"Controlling Test Generation","text":"<p>By default, all resource or data source types which support transparent tagging will have tagging tests generated. Individual resource or data source types can be excluded from generated acceptance tests by adding the annotation <code>@Testing(tagsTest=false)</code> to the resource type declaration. If a resource or data source type supports tags but does not use transparent tagging, generate the tests by adding the annotion <code>@Testing(tagsTest=true)</code></p> <p>Additional <code>@Testing(...)</code> parameters can be used to control the generated tests.</p>"},{"location":"resource-tagging/#precheck-parameters","title":"PreCheck parameters","text":"<p>All generated tagging tests include the standard <code>acctest.PreCheck</code> PreCheck function. In some cases, acceptance tests will require additional PreCheck functions. Specify them with the annotation <code>@Testing(preCheck=&lt;reference&gt;)</code>. The reference optionally contains a Go package path and package alias, using the format <code>[&lt;package path&gt;;[&lt;package alias&gt;;]]&lt;function name&gt;</code>. The function is assumed to have the signature <code>func(ctx context.Context, t *testing.T)</code>. Multiple <code>@Testing(preCheck)</code> annotations are allowed.</p>"},{"location":"resource-tagging/#required-argument-parameters","title":"Required Argument parameters","text":"<p>Most testing configurations take a single parameter, often a name or a domain name. The most common case is parameter <code>rName</code> with a value generated by <code>sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)</code>, so this is the default. If no <code>rName</code> is required, add the annotation <code>@Testing(generator=false)</code>. Other values can be used by setting the <code>generator</code> to a reference to a function call. The reference optionally contains a Go package path and package alias, using the format <code>[&lt;package path&gt;;[&lt;package alias&gt;;]]&lt;function call&gt;</code>. For example, the Service Catalog Portfolio uses a five-character long random string</p> <pre><code>// @Testing(generator=\"github.com/hashicorp/terraform-plugin-testing/helper/acctest;sdkacctest;sdkacctest.RandString(5)\")\n</code></pre> <p>Some acceptance tests also require a TLS key and certificate. This can be included by setting the annotation <code>@Testing(tlsKey=true)</code>, which will add the Terraform variables <code>certificate_pem</code> and <code>private_key_pem</code> to the configuration. By default, the common name for the certificate is <code>example.com</code>. To override the common name, set the annotation <code>@Testing(tlsKeyDomain=&lt;reference&gt;)</code> to reference an existing variable. For example, the API Gateway v2 Domain Name sets the variable <code>rName</code> to <code>acctest.RandomSubdomain()</code> and sets the annotation <code>@Testing(tlsKeyDomain=rName)</code> to reference it.</p> <p>Some acceptance tests require a TLS ECDSA public key PEM. This can be included by setting the annotation <code>@Testing(tlsEcdsaPublicKeyPem=true)</code>. The Terraform variable name will be <code>rTlsEcdsaPublicKeyPem</code>.</p> <p>Some acceptance tests related to networking require a random BGP ASN value. This can be included by setting the annotation <code>@Testing(randomBsgAsn=\"&lt;low end&gt;;&lt;high end&gt;)</code>, where <code>&lt;low end&gt;</code> and <code>&lt;high end&gt;</code> are the upper and lower bounds for the randomly-generated ASN value. The Terraform variable name will be <code>rBgpAsn</code>.</p> <p>Some acceptance tests related to networking require a random IPv4 address. This can be included by setting the annotation <code>@Testing(randomIPv4Address=\"&lt;CIDR range&gt;)</code>. The randomly-generated IPv4 address value will be contained within the <code>&lt;CIDR range&gt;</code>. The Terraform variable name will be <code>rIPv4Address</code>.</p> <p>No additional parameters can be defined currently. If additional parameters are required, and cannot be derived from <code>rName</code>, the resource type must use manually created acceptance tests as described below.</p>"},{"location":"resource-tagging/#exists-and-destroy-parameters","title":"Exists and Destroy parameters","text":"<p>Most <code>Exists</code> functions used in acceptance tests take a pointer to the returned API object. To specify the type of this parameter, use the annotion <code>@Testing(existsType=&lt;reference&gt;)</code>. This references a Go type and package path with optional package alias, using the format <code>&lt;package path&gt;;[&lt;package alias&gt;;]&lt;function call&gt;</code>. For example, the S3 Object uses</p> <pre><code>// @Testing(existsType=\"github.com/aws/aws-sdk-go-v2/service/s3;s3.GetObjectOutput\")\n</code></pre> <p>Some services or resource types are using a new variant of the standard <code>Exists</code> and <code>DestroyCheck</code> functions that use <code>acctest.ProviderMeta</code> internally, and thus take a <code>testing.T</code> as a parameter. In that case, add the annotations <code>@Testing(existsTakesT=true)</code> and <code>@Testing(destroyTakesT=true)</code>, respectively.</p> <p>Some resource types use the no-op <code>CheckDestroy</code> function <code>acctest.CheckDestroyNoop</code>. Use the annotation <code>@Testing(checkDestroyNoop=true)</code>.</p>"},{"location":"resource-tagging/#import-parameters","title":"Import parameters","text":"<p>The generated acceptance tests use <code>ImportState</code> steps. In most cases, these will work as-is. To ignore the values of certain parameters when importing, set the annotation <code>@Testing(importIgnore=\"...\")</code> to a list of the parameter names separated by semi-colons (<code>;</code>). There are multiple methods for overriding the import ID, if needed. To use the value of an existing variable, use the annotation <code>@Testing(importStateId=&lt;var name&gt;)</code>. If the identifier can be retrieved from a specific resource attribute, use the annotation <code>@Testing(importStateIdAttribute=&lt;attribute name&gt;)</code>. If the identifier can be retrieved from a <code>resource.ImportStateIdFunc</code>, use the annotation <code>@Testing(importStateIdFunc=&lt;func name&gt;)</code>. If the resource type does not support importing, use the annotation <code>@Testing(noImport=true)</code>.</p>"},{"location":"resource-tagging/#serialization-parameters","title":"Serialization parameters","text":"<p>If the tests need to be serialized, use the annotation <code>@Testing(serialize=true)</code>. If a delay is needed between serialized tests, also use the annotation <code>@Testing(serializeDelay=&lt;duration&gt;)</code> with a duration in the format used by <code>time.ParseDuration()</code>. For example, 3 minutes and 30 seconds is <code>3m30s</code>.</p>"},{"location":"resource-tagging/#empty-and-null-tag-parameters","title":"Empty and Null Tag parameters","text":"<p>Some services do not support tags with an empty string value. In that case, use the annotation <code>@Testing(skipEmptyTags=true)</code>.</p> <p>Some services do not support tags with a null string value. In that case, use the annotation <code>@Testing(skipNullTags=true)</code>.</p>"},{"location":"resource-tagging/#tag-update-parameters","title":"Tag Update parameters","text":"<p>For some resource types, tags cannot be modified without recreating the resource. Use the annotation <code>@Testing(tagsUpdateForceNew=true)</code>.</p> <p>Resource types which pass the result of <code>getTagsIn</code> directly onto their Update Input may have an error where ignored tags are not correctly excluded from the update. Use the annotation <code>@Testing(tagsUpdateGetTagsIn=true)</code>.</p>"},{"location":"resource-tagging/#resource-identifier-parameters","title":"Resource Identifier parameters","text":"<p>Some tests read the tag values directly from the AWS API. If the resource type does not specify <code>identifierAttribute</code> in its <code>@Tags</code> annotation, specify a <code>@Testing(tagsIdentifierAttribute=&lt;attribute name&gt;)</code> annotation to identify which attribute value should be used by the <code>listTags</code> function. If a resource type is also needed for the <code>listTags</code> function, also specify the <code>tagsResourceType</code> annotation.</p>"},{"location":"resource-tagging/#tag-removal-parameters","title":"Tag Removal parameters","text":"<p>At least one resource type, the Service Catalog Provisioned Product, does not support removing tags. This is likely an error on the AWS side. Add the annotation <code>@Testing(noRemoveTags=true)</code> as a workaround.</p>"},{"location":"resource-tagging/#test-terraform-configurations","title":"Test Terraform Configurations","text":"<p>The generated acceptance tests use <code>ConfigDirectory</code> to specify the test configurations in a directory of Terraform <code>.tf</code> files. The configuration files are generated from a Go template file located in <code>testdata/tmpl/&lt;name&gt;_tags.gtpl</code>, where <code>name</code> is the name of the resource type's implementation file wihtout the <code>.go</code> extension. For example, the ELB v2 Load Balancer's implementation file is <code>load_balancer.go</code>, so the template is <code>testdata/tmpl/load_balancer_tags.gtpl</code>.</p> <p>To generate a configuration for a data source test, the generator reuses the configuration for the corresponding resource type. Add an additional file <code>testdata/tmpl/&lt;name&gt;_data_source.gtpl</code> which contains only the data source block populated with the parameters needed to associate it with the resource. For example, the ELB v2 Load Balancer's data source template is <code>testdata/tmpl/load_balancer_data_source.gtpl</code>.</p> <p>Replace the <code>tags</code> attribute with the Go template directive <code>{{- template \"tags\" . }}</code>. When the configurations are generated, this will be replaced with the appropriate assignment to the <code>tags</code> attribute.</p> <p>Tags should only be applied to the resource that is being tested.</p>"},{"location":"resource-tagging/#manually-created-acceptance-tests","title":"Manually Created Acceptance Tests","text":"<p>In the resource acceptance tests (e.g., <code>internal/service/eks/cluster_test.go</code>), verify that existing resources without tagging are unaffected and do not have tags saved into their Terraform state. This should be done in the <code>_basic</code> acceptance test by adding one line similar to <code>resource.TestCheckResourceAttr(resourceName, \"tags.%\", \"0\"),</code> and one similar to <code>resource.TestCheckResourceAttr(resourceName, \"tags_all.%\", \"0\"),</code></p> <p>Add a new test named <code>_tags</code> with associated configurations, that verifies creating the resource with tags and updating tags. E.g., EKS Clusters:</p> <pre><code>func TestAccEKSCluster_tags(t *testing.T) {\nctx := acctest.Context(t)\nvar cluster1, cluster2, cluster3 eks.Cluster\nrName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\nresourceName := \"aws_eks_cluster.test\"\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck:                 func() { acctest.PreCheck(ctx, t); testAccPreCheck(t) },\nErrorCheck:               acctest.ErrorCheck(t, names.EKSServiceID),\nProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories,\nCheckDestroy:             testAccCheckClusterDestroy(ctx),\nSteps: []resource.TestStep{\n{\nConfig: testAccClusterConfig_tags1(rName, \"key1\", \"value1\"),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheckClusterExists(ctx, resourceName, &amp;cluster1),\nresource.TestCheckResourceAttr(resourceName, \"tags.%\", \"1\"),\nresource.TestCheckResourceAttr(resourceName, \"tags.key1\", \"value1\"),\n),\n},\n{\nResourceName:      resourceName,\nImportState:       true,\nImportStateVerify: true,\n},\n{\nConfig: testAccClusterConfig_tags2(rName, \"key1\", \"value1updated\", \"key2\", \"value2\"),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheckClusterExists(ctx, resourceName, &amp;cluster2),\nresource.TestCheckResourceAttr(resourceName, \"tags.%\", \"2\"),\nresource.TestCheckResourceAttr(resourceName, \"tags.key1\", \"value1updated\"),\nresource.TestCheckResourceAttr(resourceName, \"tags.key2\", \"value2\"),\n),\n},\n{\nConfig: testAccClusterConfig_tags1(rName, \"key2\", \"value2\"),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheckClusterExists(ctx, resourceName, &amp;cluster3),\nresource.TestCheckResourceAttr(resourceName, \"tags.%\", \"1\"),\nresource.TestCheckResourceAttr(resourceName, \"tags.key2\", \"value2\"),\n),\n},\n},\n})\n}\n\nfunc testAccClusterConfig_tags1(rName, tagKey1, tagValue1 string) string {\nreturn acctest.ConfigCompose(testAccClusterConfig_base(rName), fmt.Sprintf(`\nresource \"aws_eks_cluster\" \"test\" {\n  name     = %[1]q\n  role_arn = aws_iam_role.test.arn\n\n  tags = {\n    %[2]q = %[3]q\n  }\n\n  vpc_config {\n    subnet_ids = aws_subnet.test[*].id\n  }\n\n  depends_on = [aws_iam_role_policy_attachment.test-AmazonEKSClusterPolicy]\n}\n`, rName, tagKey1, tagValue1))\n}\n\nfunc testAccClusterConfig_tags2(rName, tagKey1, tagValue1, tagKey2, tagValue2 string) string {\nreturn acctest.ConfigCompose(testAccClusterConfig_base(rName), fmt.Sprintf(`\nresource \"aws_eks_cluster\" \"test\" {\n  name     = %[1]q\n  role_arn = aws_iam_role.test.arn\n\n  tags = {\n    %[2]q = %[3]q\n    %[4]q = %[5]q\n  }\n\n  vpc_config {\n    subnet_ids = aws_subnet.test[*].id\n  }\n\n  depends_on = [aws_iam_role_policy_attachment.test-AmazonEKSClusterPolicy]\n}\n`, rName, tagKey1, tagValue1, tagKey2, tagValue2))\n}\n</code></pre> <p>Verify all acceptance testing passes for the resource (e.g., <code>make testacc TESTS=TestAccEKSCluster_ PKG=eks</code>)</p>"},{"location":"resource-tagging/#resource-documentation","title":"Resource Documentation","text":"<p>In the resource documentation (e.g., <code>website/docs/r/service_example.html.markdown</code>), add the following to the arguments reference:</p> <pre><code>* `tags` - (Optional) Map of tags assigned to the resource. If configured with a provider [`default_tags` configuration block](/docs/providers/aws/index.html#default_tags-configuration-block) present, tags with matching keys will overwrite those defined at the provider-level.\n</code></pre> <p>In the resource documentation (e.g., <code>website/docs/r/service_example.html.markdown</code>), add the following to the attribute reference:</p> <pre><code>* `tags_all` - Map of tags assigned to the resource, including those inherited from the provider [`default_tags` configuration block](/docs/providers/aws/index.html#default_tags-configuration-block).\n</code></pre>"},{"location":"retries-and-waiters/","title":"Retries and Waiters","text":""},{"location":"retries-and-waiters/#retries-and-waiters","title":"Retries and Waiters","text":"<p>Terraform plugins may run into situations where calling the remote system after an operation may be necessary. These typically fall under three classes where:</p> <ul> <li>The request never reaches the remote system.</li> <li>The request reaches the remote system and responds that it cannot handle the request temporarily.</li> <li>The implementation of the remote system requires additional requests to ensure success.</li> </ul> <p>This guide describes the behavior of the Terraform AWS Provider and provides code implementations that help ensure success in each of these situations.</p> <p>Note</p> <p>The helper functions detailed below are compatible with Terraform Plugin Framework based resources (the required library for net-new resources). While these functions currently reside in the legacy Terraform Plugin SDK repository, they are not directly tied to functionality exclusive to this library, and likely will be moved to a standalone library or into the AWS provider itself in the future.</p>"},{"location":"retries-and-waiters/#terraform-plugin-sdk-functionality","title":"Terraform Plugin SDK Functionality","text":"<p>The Terraform Plugin SDK, which the AWS Provider uses, provides the <code>retry.StateChangeConf{}</code> struct, used for handling resource state consistency. We will discuss it throughout the rest of this guide. Since it helps keep the AWS Provider code consistent, we heavily prefer it over custom implementations.</p> <p>This guide goes beyond the Terraform Plugin SDK v2 documentation by providing additional context and emergent implementations specific to the Terraform AWS Provider.</p>"},{"location":"retries-and-waiters/#state-change-configuration-and-functions","title":"State Change Configuration and Functions","text":"<p>The <code>retry.StateChangeConf</code> type, along with its receiver methods <code>WaitForState()</code> and <code>WaitForStateContext()</code> is a generic primitive for repeating operations in Terraform resource logic until desired value(s) are received. The \"state change\" in this case is generic to any value and not specific to the Terraform State. Among other functionality, it supports some of these desirable optional properties:</p> <ul> <li>Expecting specific value(s) while waiting for the target value(s) to be reached. Unexpected values are returned as an error which can be augmented with additional details.</li> <li>Expecting the target value(s) to be returned multiple times in succession.</li> <li>Allowing various polling configurations such as delaying the initial request and setting the time between polls.</li> </ul>"},{"location":"retries-and-waiters/#retry-functions","title":"Retry Functions","text":"<p>The <code>tfresource.Retry()</code> function provides a simplified retry implementation. The most common use is for simple error-based retries.</p>"},{"location":"retries-and-waiters/#aws-request-handling","title":"AWS Request Handling","text":"<p>The Terraform AWS Provider's requests to AWS service APIs happen on top of Hypertext Transfer Protocol (HTTP). The following is a simplified description of the layers and handling that requests pass through:</p> <ul> <li>A Terraform resource calls an AWS Go SDK function.</li> <li>The AWS Go SDK generates an AWS-compatible HTTP request using the Go standard library <code>net/http</code> package. This includes the following:<ul> <li>Adding HTTP headers for authentication and signing of requests to ensure authenticity.</li> <li>Converting operation inputs into required HTTP URI parameters and/or request body type (XML or JSON).</li> <li>If debug logging is enabled, logging of the HTTP request.</li> </ul> </li> <li>The AWS Go SDK transmits the <code>net/http</code> request using Go's standard handling of the Operating System (OS) and Domain Name System (DNS) configuration.</li> <li>The AWS service potentially receives the request and responds, typically adding a request identifier HTTP header which can be used for AWS Support cases.</li> <li>The OS and Go <code>net/http</code> receive the response and pass it to the AWS Go SDK.</li> <li>The AWS Go SDK attempts to handle the response. This may include:<ul> <li>Parsing output</li> <li>Converting errors into operation errors (Go <code>error</code> type of wrapped <code>awserr.Error</code> type).</li> <li>Converting response elements into operation outputs (AWS Go SDK operation-specific types).</li> <li>Triggering automatic request retries based on default and custom logic.</li> </ul> </li> <li>The Terraform resource receives the response, including any output and errors, from the AWS Go SDK.</li> </ul> <p>In cases where custom operation handling is configured for a specific service client, the changes can typically be found in <code>internal/service/{service-name}/service_package.go</code>.</p>"},{"location":"retries-and-waiters/#default-aws-go-sdk-retries","title":"Default AWS Go SDK Retries","text":"<p>In some situations, while handling a response, the AWS Go SDK automatically retries a request before returning the output and error. The retry mechanism implements an exponential backoff algorithm. The default conditions triggering automatic retries (implemented through <code>client.DefaultRetryer</code>) include:</p> <ul> <li>Certain network errors. A common exception to this is connection reset errors.</li> <li>HTTP status codes 429 and 5xx.</li> <li>Certain API error codes, which are common across various AWS services (e.g., <code>ThrottledException</code>). However, not all AWS services implement these error codes consistently. A common exception to this is certain expired credentials errors.</li> </ul> <p>By default, the Terraform AWS Provider sets the maximum number of AWS Go SDK retries based on the <code>max_retries</code> provider configuration. The provider configuration defaults to 25 and the exponential backoff roughly equates to one hour of retries. This very high default value was present before the Terraform AWS Provider codebase was split from Terraform CLI in version 0.10.</p>"},{"location":"retries-and-waiters/#lower-network-error-retries","title":"Lower Network Error Retries","text":"<p>Given the very high default number of AWS Go SDK retries configured in the Terraform AWS Provider and the excessive wait that practitioners would face, the <code>hashicorp/aws-sdk-go-base</code> codebase lowers retries to 10 for certain network errors that typically cannot be remediated via retries. This roughly equates to 30 seconds of retries.</p>"},{"location":"retries-and-waiters/#terraform-aws-provider-service-retries","title":"Terraform AWS Provider Service Retries","text":"<p>The AWS Go SDK provides hooks for injecting custom logic into service client handlers. We prefer this handling in situations where contributors would need to apply the retry behavior to many resources. For example, in cases where the AWS service API does not mark an error code as automatically retriable. The AWS Provider includes other retry-changing behaviors using this method. When custom service client configurations are applied, these will be defined in <code>internal/service/{service-name}/service_package.go</code>.</p> <p>With V2 of the AWS Go SDK, the retrier is extended directly in client construction.</p> <pre><code>// NewClient returns a new AWS SDK for Go v2 client for this service package's AWS API.\nfunc (p *servicePackage) NewClient(ctx context.Context, config map[string]any) (*s3_sdkv2.Client, error) {\ncfg := *(config[\"aws_sdkv2_config\"].(*aws_sdkv2.Config))\n\nreturn s3_sdkv2.NewFromConfig(cfg,\ns3.WithEndpointResolverV2(newEndpointResolverSDKv2()),\nwithBaseEndpoint(config[names.AttrEndpoint].(string)),\nfunc(o *s3_sdkv2.Options) {\n// ..other configuration..\n\no.Retryer = conns.AddIsErrorRetryables(cfg.Retryer().(aws_sdkv2.RetryerV2), retry_sdkv2.IsErrorRetryableFunc(func(err error) aws_sdkv2.Ternary {\nif tfawserr_sdkv2.ErrMessageContains(err, errCodeOperationAborted, \"A conflicting conditional operation is currently in progress against this resource. Please try again.\") {\nreturn aws_sdkv2.TrueTernary\n}\nreturn aws_sdkv2.UnknownTernary // Delegate to configured Retryer.\n}))\n},\n), nil\n}\n</code></pre>"},{"location":"retries-and-waiters/#eventual-consistency","title":"Eventual Consistency","text":"<p>Eventual consistency is a temporary condition where the remote system can return outdated information or errors due to not being strongly read-after-write consistent. This is a pattern found in remote systems that must be highly scaled for broad usage.</p> <p>Terraform expects any planned resource lifecycle change (create, update, destroy of the resource itself) and planned resource attribute value change to match after being applied. Conversely, operators typically expect that Terraform resources also implement the concept of drift detection for resources and their attributes, which requires reading information back from the remote system after an operation. A common implementation is calling the underlying <code>Get</code>/<code>Describe</code> AWS API after <code>Create</code> and <code>Update</code>.</p> <p>These two concepts conflict with each other and require additional handling in Terraform resource logic as shown in the following sections. These issues are not reliably reproducible, especially in the case of writing acceptance testing, so they can be elusive with false positives to verify fixes.</p>"},{"location":"retries-and-waiters/#operation-specific-error-retries","title":"Operation Specific Error Retries","text":"<p>Even given a properly ordered Terraform configuration, eventual consistency can unexpectedly prevent downstream operations from succeeding. A simple retry after a few seconds resolves many of these issues. To reduce frustrating behavior for operators, wrap AWS Go SDK operations with the <code>tfresource.Retry()</code> function. These retries should have a reasonably low timeout (typically two minutes but up to five minutes). Save them in a constant for reusability. These functions are preferably in line with the associated resource logic to remove any indirection with the code.</p> <p>Do not use this type of logic to overcome improperly ordered Terraform configurations. The approach may not work in larger environments.</p> <pre><code>const (\n// Maximum amount of time to wait for Thing operation eventual consistency\nThingOperationTimeout = 2 * time.Minute\n)\n</code></pre> <pre><code>// internal/service/{service}/{thing}.go\n\n// ... Create, Read, Update, or Delete function ...\nerr := tfresource.Retry(ctx, ThingOperationTimeout, func(ctx context.Context) *tfresource.RetryError {\n_, err := conn./* ... AWS Go SDK operation with eventual consistency errors ... */\n\n// Retryable conditions which can be checked.\n// These must be updated to match the AWS service API error code and message.\nif errs.IsAErrorMessageContains[/* error type */](err, /* error message */) {\nreturn tfresource.RetryableError(err)\n}\n\nif err != nil {\nreturn tfresource.NonRetryableError(err)\n}\n\nreturn nil\n})\n\nif err != nil {\nreturn fmt.Errorf(\"... error message context ... : %w\", err)\n}\n</code></pre>"},{"location":"retries-and-waiters/#iam-error-retries","title":"IAM Error Retries","text":"<p>A common eventual consistency issue is an error returned due to IAM permissions. The IAM service itself is eventually consistent along with the propagation of its components and permissions to other AWS services. For example, if the following operations occur in quick succession:</p> <ul> <li>Create an IAM Role</li> <li>Attach an IAM Policy to the IAM Role</li> <li>Reference the new IAM Role in another AWS service, such as creating a Lambda Function</li> </ul> <p>The last operation can receive varied API errors ranging from:</p> <ul> <li>IAM Role being reported as not existing</li> <li>IAM Role being reported as not having permissions for the other service to use it (assume role permissions)</li> <li>IAM Role being reported as not having sufficient permissions (inline or attached role permissions)</li> </ul> <p>Each AWS service API (and sometimes even operations within the same API) varies in the implementation of these errors. To handle them, it is recommended to use the Operation Specific Error Retries pattern. The Terraform AWS Provider implements a standard timeout constant of two minutes in the <code>internal/service/iam</code> package which should be used for all retry timeouts associated with IAM errors. This timeout was derived from years of Terraform operational experience with all AWS APIs.</p> <pre><code>// internal/service/{service}/{thing}.go\n\nimport (\n// ... other imports ...\ntfiam \"github.com/hashicorp/terraform-provider-aws/internal/service/iam\"\n)\n\n// ... Create and typically Update function ...\nerr := tfresource.Retry(ctx, iamwaiter.PropagationTimeout, func(ctx context.Context) *tfresource.RetryError {\n_, err := conn./* ... AWS Go SDK operation with IAM eventual consistency errors ... */\n\n// Example retryable condition\n// This must be updated to match the AWS service API error code and message.\nif errs.IsAErrorMessageContains[/* error type */](err, /* error message */) {\nreturn tfresource.RetryableError(err)\n}\n\nif err != nil {\nreturn tfresource.NonRetryableError(err)\n}\n\nreturn nil\n})\n\nif err != nil {\nreturn fmt.Errorf(\"... error message context ... : %w\", err)\n}\n</code></pre>"},{"location":"retries-and-waiters/#asynchronous-operation-error-retries","title":"Asynchronous Operation Error Retries","text":"<p>Some remote system operations run asynchronously as detailed in the Asynchronous Operations section. In these cases, it is possible that the initial operation will immediately return as successful, but potentially return a retryable failure while checking the operation status that requires starting everything over. The handling for these is complicated by the fact that there are two timeouts, one for the retryable failure and one for the asynchronous operation status checking.</p> <p>The below code example highlights this situation for a resource creation that also exhibited IAM eventual consistency.</p> <pre><code>// internal/service/{service}/{thing}.go\n\nimport (\n// ... other imports ...\ntfiam \"github.com/hashicorp/terraform-provider-aws/internal/service/iam\"\n)\n\n// ... Create function ...\n\n// Underlying IAM eventual consistency errors can occur after the creation\n// operation. The goal is only retry these types of errors up to the IAM\n// timeout. Since the creation process is asynchronous and can take up to\n// its own timeout, we store a stop time upfront for checking.\niamwaiterStopTime := time.Now().Add(tfiam.PropagationTimeout)\n\n// Ensure to add IAM eventual consistency timeout in case of retries\nerr = tfresource.Retry(ctx, tfiam.PropagationTimeout+ThingOperationTimeout, func(ctx context.Context) *tfresource.RetryError {\n// Only retry IAM eventual consistency errors up to that timeout\niamwaiterRetry := time.Now().Before(iamwaiterStopTime)\n\n_, err := conn./* ... AWS Go SDK operation without eventual consistency errors ... */\n\nif err != nil {\nreturn tfresource.NonRetryableError(err)\n}\n\n_, err = ThingOperation(conn, d.Id())\n\nif err != nil {\nif iamwaiterRetry &amp;&amp; /* eventual consistency error checking */ {\nreturn tfresource.RetryableError(err)\n}\n\nreturn tfresource.NonRetryableError(err)\n}\n\nreturn nil\n})\n</code></pre>"},{"location":"retries-and-waiters/#resource-lifecycle-retries","title":"Resource Lifecycle Retries","text":"<p>Resource lifecycle eventual consistency is a type of consistency issue that relates to the existence or state of an AWS infrastructure component. For example, if you create a resource and immediately try to get information about it, some AWS services and operations will return a \"not found\" error. Depending on the service and general AWS load, these errors can be frequent or rare.</p> <p>In order to avoid this issue, identify operations that make changes. Then, when calling any other operations that rely on the changes, account for the possibility that the AWS service has not yet fully realized them.</p> <p>Handling eventual consistency looks slightly different for Terraform Plugin Framework based resources versus Plugin SDK V2. For Terraform Framework based resources, a <code>Get</code>/<code>Describe</code> API call should be made after the create request completes, all within the <code>Create</code> method. For Terraform Plugin SDK V2 resources, the <code>Create</code> function should return by calling the <code>Read</code> function. Both approaches fill in computed attributes and ensure that the AWS service applied the configuration correctly. Add retry logic to the <code>Get</code>/<code>Describe</code> API call (Plugin Framework) or <code>Read</code> function (Plugin SDK V2) to overcome the temporary condition on resource creation.</p> <p>Note</p> <p>For eventually consistent resources, \"not found\" errors can still occur in the <code>Read</code> function even after implementing Resource Lifecycle Waiters.</p> <pre><code>const (\n// Maximum amount of time to wait for Thing eventual consistency on creation\nThingCreationTimeout = 2 * time.Minute\n)\n</code></pre> Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <pre><code>// internal/service/{service}/{thing}.go\n\nfunc (r *resourceThing) Create(ctx context.Context, req resource.CreateRequest, resp *resource.CreateResponse) {\nconn := meta.(*AWSClient).ExampleClient()\n\n// ...Creation steps...\n\ninput := example.OperationInput{/* ... */}\n\nvar output *example.OperationOutput\ncreateTimeout := r.CreateTimeout(ctx, plan.Timeouts)\nerr := tfresource.Retry(ctx, createTimeout, func(ctx context.Context) *tfresource.RetryError {\nvar err error\noutput, err = conn.Operation(ctx, &amp;input)\n\nif errs.IsA[*types.ResourceNotFoundException(err) {\nreturn tfresource.RetryableError(err)\n}\n\nif err != nil {\nreturn tfresource.NonRetryableError(err)\n}\n\nreturn nil\n})\n\nif err != nil {\nresp.Diagnostics.AddError(\ncreate.ProblemStandardMessage(names.Example, create.ErrActionWaitingForCreation, ResNameThing, plan.ID.String(), err),\nerr.Error(),\n)\nreturn\n}\n\n// Prevent panics.\nif output == nil {\nresp.Diagnostics.AddError(\ncreate.ProblemStandardMessage(names.Example, create.ErrActionWaitingForCreation, ResNameThing, plan.ID.String(), errors.New(\"empty output\")),\nerr.Error(),\n)\nreturn\n}\n\n// ... refresh Terraform state as normal ...\n}\n</code></pre> <pre><code>// internal/service/{service}/{thing}.go\n\nfunc ExampleThingCreate(ctx context.Context, d *schema.ResourceData, meta any) diag.Diagnostics {\nvar diags diag.Diagnostics\n// ...\nreturn append(diags, ExampleThingRead(ctx, d, meta)...)\n}\n\nfunc ExampleThingRead(ctx context.Context, d *schema.ResourceData, meta any) diag.Diagnostics {\nvar diags diag.Diagnostics\n\nconn := meta.(*AWSClient).ExampleConn()\n\ninput := example.OperationInput{/* ... */}\n\nvar output *example.OperationOutput\nerr := tfresource.Retry(ctx, ThingCreationTimeout, func(ctx context.Context) *tfresource.RetryError {\nvar err error\noutput, err = conn.Operation(ctx, &amp;input)\n\n// Retry on any API \"not found\" errors, but only on new resources.\nif d.IsNewResource() &amp;&amp; tfawserr.ErrorCodeEquals(err, example.ErrCodeResourceNotFoundException) {\nreturn tfresource.RetryableError(err)\n}\n\nif err != nil {\nreturn tfresource.NonRetryableError(err)\n}\n\nreturn nil\n})\n\n// Prevent confusing Terraform error messaging to operators by\n// Only ignoring API \"not found\" errors if not a new resource.\nif !d.IsNewResource() &amp;&amp; tfawserr.ErrorCodeEquals(err, example.ErrCodeNoSuchEntityException) {\nlog.Printf(\"[WARN] Example Thing (%s) not found, removing from state\", d.Id())\nd.SetId(\"\")\nreturn diags\n}\n\nif err != nil {\nreturn sdkdiag.AppendErrorf(diags, \"reading Example Thing (%s): %w\", d.Id(), err)\n}\n\n// Prevent panics.\nif output == nil {\nreturn sdkdiag.AppendErrorf(diags, \"reading Example Thing (%s): empty response\", d.Id())\n}\n\n// ... refresh Terraform state as normal ...\nd.Set(\"arn\", output.Arn)\n}\n</code></pre> <p>Some other general guidelines are:</p> <ul> <li>If the <code>Create</code> function uses <code>retry.StateChangeConf</code>, the underlying <code>resource.RefreshStateFunc</code> should <code>return nil, \"\", nil</code> instead of the API \"not found\" error. This way the <code>StateChangeConf</code> logic will automatically retry.</li> <li>If the <code>Create</code> function uses <code>tfresource.Retry()</code>, the API \"not found\" error should be caught and <code>return tfresource.RetryableError(err)</code> to automatically retry.</li> </ul> <p>In rare cases, it may be easier to duplicate all <code>Read</code> function logic in the <code>Create</code> function to handle all retries in one place.</p>"},{"location":"retries-and-waiters/#resource-attribute-value-waiters","title":"Resource Attribute Value Waiters","text":"<p>An emergent solution for handling eventual consistency with attribute values on updates is to introduce a custom <code>retry.StateChangeConf</code> and <code>resource.RefreshStateFunc</code> handlers. For example, the waiting logic can be implemented as:</p> <pre><code>// ThingAttribute fetches the Thing and its Attribute\nfunc ThingAttribute(ctx context.Context, conn *example.Client, id string) retry.StateRefreshFunc {\nreturn func() (any, string, error) {\noutput, err := /* ... AWS Go SDK operation to fetch resource/value ... */\n\nif errs.IsA[*types.ResourceNotFoundException](err) {\nreturn nil, \"\", nil\n}\n\nif err != nil {\nreturn nil, \"\", err\n}\n\nif output == nil {\nreturn nil, \"\", nil\n}\n\nreturn output, aws.ToString(output.Attribute), nil\n}\n}\n\n// waitThingAttributeUpdated is an attribute waiter for Thing.Attribute\nfunc waitThingAttributeUpdated(ctx context.Context, conn *example.Example, id string, expectedValue string, timeout time.Duration) (*example.Thing, error) {\nstateConf := &amp;retry.StateChangeConf{\nTarget:  []string{expectedValue},\nRefresh: ThingAttribute(ctx, conn, id),\nTimeout: timeout,\n}\n\noutputRaw, err := stateConf.WaitForState()\n\nif output, ok := outputRaw.(*example.Thing); ok {\nreturn output, err\n}\n\nreturn nil, err\n}\n</code></pre> <p>And consumed within the resource update workflow as follows:</p> Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <pre><code>func (r *resourceThing) Update(ctx context.Context, req resource.UpdateRequest, resp resource.UpdateResponse) {\n// ...\n\n!plan.Attribute.Equal(state.Attribute) {\n// ... AWS Go SDK logic to update attribute ...\n\nupdateTimeout := r.UpdateTimeout(ctx, plan.Timeouts)\nif _, err := waitThingAttributeUpdated(ctx, conn, plan.ID.StringValue(), plan.Attribute.StringValue(), updateTimeout); err != nil {\nresp.Diagnostics.AddError(\ncreate.ProblemStandardMessage(names.Example, create.ErrActionWaitingForUpdate, ResNameThing, plan.ID.String(), err),\nerr.Error(),\n)\nreturn\n}\n}\n\n// ...\n}\n</code></pre> <pre><code>func resourceThingUpdate(ctx context.Context, d *schema.ResourceData, meta any) diags.Diagnostics {\n// ...\n\nd.HasChange(\"attribute\") {\n// ... AWS Go SDK logic to update attribute ...\n\nif _, err := waitThingAttributeUpdated(ctx, conn, d.Id(), d.Get(\"attribute\").(string), d.Timeout(schema.TimeoutUpdate)); err != nil {\nreturn create.AppendDiagError(diags, names.Example, create.ErrActionWaitingForUpdate, ResNameThing, d.Id(), err)\n}\n}\n\n// ...\n}\n</code></pre>"},{"location":"retries-and-waiters/#asynchronous-operations","title":"Asynchronous Operations","text":"<p>When you initiate a long-running operation, an AWS service may return a successful response immediately and continue working on the request asynchronously. A resource can track the status with a component-level field (e.g., <code>CREATING</code>, <code>UPDATING</code>, etc.) or an explicit tracking identifier.</p> <p>Terraform resources should wait for these background operations to complete. Failing to do so can introduce incomplete state information and downstream errors in other resources. In rare scenarios involving very long-running operations, operators may request a flag to skip the waiting. However, these should only be implemented case-by-case to prevent those previously mentioned confusing issues.</p>"},{"location":"retries-and-waiters/#aws-go-sdk-waiters","title":"AWS Go SDK Waiters","text":"<p>The AWS SDK for Go provides waiters for some asynchronous operations. We required using Resource Lifecycle Waiters instead since they are more commonly used throughout the codebase and provide more options for customization.</p>"},{"location":"retries-and-waiters/#resource-lifecycle-waiters","title":"Resource Lifecycle Waiters","text":"<p>Most of the codebase uses <code>retry.StateChangeConf</code> and <code>retry.StateRefreshFunc</code> handlers for tracking either component-level status fields or explicit tracking identifiers. These should be placed in the <code>internal/service/{SERVICE}</code> package and split into separate functions. For example:</p> <pre><code>// ThingStatus fetches the Thing and its Status\nfunc ThingStatus(ctx context.Context, conn *example.Client, id string) retry.StateRefreshFunc {\nreturn func() (any, string, error) {\noutput, err := /* ... AWS Go SDK operation to fetch resource/status ... */\n\nif errs.IsA[*types.ResourceNotFoundException](err) {\nreturn nil, \"\", nil\n}\n\nif err != nil {\nreturn nil, \"\", err\n}\n\nif output == nil {\nreturn nil, \"\", nil\n}\n\nreturn output, aws.ToString(output.Status), nil\n}\n}\n</code></pre> <pre><code>// waitThingCreated is a resource waiter for Thing creation\nfunc waitThingCreated(ctx context.Context, conn *example.Example, id string, timeout time.Duration) (*example.Thing, error) {\nstateConf := &amp;retry.StateChangeConf{\nPending: []string{example.StatusCreating},\nTarget:  []string{example.StatusCreated},\nRefresh: ThingStatus(ctx, conn, id),\nTimeout: timeout,\n}\n\noutputRaw, err := stateConf.WaitForState()\n\nif output, ok := outputRaw.(*example.Thing); ok {\nreturn output, err\n}\n\nreturn nil, err\n}\n\n// waitThingDeleted is a resource waiter for Thing deletion\nfunc waitThingDeleted(ctx context.Context, conn *example.Example, id string, timeout time.Duration) (*example.Thing, error) {\nstateConf := &amp;retry.StateChangeConf{\nPending: []string{example.StatusDeleting},\nTarget:  []string{}, // Use empty list if the resource disappears and does not have \"deleted\" status\nRefresh: ThingStatus(conn, id),\nTimeout: timeout,\n}\n\noutputRaw, err := stateConf.WaitForState()\n\nif output, ok := outputRaw.(*example.Thing); ok {\nreturn output, err\n}\n\nreturn nil, err\n}\n</code></pre> Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <pre><code>func (r *resourceThing) Create(ctx context.Context, req resource.CreateRequest, resp resource.CreateResponse) {\n// ... AWS Go SDK logic to create resource ...\n\ncreateTimeout := r.CreateTimeout(ctx, plan.Timeouts)\nif _, err = waitThingCreated(ctx, conn, plan.ID.ValueString(), createTimeout); err != nil {\nresp.Diagnostics.AddError(\ncreate.ProblemStandardMessage(names.Example, create.ErrActionWaitingForCreation, ResNameThing, plan.ID.String(), err),\nerr.Error(),\n)\nreturn\n}\n\nresp.Diagnostics.Append(resp.State.Set(ctx, plan)...)\n}\n\nfunc (r *resourceThing) Delete(ctx context.Context, req resource.DeleteRequest, resp *resource.DeleteResponse) {\n// ... AWS Go SDK logic to delete resource ...\n\ndeleteTimeout := r.DeleteTimeout(ctx, plan.Timeouts)\nif _, err = waitThingDeleted(ctx, conn, plan.ID.ValueString(), deleteTimeout); err != nil {\nresp.Diagnostics.AddError(\ncreate.ProblemStandardMessage(names.Example, create.ErrActionWaitingForDeletion, ResNameThing, plan.ID.String(), err),\nerr.Error(),\n)\nreturn\n}\n}\n</code></pre> <pre><code>func resourceThingCreate(ctx context.Context, d *schema.ResourceData, meta any) diag.Diagnostics {\nvar diags diag.Diagnostics\n\n// ... AWS Go SDK logic to create resource ...\n\nif _, err := waitThingCreated(ctx, conn, d.Id(), d.Timeout(schema.TimeoutCreate)) {\nreturn create.AppendDiagError(diags, names.Example, create.ErrActionWaitingForCreation, ResNameThing, d.Id(), err)\n}\n\nreturn append(diags, ExampleThingRead(ctx, d, meta)...)\n}\n\nfunc resourceThingDelete(ctx context.Context, d *schema.ResourceData, meta any) diag.Diagnostics {\n// ... AWS Go SDK logic to delete resource ...\n\nif _, err := waitThingDeleted(conn, d.Id(), d.Timeout(schema.TimeoutDelete)); err != nil {\nreturn create.AppendDiagError(diags, names.Example, create.ErrActionWaitingForDeletion, ResNameThing, d.Id(), err)\n}\n\nreturn diags\n}\n</code></pre> <p>Typically, the AWS Go SDK should include constants for various status field values (e.g., <code>StatusCreating</code> for <code>CREATING</code>). If not, create them in a file named <code>internal/service/{SERVICE}/consts.go</code>.</p>"},{"location":"running-and-writing-acceptance-tests/","title":"Acceptance Tests","text":""},{"location":"running-and-writing-acceptance-tests/#running-and-writing-acceptance-tests","title":"Running and Writing Acceptance Tests","text":"<p>Terraform includes an acceptance test harness that does most of the repetitive work involved in testing a resource. For additional information about testing Terraform Providers, see the SDKv2 documentation.</p>"},{"location":"running-and-writing-acceptance-tests/#in-context","title":"In context","text":"<p>To help place acceptance testing in context, here is an overview of the Terraform AWS Provider's three types of tests.</p> <ol> <li>Acceptance tests (You are here!) are end-to-end evaluations of interactions with AWS. They validate functionalities like creating, reading, and destroying resources within AWS.</li> <li>Unit tests focus on testing isolated units of code within the software, typically at the function level. They assess functionalities solely within the provider itself.</li> <li>Continuous integration tests encompass a suite of automated tests that are executed on every pull request and include linting, compiling code, running unit tests, and performing static analysis.</li> </ol>"},{"location":"running-and-writing-acceptance-tests/#acceptance-tests-often-cost-money-to-run","title":"Acceptance Tests Often Cost Money to Run","text":"<p>Our acceptance test suite creates real resources, and as a result, they cost real money to run. Because the resources only exist for a short period of time, the total amount of money required is usually relatively small. That said there are particular services which are very expensive to run and it's important to be prepared for those costs.</p> <p>Some services which can be cost-prohibitive include (among others):</p> <ul> <li>ACM (Amazon Certificate Manager)</li> <li>Bedrock</li> <li>EC2</li> <li>ElastiCache</li> <li>FSx</li> <li>Glue</li> <li>Kinesis Analytics</li> <li>OpenSearch</li> <li>RDS</li> <li>Storage Gateway</li> <li>WorkSpaces</li> </ul> <p>We don't want financial limitations to be a barrier to contribution, so if you are unable to pay to run acceptance tests for your contribution, mention this in your pull request. We will happily accept \"best effort\" implementations of acceptance tests and run them for you on our side. This might mean that your PR takes a bit longer to merge, but it most definitely is not a blocker for contributions.</p>"},{"location":"running-and-writing-acceptance-tests/#running-an-acceptance-test","title":"Running an Acceptance Test","text":"<p>Acceptance tests can be run using the <code>testacc</code> target in the Terraform <code>Makefile</code>. The individual tests to run can be controlled using a regular expression. Prior to running the tests provider configuration details such as access keys must be made available as environment variables.</p> <p>For example, to run an acceptance test against the Amazon Web Services provider, the following environment variables must be set:</p> <pre><code># Using a profile\nexport AWS_PROFILE=...\n# Otherwise\nexport AWS_ACCESS_KEY_ID=...\nexport AWS_SECRET_ACCESS_KEY=...\nexport AWS_DEFAULT_REGION=...\n</code></pre> <p>Please note that the default region for the testing is <code>us-west-2</code> and must be overridden via the <code>AWS_DEFAULT_REGION</code> environment variable, if necessary. This is especially important for testing AWS GovCloud (US), which requires:</p> <pre><code>export AWS_DEFAULT_REGION=us-gov-west-1\n</code></pre> <p>Tests can then be run by specifying a regular expression defining the tests to run and the package in which the tests are defined:</p> <pre><code>make testacc TESTS=TestAccCloudWatchDashboard_updateName PKG=cloudwatch\n</code></pre> <pre><code>==&gt; Checking that code complies with gofmt requirements...\nTF_ACC=1 go test ./internal/service/cloudwatch/... -v -count 1 -parallel 20 -run=TestAccCloudWatchDashboard_updateName -timeout 180m\n=== RUN   TestAccCloudWatchDashboard_updateName\n=== PAUSE TestAccCloudWatchDashboard_updateName\n=== CONT  TestAccCloudWatchDashboard_updateName\n--- PASS: TestAccCloudWatchDashboard_updateName (25.33s)\nPASS\nok      github.com/hashicorp/terraform-provider-aws/internal/service/cloudwatch 25.387s\n</code></pre> <p>Entire resource test suites can be targeted by using the naming convention to write the regular expression. For example, to run all tests of the <code>aws_cloudwatch_dashboard</code> resource rather than just the <code>updateName</code> test, you can start testing like this:</p> <pre><code>make testacc TESTS=TestAccCloudWatchDashboard PKG=cloudwatch\n</code></pre> <pre><code>==&gt; Checking that code complies with gofmt requirements...\nTF_ACC=1 go test ./internal/service/cloudwatch/... -v -count 1 -parallel 20 -run=TestAccCloudWatchDashboard -timeout 180m\n=== RUN   TestAccCloudWatchDashboard_basic\n=== PAUSE TestAccCloudWatchDashboard_basic\n=== RUN   TestAccCloudWatchDashboard_update\n=== PAUSE TestAccCloudWatchDashboard_update\n=== RUN   TestAccCloudWatchDashboard_updateName\n=== PAUSE TestAccCloudWatchDashboard_updateName\n=== CONT  TestAccCloudWatchDashboard_basic\n=== CONT  TestAccCloudWatchDashboard_updateName\n=== CONT  TestAccCloudWatchDashboard_update\n--- PASS: TestAccCloudWatchDashboard_basic (15.83s)\n--- PASS: TestAccCloudWatchDashboard_updateName (26.69s)\n--- PASS: TestAccCloudWatchDashboard_update (27.72s)\nPASS\nok      github.com/hashicorp/terraform-provider-aws/internal/service/cloudwatch 27.783s\n</code></pre> <p>Running acceptance tests requires version 0.12.26 or higher of the Terraform CLI to be installed.</p> <p>For advanced developers, the acceptance testing framework accepts some additional environment variables that can be used to control Terraform CLI binary selection, logging, and other behaviors. See the SDKv2 documentation for more information.</p> <p>Please Note: On macOS 10.14 and later (and some Linux distributions), the default user open file limit is 256. This may cause unexpected issues when running the acceptance testing since this can prevent various operations from occurring such as opening network connections to AWS. To view this limit, the <code>ulimit -n</code> command can be run. To update this limit, run <code>ulimit -n 1024</code>  (or higher).</p>"},{"location":"running-and-writing-acceptance-tests/#running-cross-account-tests","title":"Running Cross-Account Tests","text":"<p>Certain testing requires multiple AWS accounts. This additional setup is not typically required and the testing will return an error (shown below) if your current setup does not have the secondary AWS configuration:</p> <pre><code>make testacc TESTS=TestAccRDSInstance_DBSubnetGroupName_ramShared PKG=rds\n</code></pre> <pre><code>TF_ACC=1 go test ./internal/service/rds/... -v -count 1 -parallel 20 -run=TestAccRDSInstance_DBSubnetGroupName_ramShared -timeout 180m\n=== RUN   TestAccRDSInstance_DBSubnetGroupName_ramShared\n=== PAUSE TestAccRDSInstance_DBSubnetGroupName_ramShared\n=== CONT  TestAccRDSInstance_DBSubnetGroupName_ramShared\n    acctest.go:674: skipping test because at least one environment variable of [AWS_ALTERNATE_PROFILE AWS_ALTERNATE_ACCESS_KEY_ID] must be set. Usage: credentials for running acceptance testing in alternate AWS account.\n--- SKIP: TestAccRDSInstance_DBSubnetGroupName_ramShared (0.85s)\nPASS\nok      github.com/hashicorp/terraform-provider-aws/internal/service/rds        0.888s\n</code></pre> <p>Running these acceptance tests is the same as before, except the following additional AWS credential information is required:</p> <pre><code># Using a profile\nexport AWS_ALTERNATE_PROFILE=...\n# Otherwise\nexport AWS_ALTERNATE_ACCESS_KEY_ID=...\nexport AWS_ALTERNATE_SECRET_ACCESS_KEY=...\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#running-cross-region-tests","title":"Running Cross-Region Tests","text":"<p>Certain testing requires multiple AWS regions. Additional setup is not typically required because the testing defaults the second AWS region to <code>us-east-1</code> and the third AWS region to <code>us-east-2</code>.</p> <p>Running these acceptance tests is the same as before, but if you wish to override the second and third regions:</p> <pre><code>export AWS_ALTERNATE_REGION=...\nexport AWS_THIRD_REGION=...\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#running-only-short-tests","title":"Running Only Short Tests","text":"<p>Some tests have been manually marked as long-running (longer than 300 seconds) and can be skipped using the <code>-short</code> flag. However, we are adding long-running guards little by little and many services have no guarded tests.</p> <p>Where guards have been implemented, do not always skip long-running tests. However, for intermediate test runs during development, or to verify functionality unrelated to the specific long-running tests, skipping long-running tests makes work more efficient. We recommend that for the final test run before submitting a PR you run affected tests without the <code>-short</code> flag.</p> <p>If you want to run only short-running tests, you can use either one of these equivalent statements. Note the use of <code>-short</code>.</p> <p>For example:</p> <pre><code>make testacc TESTS='TestAccECSTaskDefinition_' PKG=ecs TESTARGS=-short\n</code></pre> <p>Or:</p> <pre><code>TF_ACC=1 go test ./internal/service/ecs/... -v -count 1 -parallel 20 -run='TestAccECSTaskDefinition_' -short -timeout 180m\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#writing-an-acceptance-test","title":"Writing an Acceptance Test","text":"<p>Terraform has a framework for writing acceptance tests which minimizes the amount of boilerplate code necessary to use common testing patterns. This guide is meant to augment the general SDKv2 documentation with Terraform AWS Provider specific conventions and helpers.</p>"},{"location":"running-and-writing-acceptance-tests/#anatomy-of-an-acceptance-test","title":"Anatomy of an Acceptance Test","text":"<p>This section describes in detail how the Terraform acceptance testing framework operates with respect to the Terraform AWS Provider. We recommend those unfamiliar with this provider, or Terraform resource testing in general, take a look here first to generally understand how we interact with AWS and the resource code to verify functionality.</p> <p>The entry point to the framework is the <code>resource.ParallelTest()</code> function. This wraps our testing to work with the standard Go testing framework, while also preventing unexpected usage of AWS by requiring the <code>TF_ACC=1</code> environment variable. This function accepts a <code>TestCase</code> parameter, which has all the details about the test itself. For example, this includes the test steps (<code>TestSteps</code>) and how to verify resource deletion in the API after all steps have been run (<code>CheckDestroy</code>).</p> <p>Each <code>TestStep</code> proceeds by applying some Terraform configuration using the provider under test, and then verifying that results are as expected by making assertions using the provider API. It is common for a single test function to exercise both the creation of and updates to a single resource. Most tests follow a similar structure.</p> <ol> <li>Pre-flight checks are made to ensure that sufficient provider configuration    is available to be able to proceed - for example in an acceptance test    targeting AWS, <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> must be set prior    to running acceptance tests. This is common to all tests exercising a single    provider.</li> </ol> <p>Most assertion functions are defined out of band with the tests. This keeps the tests readable, and allows reuse of assertion functions across different tests of the same type of resource. The definition of a complete test looks like this:</p> <pre><code>func TestAccCloudWatchDashboard_basic(t *testing.T) {\nctx := acctest.Context(t)\nvar dashboard cloudwatch.GetDashboardOutput\nrInt := acctest.RandInt()\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck:                 func() { acctest.PreCheck(ctx, t) },\nErrorCheck:               acctest.ErrorCheck(t, names.CloudWatchServiceID),\nProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories,\nCheckDestroy:             testAccCheckDashboardDestroy(ctx),\nSteps: []resource.TestStep{\n{\nConfig: testAccDashboardConfig(rInt),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheckDashboardExists(ctx, \"aws_cloudwatch_dashboard.foobar\", &amp;dashboard),\nresource.TestCheckResourceAttr(\"aws_cloudwatch_dashboard.foobar\", \"dashboard_name\", testAccDashboardName(rInt)),\n),\n},\n},\n})\n}\n</code></pre> <p>When executing the test, the following steps are taken for each <code>TestStep</code>:</p> <ol> <li> <p>The Terraform configuration required for the test is applied. This is    responsible for configuring the resource under test, and any dependencies it    may have. For example, to test the <code>aws_cloudwatch_dashboard</code> resource, a valid configuration with the requisite fields is required. This results in a configuration which looks like this:</p> <pre><code>resource \"aws_cloudwatch_dashboard\" \"foobar\" {\ndashboard_name = \"terraform-test-dashboard-%[1]d\"\ndashboard_body = &lt;&lt;EOF\n  {\n    \"widgets\": [{\n      \"type\": \"text\",\n      \"x\": 0,\n      \"y\": 0,\n      \"width\": 6,\n      \"height\": 6,\n      \"properties\": {\n        \"markdown\": \"Hi there from Terraform: CloudWatch\"\n      }\n    }]\n  }\n  EOF\n}\n</code></pre> </li> <li> <p>Assertions are run using the provider API. These use the provider API    directly rather than asserting against the resource state. For example, to    verify that the <code>aws_cloudwatch_dashboard</code> described above was created    successfully, a test function like this is used:</p> <pre><code>func testAccCheckDashboardExists(ctx context.Context, n string, dashboard *cloudwatch.GetDashboardOutput) resource.TestCheckFunc {\nreturn func(s *terraform.State) error {\nrs, ok := s.RootModule().Resources[n]\nif !ok {\nreturn fmt.Errorf(\"Not found: %s\", n)\n}\n\nconn := acctest.Provider.Meta().(*conns.AWSClient).CloudWatchConn(ctx)\nparams := cloudwatch.GetDashboardInput{\nDashboardName: aws.String(rs.Primary.ID),\n}\n\nresp, err := conn.GetDashboardWithContext(ctx, &amp;params)\nif err != nil {\nreturn err\n}\n\n*dashboard = *resp\n\nreturn nil\n}\n}\n</code></pre> </li> </ol> <p>Notice that the only information used from the Terraform state is the ID of    the resource. For computed properties, we instead assert that the value saved in the Terraform state was the    expected value if possible. The testing framework provides helper functions    for several common types of checks - for example:</p> <pre><code>resource.TestCheckResourceAttr(\"aws_cloudwatch_dashboard.foobar\", \"dashboard_name\", testAccDashboardName(rInt)),\n</code></pre> <ol> <li> <p>The resources created by the test are destroyed. This step happens    automatically, and is the equivalent of calling <code>terraform destroy</code>.</p> </li> <li> <p>Assertions are made against the provider API to verify that the resources    have indeed been removed. If these checks fail, the test fails and reports    \"dangling resources\". The code to ensure that the <code>aws_cloudwatch_dashboard</code> shown    above has been destroyed looks like this:</p> <pre><code>func testAccCheckDashboardDestroy(ctx context.Context) resource.TestCheckFunc {\nreturn func(s *terraform.State) error {\nconn := acctest.Provider.Meta().(*conns.AWSClient).CloudWatchConn(ctx)\n\nfor _, rs := range s.RootModule().Resources {\nif rs.Type != \"aws_cloudwatch_dashboard\" {\ncontinue\n}\n\nparams := cloudwatch.GetDashboardInput{\nDashboardName: aws.String(rs.Primary.ID),\n}\n\n_, err := conn.GetDashboardWithContext(ctx, &amp;params)\nif err == nil {\nreturn fmt.Errorf(\"Dashboard still exists: %s\", rs.Primary.ID)\n}\nif !isDashboardNotFoundErr(err) {\nreturn err\n}\n}\n\nreturn nil\n}\n}\n</code></pre> </li> </ol> <p>These functions usually test only for the resource directly under test.</p>"},{"location":"running-and-writing-acceptance-tests/#resource-acceptance-testing","title":"Resource Acceptance Testing","text":"<p>Most resources that implement standard Create, Read, Update, and Delete functionality should follow the pattern below. Each test type has a section that describes them in more detail:</p> <ul> <li>basic: This represents the bare minimum verification that the resource can be created, read, deleted, and optionally imported.</li> <li>disappears: A test that verifies Terraform will offer to recreate a resource if it is deleted outside of Terraform (e.g., via the Console) instead of returning an error that it cannot be found.</li> <li>Per Attribute: A test that verifies the resource with a single additional argument can be created, read, optionally updated (or force resource recreation), deleted, and optionally imported.</li> </ul> <p>The leading sections below highlight additional recommended patterns.</p>"},{"location":"running-and-writing-acceptance-tests/#test-configurations","title":"Test Configurations","text":"<p>Most of the existing test configurations you will find in the Terraform AWS Provider are written in the following function-based style:</p> <pre><code>func TestAccExampleThing_basic(t *testing.T) {\n// ... omitted for brevity ...\n\nresource.ParallelTest(t, resource.TestCase{\n// ... omitted for brevity ...\nSteps: []resource.TestStep{\n{\nConfig: testAccExampleThingConfig(),\n// ... omitted for brevity ...\n},\n},\n})\n}\n\nfunc testAccExampleThingConfig() string {\nreturn `\nresource \"aws_example_thing\" \"test\" {\n  # ... omitted for brevity ...\n}\n`\n}\n</code></pre> <p>Even when no values need to be passed in to the test configuration, we have found this setup to be the most flexible for allowing that to be easily implemented. Any configurable values are handled via <code>fmt.Sprintf()</code>. Using <code>text/template</code> or other templating styles is explicitly forbidden.</p> <p>For consistency, resources in the test configuration should be named <code>resource \"...\" \"test\"</code> unless multiple of that resource are necessary.</p> <p>We discourage re-using test configurations across test files (except for some common configuration helpers we provide) as it is much harder to discover potential testing regressions.</p> <p>Please also note that the newline on the first line of the configuration (before <code>resource</code>) and the newline after the last line of configuration (after <code>}</code>) are important to allow test configurations to be easily combined without generating Terraform configuration language syntax errors.</p>"},{"location":"running-and-writing-acceptance-tests/#test-configuration-independence","title":"Test Configuration Independence","text":"<p>Across the entire provider, all test configurations should be as independent of each other as possible. For example, a common place this concept comes up is with the default VPC. Since we have tests that reconfigure the default VPC, if your configuration requires a VPC, it should not rely on the default VPC. Instead, include a VPC that will be created and destroyed as part of the test.</p> <p>Make sure that your test configuration:</p> <ol> <li>Includes everything required for Terraform to run the test</li> <li>Does not assume that any user-managed infrastructure will be in place, such as S3 buckets, IAM roles, KMS keys, VPCs, subnets, etc.</li> </ol>"},{"location":"running-and-writing-acceptance-tests/#combining-test-configurations","title":"Combining Test Configurations","text":"<p>We include a helper function, <code>acctest.ConfigCompose()</code> for iteratively building and chaining test configurations together. It accepts any number of configurations to combine them. This simplifies a single resource's testing by allowing the creation of a \"base\" test configuration for all the other test configurations (if necessary) and also allows the maintainers to curate common configurations. Each of these is described in more detail in the below sections.</p> <p>Please note that we do discourage excessive chaining of configurations such as implementing multiple layers of \"base\" configurations. Usually these configurations are harder for maintainers and other future readers to understand due to the multiple levels of indirection.</p>"},{"location":"running-and-writing-acceptance-tests/#base-test-configurations","title":"Base Test Configurations","text":"<p>If a resource requires the same Terraform configuration as a prerequisite for all test configurations, then a common pattern is implementing a \"base\" test configuration that is combined with each test configuration.</p> <p>For example:</p> <pre><code>func testAccExampleThingConfigBase() string {\nreturn `\nresource \"aws_iam_role\" \"test\" {\n  # ... omitted for brevity ...\n}\n\nresource \"aws_iam_role_policy\" \"test\" {\n  # ... omitted for brevity ...\n}\n`\n}\n\nfunc testAccExampleThingConfig() string {\nreturn acctest.ConfigCompose(\ntestAccExampleThingConfigBase(),\n`\nresource \"aws_example_thing\" \"test\" {\n  # ... omitted for brevity ...\n}\n`)\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#available-common-test-configurations","title":"Available Common Test Configurations","text":"<p>These test configurations are typical implementations we have found or allow testing to implement best practices easier, since the Terraform AWS Provider testing is expected to run against various AWS Regions and Partitions.</p> <ul> <li><code>acctest.AvailableEC2InstanceTypeForRegion(\"type1\", \"type2\", ...)</code>: Typically used to replace hardcoded EC2 Instance Types. Uses <code>aws_ec2_instance_type_offering</code> data source to return an available EC2 Instance Type in preferred ordering. Reference the instance type via: <code>data.aws_ec2_instance_type_offering.available.instance_type</code>. Use <code>acctest.AvailableEC2InstanceTypeForRegionNamed(\"name\", \"type1\", \"type2\", ...)</code> to specify a name for the data source</li> <li><code>acctest.ConfigLatestAmazonLinuxHVMEBSAMI()</code>: Typically used to replace hardcoded EC2 Image IDs (<code>ami-12345678</code>). Uses <code>aws_ami</code> data source to find the latest Amazon Linux image. Reference the AMI ID via: <code>data.aws_ami.amzn-ami-minimal-hvm-ebs.id</code></li> </ul>"},{"location":"running-and-writing-acceptance-tests/#randomized-naming","title":"Randomized Naming","text":"<p>For AWS resources that require unique naming, the tests should implement a randomized name, typically coded as a <code>rName</code> variable in the test and passed as a parameter to create the test configuration.</p> <p>For example:</p> <pre><code>func TestAccExampleThing_basic(t *testing.T) {\nctx := acctest.Context(t)\nrName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\n// ... omitted for brevity ...\n\nresource.ParallelTest(t, resource.TestCase{\n// ... omitted for brevity ...\nSteps: []resource.TestStep{\n{\nConfig: testAccExampleThingConfigName(rName),\n// ... omitted for brevity ...\n},\n},\n})\n}\n\nfunc testAccExampleThingConfigName(rName string) string {\nreturn fmt.Sprintf(`\nresource \"aws_example_thing\" \"test\" {\n  name = %[1]q\n}\n`, rName)\n}\n</code></pre> <p>Typically the <code>rName</code> is always the first argument to the test configuration function, if used, for consistency.</p> <p>Note that if <code>rName</code> (or any other variable) is used multiple times in the <code>fmt.Sprintf()</code> statement, do not repeat <code>rName</code> in the <code>fmt.Sprintf()</code> arguments. Using <code>fmt.Sprintf(..., rName, rName)</code>, for example, would not be correct. Instead, use the indexed <code>%[1]q</code> (or <code>%[x]q</code>, <code>%[x]s</code>, <code>%[x]t</code>, or <code>%[x]d</code>, where <code>x</code> represents the index number) verb multiple times. For example:</p> <pre><code>func testAccExampleThingConfigName(rName string) string {\nreturn fmt.Sprintf(`\nresource \"aws_example_thing\" \"test\" {\n  name = %[1]q\n\n  tags = {\n    Name = %[1]q\n  }\n}\n`, rName)\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#other-recommended-variables","title":"Other Recommended Variables","text":"<p>We also typically recommend saving a <code>resourceName</code> variable in the test that contains the resource reference, e.g., <code>aws_example_thing.test</code>, which is repeatedly used in the checks.</p> <p>For example:</p> <pre><code>func TestAccExampleThing_basic(t *testing.T) {\nctx := acctest.Context(t)\n// ... omitted for brevity ...\nresourceName := \"aws_example_thing.test\"\n\nresource.ParallelTest(t, resource.TestCase{\n// ... omitted for brevity ...\nSteps: []resource.TestStep{\n{\n// ... omitted for brevity ...\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheckExampleThingExists(ctx, resourceName),\nacctest.CheckResourceAttrRegionalARN(resourceName, \"arn\", \"example\", fmt.Sprintf(\"thing/%s\", rName)),\nresource.TestCheckResourceAttr(resourceName, \"description\", \"\"),\nresource.TestCheckResourceAttr(resourceName, \"name\", rName),\n),\n},\n{\nResourceName:      resourceName,\nImportState:       true,\nImportStateVerify: true,\n},\n},\n})\n}\n\n// below all TestAcc functions\n\nfunc testAccExampleThingConfigName(rName string) string {\nreturn fmt.Sprintf(`\nresource \"aws_example_thing\" \"test\" {\n  name = %[1]q\n}\n`, rName)\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#basic-acceptance-tests","title":"Basic Acceptance Tests","text":"<p>Usually this test is implemented first. The test configuration should contain only required arguments (<code>Required: true</code> attributes) and it should check the values of all read-only attributes (<code>Computed: true</code> without <code>Optional: true</code>). If the resource supports it, it verifies import. It should NOT perform other <code>TestStep</code> such as updates or verify recreation.</p> <p>These are typically named <code>TestAcc{SERVICE}{THING}_basic</code>, e.g., <code>TestAccCloudWatchDashboard_basic</code></p> <p>For example:</p> <pre><code>func TestAccExampleThing_basic(t *testing.T) {\nctx := acctest.Context(t)\nrName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\nresourceName := \"aws_example_thing.test\"\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck:                 func() { acctest.PreCheck(ctx, t) },\nErrorCheck:               acctest.ErrorCheck(t, names.ExampleServiceID),\nProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories,\nCheckDestroy:             testAccCheckExampleThingDestroy(ctx),\nSteps: []resource.TestStep{\n{\nConfig: testAccExampleThingConfigName(rName),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheckExampleThingExists(ctx, resourceName),\nacctest.CheckResourceAttrRegionalARN(resourceName, \"arn\", \"example\", fmt.Sprintf(\"thing/%s\", rName)),\nresource.TestCheckResourceAttr(resourceName, \"description\", \"\"),\nresource.TestCheckResourceAttr(resourceName, \"name\", rName),\n),\n},\n{\nResourceName:      resourceName,\nImportState:       true,\nImportStateVerify: true,\n},\n},\n})\n}\n\n// below all TestAcc functions\n\nfunc testAccExampleThingConfigName(rName string) string {\nreturn fmt.Sprintf(`\nresource \"aws_example_thing\" \"test\" {\n  name = %[1]q\n}\n`, rName)\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#prechecks","title":"PreChecks","text":"<p>Acceptance test cases have a PreCheck. The PreCheck ensures that the testing environment meets certain preconditions. If the environment does not meet the preconditions, Go skips the test. Skipping a test avoids reporting a failure and wasting resources where the test cannot succeed.</p> <p>Here is an example of the default PreCheck:</p> <pre><code>func TestAccExampleThing_basic(t *testing.T) {\nctx := acctest.Context(t)\nrName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\nresourceName := \"aws_example_thing.test\"\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck:     func() { acctest.PreCheck(ctx, t) },\n// ... additional checks follow ...\n})\n}\n</code></pre> <p>Extend the default PreCheck by adding calls to functions in the anonymous PreCheck function. The functions can be existing functions in the provider or custom functions you add for new capabilities.</p>"},{"location":"running-and-writing-acceptance-tests/#standard-provider-prechecks","title":"Standard Provider PreChecks","text":"<p>If you add a new test that has preconditions which are checked by an existing provider function, use that standard PreCheck instead of creating a new one. Some existing tests are missing standard PreChecks and you can help by adding them where appropriate.</p> <p>These are some of the standard provider PreChecks:</p> <ul> <li><code>acctest.PreCheckRegion(t *testing.T, regions ...string)</code> checks that the test region is one of the specified AWS Regions.</li> <li><code>acctest.PreCheckRegionNot(t *testing.T, regions ...string)</code> checks that the test region is not one of the specified AWS Regions.</li> <li><code>acctest.PreCheckAlternateRegionIs(t *testing.T, region string)</code> checks that the alternate test region is the specified AWS Region.</li> <li><code>acctest.PreCheckPartition(t *testing.T, partition string)</code> checks that the test partition is the specified partition.</li> <li><code>acctest.PreCheckPartitionNot(t *testing.T, partitions ...string)</code> checks that the test partition is not one of the specified partitions.</li> <li><code>acctest.PreCheckPartitionHasService(t *testing.T, serviceID string)</code> checks whether the current partition lists the service as part of its offerings. Note: AWS may not add new or public preview services to the service list immediately. This function will return a false positive in that case.</li> <li><code>acctest.PreCheckOrganizationsAccount(ctx context.Context, t *testing.T)</code> checks whether the current account can perform AWS Organizations tests.</li> <li><code>acctest.PreCheckAlternateAccount(t *testing.T)</code> checks whether the environment is set up for tests across accounts.</li> <li><code>acctest.PreCheckMultipleRegion(t *testing.T, regions int)</code> checks whether the environment is set up for tests across regions.</li> </ul> <p>This is an example of using a standard PreCheck function. For an established service, such as WAF or FSx, use <code>acctest.PreCheckPartitionHasService()</code> and the service endpoint ID to check that a partition supports the service.</p> <pre><code>func TestAccExampleThing_basic(t *testing.T) {\nctx := acctest.Context(t)\nrName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\nresourceName := \"aws_example_thing.test\"\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck:     func() { acctest.PreCheck(ctx, t); acctest.PreCheckPartitionHasService(t, waf.EndpointsID) },\n// ... additional checks follow ...\n})\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#custom-prechecks","title":"Custom PreChecks","text":"<p>In situations where standard PreChecks do not test for the required preconditions, create a custom PreCheck.</p> <p>Below is an example of adding a custom PreCheck function. For a new or preview service that AWS does not include in the partition service list yet, you can verify the existence of the service with a simple read-only request (e.g., list all X service things). (For acceptance tests of established services, use <code>acctest.PreCheckPartitionHasService()</code> instead.)</p> <pre><code>func TestAccExampleThing_basic(t *testing.T) {\nctx := acctest.Context(t)\nrName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\nresourceName := \"aws_example_thing.test\"\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck:     func() { acctest.PreCheck(ctx, t), testAccPreCheckExample(ctx, t) },\n// ... additional checks follow ...\n})\n}\n\nfunc testAccPreCheckExample(ctx context.Context, t *testing.T) {\nconn := acctest.Provider.Meta().(*conns.AWSClient).ExampleConn(ctx)\ninput := example.ListThingsInput{}\n_, err := conn.ListThingsWithContext(ctx, &amp;input)\nif testAccPreCheckSkipError(err) {\nt.Skipf(\"skipping acceptance testing: %s\", err)\n}\nif err != nil {\nt.Fatalf(\"unexpected PreCheck error: %s\", err)\n}\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#errorchecks","title":"ErrorChecks","text":"<p>Acceptance test cases have an ErrorCheck. The ErrorCheck provides a chance to take a look at errors before the test fails. While most errors should result in test failure, some should not. For example, an error that indicates an API operation is not supported in a particular region should cause the test to skip instead of fail. Since errors should flow through the ErrorCheck, do not handle the vast majority of failing conditions. Instead, in ErrorCheck, focus on the rare errors that should cause a test to skip, or in other words, be ignored.</p>"},{"location":"running-and-writing-acceptance-tests/#common-errorcheck","title":"Common ErrorCheck","text":"<p>In many situations, the common ErrorCheck is sufficient. It will skip tests for several normal occurrences such as when AWS reports a feature is not supported in the current region.</p> <p>Here is an example of the common ErrorCheck:</p> <pre><code>func TestAccExampleThing_basic(t *testing.T) {\nrName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\nresourceName := \"aws_example_thing.test\"\n\nresource.ParallelTest(t, resource.TestCase{\n// PreCheck\nErrorCheck:   acctest.ErrorCheck(t, names.ExampleServiceID),\n// ... additional checks follow ...\n})\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#service-specific-errorchecks","title":"Service-Specific ErrorChecks","text":"<p>However, some services have special conditions that aren't caught by the common ErrorCheck. In these cases, you can create a service-specific ErrorCheck.</p> <p>To add a service-specific ErrorCheck, follow these steps:</p> <ol> <li>Make sure there is not already an ErrorCheck for the service you have in mind. For example, search the codebase for <code>acctest.RegisterServiceErrorCheckFunc(names.ExampleServiceID</code> replacing \"service\" with the package name of the service you're working on (e.g., <code>ec2</code>). If there is already an ErrorCheck for the service, add it to the existing service-specific ErrorCheck.</li> <li>Create the service-specific ErrorCheck in an <code>_test.go</code> file for the service. See the example below.</li> <li>Register the new service-specific ErrorCheck in the <code>init()</code> at the top of the <code>_test.go</code> file. See the example below.</li> </ol> <p>An example of adding a service-specific ErrorCheck:</p> <pre><code>// just after the imports, create or add to the init() function\nfunc init() {\nacctest.RegisterServiceErrorCheck(names.ExampleServiceID, testAccErrorCheckSkipService)\n}\n\n// ... additional code and tests ...\n\n// this is the service-specific ErrorCheck\nfunc testAccErrorCheckSkipService(t *testing.T) resource.ErrorCheckFunc {\nreturn acctest.ErrorCheckSkipMessagesContaining(t,\n\"Error message specific to the service that indicates unsupported features\",\n\"You can include from one to many portions of error messages\",\n\"Be careful to not inadvertently capture errors that should not be skipped\",\n)\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#long-running-test-guards","title":"Long-Running Test Guards","text":"<p>For any acceptance tests that typically run longer than 300 seconds (5 minutes), add a <code>-short</code> test guard at the top of the test function.</p> <p>For example:</p> <pre><code>func TestAccExampleThing_longRunningTest(t *testing.T) {\nif testing.Short() {\nt.Skip(\"skipping long-running test in short mode\")\n}\n\n// ... omitted for brevity ...\n\nresource.ParallelTest(t, resource.TestCase{\n// ... omitted for brevity ...\n})\n}\n</code></pre> <p>When running acceptance tests, tests with these guards can be skipped using the Go <code>-short</code> flag. See Running Only Short Tests for examples.</p>"},{"location":"running-and-writing-acceptance-tests/#disappears-acceptance-tests","title":"Disappears Acceptance Tests","text":"<p>This test is generally implemented second. It is straightforward to set up once the basic test is passing since it can reuse that test configuration. It prevents a common bug report with Terraform resources that error when they can not be found (e.g., deleted outside Terraform).</p> <p>These are typically named <code>TestAcc{SERVICE}{THING}_disappears</code>, e.g., <code>TestAccCloudWatchDashboard_disappears</code></p> <p>For example:</p> <pre><code>func TestAccExampleThing_disappears(t *testing.T) {\nctx := acctest.Context(t)\nrName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\nresourceName := \"aws_example_thing.test\"\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck:                 func() { acctest.PreCheck(ctx, t) },\nErrorCheck:               acctest.ErrorCheck(t, names.ExampleServiceID),\nProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories,\nCheckDestroy:             testAccCheckExampleThingDestroy(ctx),\nSteps: []resource.TestStep{\n{\nConfig: testAccExampleThingConfigName(rName),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheckExampleThingExists(ctx, resourceName, &amp;job),\nacctest.CheckResourceDisappears(ctx, acctest.Provider, ResourceExampleThing(), resourceName),\n),\nExpectNonEmptyPlan: true,\nConfigPlanChecks: resource.ConfigPlanChecks{\nPostApplyPostRefresh: []plancheck.PlanCheck{\nplancheck.ExpectResourceAction(resourceName, plancheck.ResourceActionCreate),\n},\n},\n},\n},\n})\n}\n</code></pre> <p>If this test does fail, the fix for this is generally adding error handling immediately after the <code>Read</code> API call that catches the error and tells Terraform to remove the resource before returning the error:</p> <pre><code>output, err := conn.GetThing(input)\n\nif !d.IsNewResource() &amp;&amp; tfresource.NotFound(err) {\nlog.Printf(\"[WARN] Example Thing (%s) not found, removing from state\", d.Id())\nd.SetId(\"\")\nreturn nil\n}\n\nif err != nil {\nreturn fmt.Errorf(\"reading Example Thing (%s): %w\", d.Id(), err)\n}\n</code></pre> <p>For children resources that are encapsulated by a parent resource, it is also preferable to verify that removing the parent resource will not generate an error either. These are typically named <code>TestAcc{SERVICE}{THING}_disappears_{PARENT}</code>, e.g., <code>TestAccRoute53ZoneAssociation_disappears_Vpc</code></p> <pre><code>func TestAccExampleChildThing_disappears_ParentThing(t *testing.T) {\nctx := acctest.Context(t)\nrName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\nparentResourceName := \"aws_example_parent_thing.test\"\nresourceName := \"aws_example_child_thing.test\"\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck:                 func() { acctest.PreCheck(ctx, t) },\nErrorCheck:               acctest.ErrorCheck(t, names.ExampleServiceID),\nProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories,\nCheckDestroy:             testAccCheckExampleChildThingDestroy(ctx),\nSteps: []resource.TestStep{\n{\nConfig: testAccExampleThingConfigName(rName),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheckExampleThingExists(ctx, resourceName),\nacctest.CheckResourceDisappears(ctx, acctest.Provider, ResourceExampleParentThing(), parentResourceName),\n),\nExpectNonEmptyPlan: true,\nConfigPlanChecks: resource.ConfigPlanChecks{\nPostApplyPostRefresh: []plancheck.PlanCheck{\nplancheck.ExpectResourceAction(parentResourceName, plancheck.ResourceActionCreate),\nplancheck.ExpectResourceAction(resourceName, plancheck.ResourceActionCreate),\n},\n},\n},\n},\n})\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#per-attribute-acceptance-tests","title":"Per Attribute Acceptance Tests","text":"<p>These are typically named <code>TestAcc{SERVICE}{THING}_{ATTRIBUTE}</code>, e.g., <code>TestAccCloudWatchDashboard_Name</code></p> <p>For example:</p> <pre><code>func TestAccExampleThing_Description(t *testing.T) {\nctx := acctest.Context(t)\nrName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\nresourceName := \"aws_example_thing.test\"\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck:                 func() { acctest.PreCheck(ctx, t) },\nErrorCheck:               acctest.ErrorCheck(t, names.ExampleServiceID),\nProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories,\nCheckDestroy:             testAccCheckExampleThingDestroy(ctx),\nSteps: []resource.TestStep{\n{\nConfig: testAccExampleThingConfigDescription(rName, \"description1\"),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheckExampleThingExists(ctx, resourceName),\nresource.TestCheckResourceAttr(resourceName, \"description\", \"description1\"),\n),\n},\n{\nResourceName:      resourceName,\nImportState:       true,\nImportStateVerify: true,\n},\n{\nConfig: testAccExampleThingConfigDescription(rName, \"description2\"),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheckExampleThingExists(ctx, resourceName),\nresource.TestCheckResourceAttr(resourceName, \"description\", \"description2\"),\n),\n},\n},\n})\n}\n\n// below all TestAcc functions\n\nfunc testAccExampleThingConfigDescription(rName string, description string) string {\nreturn fmt.Sprintf(`\nresource \"aws_example_thing\" \"test\" {\n  description = %[2]q\n  name        = %[1]q\n}\n`, rName, description)\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#cross-account-acceptance-tests","title":"Cross-Account Acceptance Tests","text":"<p>When testing requires AWS infrastructure in a second AWS account, the below changes to the normal setup will allow the management or reference of resources and data sources across accounts:</p> <ul> <li>In the <code>PreCheck</code> function, include <code>acctest.PreCheckAlternateAccount(ctx, t)</code> to ensure a standardized set of information is required for cross-account testing credentials</li> <li>Switch usage of <code>ProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories</code> to <code>ProtoV5ProviderFactories: acctest.ProtoV5FactoriesAlternate(ctx, t)</code></li> <li>Add <code>acctest.ConfigAlternateAccountProvider()</code> to the test configuration and use <code>provider = awsalternate</code> for cross-account resources. The resource that is the focus of the acceptance test should not use the alternate provider identification to simplify the testing setup</li> <li>For any <code>TestStep</code> that includes <code>ImportState: true</code>, add the <code>Config</code> that matches the previous <code>TestStep</code> <code>Config</code></li> </ul> <p>An example acceptance test implementation can be seen below:</p> <pre><code>func TestAccExample_basic(t *testing.T) {\nctx := acctest.Context(t)\nresourceName := \"aws_example.test\"\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck: func() {\nacctest.PreCheck(ctx, t)\nacctest.PreCheckAlternateAccount(ctx, t)\n},\nErrorCheck:               acctest.ErrorCheck(t, names.ExampleServiceID),\nProtoV5ProviderFactories: acctest.ProtoV5FactoriesAlternate(ctx, t),\nCheckDestroy:             testAccCheckExampleDestroy(ctx),\nSteps: []resource.TestStep{\n{\nConfig: testAccExampleConfig(),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheckExampleExists(ctx, resourceName),\n// ... additional checks ...\n),\n},\n{\nConfig:            testAccExampleConfig(),\nResourceName:      resourceName,\nImportState:       true,\nImportStateVerify: true,\n},\n},\n})\n}\n\nfunc testAccExampleConfig() string {\nreturn acctest.ConfigCompose(\nacctest.ConfigAlternateAccountProvider(), fmt.Sprintf(`\n# Cross account resources should be handled by the cross account provider.\n# The standardized provider block to use is awsalternate as seen below.\nresource \"aws_cross_account_example\" \"test\" {\n  provider = awsalternate\n\n  # ... configuration ...\n}\n\n# The resource that is the focus of the testing should be handled by the default provider,\n# which is automatically done by not specifying the provider configuration in the resource.\nresource \"aws_example\" \"test\" {\n  # ... configuration ...\n}\n`, ...))\n}\n</code></pre> <p>Searching for the usage of <code>acctest.PreCheckAlternateAccount</code> in the codebase will yield real-world examples of this setup in action.</p>"},{"location":"running-and-writing-acceptance-tests/#cross-region-acceptance-tests","title":"Cross-Region Acceptance Tests","text":"<p>When testing requires AWS infrastructure in a second or third AWS region, the below changes to the normal setup will allow the management or reference of resources and data sources across regions:</p> <ul> <li>In the <code>PreCheck</code> function, include <code>acctest.PreCheckMultipleRegion(t, ###)</code> to ensure a standardized set of information is required for cross-region testing configuration. If the infrastructure in the second AWS region is also in a second AWS account also include <code>acctest.PreCheckAlternateAccount(ctx, t)</code></li> <li>Switch usage of <code>ProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories</code> to <code>ProtoV5ProviderFactories: acctest.ProtoV5FactoriesMultipleRegions(ctx, t, 2)</code> (where the last parameter is number of regions, 2 or 3)</li> <li>Add <code>acctest.ConfigMultipleRegionProvider(###)</code> to the test configuration and use <code>provider = awsalternate</code> (and potentially <code>provider = awsthird</code>) for cross-region resources. The resource that is the focus of the acceptance test should not use the alternative providers to simplify the testing setup. If the infrastructure in the second AWS region is also in a second AWS account use <code>testAccAlternateAccountAlternateRegionProviderConfig()</code> (EC2) instead</li> <li>For any <code>TestStep</code> that includes <code>ImportState: true</code>, add the <code>Config</code> that matches the previous <code>TestStep</code> <code>Config</code></li> </ul> <p>An example acceptance test implementation can be seen below:</p> <pre><code>func TestAccExample_basic(t *testing.T) {\nctx := acctest.Context(t)\nvar providers []*schema.Provider\nresourceName := \"aws_example.test\"\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck: func() {\nacctest.PreCheck(ctx, t)\nacctest.PreCheckMultipleRegion(t, 2)\n},\nErrorCheck:               acctest.ErrorCheck(t, names.ExampleServiceID),\nProtoV5ProviderFactories: acctest.ProtoV5FactoriesMultipleRegions(ctx, t, 2),\nCheckDestroy:             testAccCheckExampleDestroy(ctx),\nSteps: []resource.TestStep{\n{\nConfig: testAccExampleConfig(),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheckExampleExists(ctx, resourceName),\n// ... additional checks ...\n),\n},\n{\nConfig:            testAccExampleConfig(),\nResourceName:      resourceName,\nImportState:       true,\nImportStateVerify: true,\n},\n},\n})\n}\n\nfunc testAccExampleConfig() string {\nreturn acctest.ConfigCompose(\nacctest.ConfigMultipleRegionProvider(2), fmt.Sprintf(`\n# Cross region resources should be handled by the cross region provider.\n# The standardized provider is awsalternate as seen below.\nresource \"aws_cross_region_example\" \"test\" {\n  provider = awsalternate\n\n  # ... configuration ...\n}\n\n# The resource that is the focus of the testing should be handled by the default provider,\n# which is automatically done by not specifying the provider configuration in the resource.\nresource \"aws_example\" \"test\" {\n  # ... configuration ...\n}\n`, ...))\n}\n</code></pre> <p>Searching for the usage of <code>acctest.PreCheckMultipleRegion</code> in the codebase will yield real-world examples of this setup in action.</p>"},{"location":"running-and-writing-acceptance-tests/#acceptance-test-concurrency","title":"Acceptance Test Concurrency","text":"<p>Certain AWS service APIs allow a limited number of a certain component, while the acceptance testing runs at a default concurrency of twenty tests at a time. For example as of this writing, the SageMaker service only allows one SageMaker Domain per AWS Region. Running the tests with the default concurrency will fail with API errors relating to the component quota being exceeded.</p> <p>When encountering these types of components, acceptance testing can be set up to limit the available concurrency of that particular component. When limited to one component at a time, this may also be referred to as serializing the acceptance tests.</p> <p>To convert to serialized (one test at a time) acceptance testing:</p> <ul> <li>Convert all existing capital <code>T</code> test functions with the limited component to begin with a lowercase <code>t</code>, e.g., <code>TestAccSageMakerDomain_basic</code> becomes <code>testDomain_basic</code>. This will prevent the test framework from executing these tests directly as the prefix <code>Test</code> is required.<ul> <li>In each of these test functions, convert <code>resource.ParallelTest</code> to <code>resource.Test</code></li> </ul> </li> <li>Create a capital <code>T</code> <code>TestAcc{Service}{Thing}_serial</code> test function that then references all the lowercase <code>t</code> test functions. If multiple test files are referenced, this new test be created in a new shared file such as <code>internal/service/{SERVICE}/{SERVICE}_test.go</code>. The contents of this test can be set like the following:</li> </ul> <pre><code>func TestAccExampleThing_serial(t *testing.T) {\nt.Parallel()\n\ntestCases := map[string]map[string]func(t *testing.T){\n\"Thing\": {\n\"basic\":        testAccThing_basic,\n\"disappears\":   testAccThing_disappears,\n// ... potentially other resource tests ...\n},\n// ... potentially other top level resource test groups ...\n}\n\nacctest.RunSerialTests2Levels(t, testCases, 0)\n}\n</code></pre> <p>Note</p> <p>Future iterations of these acceptance testing concurrency instructions will include the ability to handle more than one component at a time including service quota lookup, if supported by the service API.</p>"},{"location":"running-and-writing-acceptance-tests/#data-source-acceptance-testing","title":"Data Source Acceptance Testing","text":"<p>Writing acceptance testing for data sources is similar to resources, with the biggest changes being:</p> <ul> <li>Adding <code>DataSource</code> to the test and configuration naming, such as <code>TestAccExampleThingDataSource_Filter</code></li> <li>The basic test may be named after the easiest lookup attribute instead, e.g., <code>TestAccExampleThingDataSource_Name</code></li> <li>No disappears testing</li> <li>Almost all checks should be done with <code>resource.TestCheckResourceAttrPair()</code> to compare the data source attributes to the resource attributes</li> <li>The usage of an additional <code>dataSourceName</code> variable to store a data source reference, e.g., <code>data.aws_example_thing.test</code></li> </ul> <p>Data sources testing should still use the <code>CheckDestroy</code> function of the resource, just to continue verifying that there are no dangling AWS resources after a test is run.</p> <p>Please note that we do not recommend re-using test configurations between resources and their associated data source as it is harder to discover testing regressions. Authors are encouraged to potentially implement similar \"base\" configurations though.</p> <p>For example:</p> <pre><code>func TestAccExampleThingDataSource_Name(t *testing.T) {\nctx := acctest.Context(t)\nrName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\ndataSourceName := \"data.aws_example_thing.test\"\nresourceName := \"aws_example_thing.test\"\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck:                 func() { acctest.PreCheck(ctx, t) },\nErrorCheck:               acctest.ErrorCheck(t, names.ExampleServiceID),\nProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories,\nCheckDestroy:             testAccCheckExampleThingDestroy(ctx),\nSteps: []resource.TestStep{\n{\nConfig: testAccExampleThingDataSourceConfigName(rName),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheckExampleThingExists(ctx, resourceName),\nresource.TestCheckResourceAttrPair(resourceName, \"arn\", dataSourceName, \"arn\"),\nresource.TestCheckResourceAttrPair(resourceName, \"description\", dataSourceName, \"description\"),\nresource.TestCheckResourceAttrPair(resourceName, \"name\", dataSourceName, \"name\"),\n),\n},\n},\n})\n}\n\n// below all TestAcc functions\n\nfunc testAccExampleThingDataSourceConfigName(rName string) string {\nreturn fmt.Sprintf(`\nresource \"aws_example_thing\" \"test\" {\n  name = %[1]q\n}\n\ndata \"aws_example_thing\" \"test\" {\n  name = aws_example_thing.test.name\n}\n`, rName)\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#acceptance-test-sweepers","title":"Acceptance Test Sweepers","text":"<p>When running the acceptance tests, especially when developing or troubleshooting Terraform resources, it's possible for code bugs or other issues to prevent the proper destruction of AWS infrastructure. To prevent lingering resources from consuming quota or causing unexpected billing, the Terraform Plugin SDK supports the test sweeper framework to clear out an AWS region of all resources. This section is meant to augment the SDKv2 documentation on test sweepers with Terraform AWS Provider specific details.</p>"},{"location":"running-and-writing-acceptance-tests/#running-test-sweepers","title":"Running Test Sweepers","text":"<p>Warning</p> <p>Test Sweepers will destroy AWS infrastructure and backups in the target AWS account and region! These are designed to override any API deletion protection. Never run these outside a development AWS account that should be completely empty of resources. </p> <p>To run the sweepers for all resources in <code>us-west-2</code> and <code>us-east-1</code> (default testing regions):</p> <pre><code>make sweep\n</code></pre> <p>To run a specific resource sweeper:</p> <pre><code>SWEEPARGS=-sweep-run=aws_example_thing make sweep\n</code></pre> <p>To run sweepers with an assumed role, use the following additional environment variables:</p> <ul> <li><code>TF_AWS_ASSUME_ROLE_ARN</code> - Required.</li> <li><code>TF_AWS_ASSUME_ROLE_DURATION</code> - Optional, defaults to 1 hour (3600).</li> <li><code>TF_AWS_ASSUME_ROLE_EXTERNAL_ID</code> - Optional.</li> <li><code>TF_AWS_ASSUME_ROLE_SESSION_NAME</code> - Optional.</li> </ul>"},{"location":"running-and-writing-acceptance-tests/#sweeper-checklists","title":"Sweeper Checklists","text":"<ul> <li>Add Resource Sweeper Implementation: See Writing Test Sweepers.</li> <li>Add Service To Sweeper List: Once a <code>sweep.go</code> file is present in the service subdirectory, run <code>make gen</code> to regenerate the list of imports in <code>internal/sweep/sweep_test.go</code>.</li> </ul>"},{"location":"running-and-writing-acceptance-tests/#writing-test-sweepers","title":"Writing Test Sweepers","text":"<p>Sweeper logic should be written to a file called <code>sweep.go</code> in the appropriate service subdirectory (<code>internal/service/{serviceName}</code>).</p> <p>First, implement the sweeper function. If the AWS SDK provides a builtin list paginator for the resource, it should be used:</p> Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <pre><code>func sweepThings(ctx context.Context, client *conns.AWSClient) ([]sweep.Sweepable, error) {\ninput := &amp;example.ListThingsInput{}\nconn := client.ExampleClient(ctx)\nsweepResources := make([]sweep.Sweepable, 0)\n\npaginator := example.NewListThingsPaginator(conn, input)\nfor paginator.HasMorePages() {\npage, err := paginator.NextPage(ctx)\n\nif err != nil {\nreturn nil, err\n}\n\nfor _, v := range page.Things {\nsweepResources = append(sweepResources, framework.NewSweepResource(newResourceThing, client,\nframework.NewAttribute(names.AttrID, aws.ToString(v.ThingId))),\n)\n}\n}\n\nreturn sweepResources, nil\n}\n</code></pre> <pre><code>func sweepThings(ctx context.Context, client *conns.AWSClient) ([]sweep.Sweepable, error) {\ninput := &amp;example.ListThingsInput{}\nconn := client.ExampleClient(ctx)\nsweepResources := make([]sweep.Sweepable, 0)\n\npaginator := example.NewListThingsPaginator(conn, input)\nfor paginator.HasMorePages() {\npage, err := paginator.NextPage(ctx)\n\nif err != nil {\nreturn nil, err\n}\n\nfor _, v := range page.Things {\nr := ResourceThing()\nd := r.Data(nil)\nd.SetId(aws.StringValue(v.Id))\n\nsweepResources = append(sweepResources, sweep.NewSweepResource(r, d, client))\n}\n}\n\nreturn sweepResources, nil\n}\n</code></pre> <p>If no paginator is available, consider generating one using the <code>listpages</code> generator, or implement the sweeper as follows:</p> Terraform Plugin Framework (Preferred)Terraform Plugin SDK V2 <pre><code>func sweepThings(ctx context.Context, client *conns.AWSClient) ([]sweep.Sweepable, error) {\ninput := &amp;example.ListThingsInput{}\nconn := client.ExampleClient(ctx)\nsweepResources := make([]sweep.Sweepable, 0)\n\nfor {\noutput, err := conn.ListThings(ctx, &amp;input)\nif err != nil {\nreturn nil, err\n}\n\nfor _, v := range output.Things {\nsweepResources = append(sweepResources, framework.NewSweepResource(newResourceThing, client,\nframework.NewAttribute(names.AttrID, aws.ToString(v.ThingId))),\n)\n}\n\nif aws.StringValue(output.NextToken) == \"\" {\nbreak\n}\n\ninput.NextToken = output.NextToken\n}\n\nreturn sweepResources, nil\n}\n</code></pre> <pre><code>func sweepThings(ctx context.Context, client *conns.AWSClient) ([]sweep.Sweepable, error) {\ninput := &amp;example.ListThingsInput{}\nconn := client.ExampleClient(ctx)\nsweepResources := make([]sweep.Sweepable, 0)\n\nfor {\noutput, err := conn.ListThings(ctx, &amp;input)\nif err != nil {\nreturn nil, err\n}\n\nfor _, v := range output.Things {\nr := ResourceThing()\nd := r.Data(nil)\nd.SetId(aws.StringValue(v.Id))\n\nsweepResources = append(sweepResources, sweep.NewSweepResource(r, d, client))\n}\n\nif aws.StringValue(output.NextToken) == \"\" {\nbreak\n}\n\ninput.NextToken = output.NextToken\n}\n\nreturn sweepResources, nil\n}\n</code></pre> <p>Once the function is implemented, register it inside the exported <code>RegisterSweepers</code> function. The final argument to the <code>awsv2.Register</code> function is a variadic string which can optionally list any dependencies which must be swept first.</p> <pre><code>func RegisterSweepers() {\nawsv2.Register(\"aws_example_thing\", sweepThings,\n// Optionally, add dependencies\n\"aws_example_other_thing\",\n)\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#acceptance-test-checklists","title":"Acceptance Test Checklists","text":"<p>There are several aspects to writing good acceptance tests. These checklists will help ensure effective testing from the design stage through to implementation details.</p>"},{"location":"running-and-writing-acceptance-tests/#basic-acceptance-test-design","title":"Basic Acceptance Test Design","text":"<p>These are basic principles to help guide the creation of acceptance tests.</p> <ul> <li>Covers Changes: Every line of resource or data source code added or changed should be covered by one or more tests. For example, if a resource has two ways of functioning, tests should cover both possible paths. Nearly every codebase change needs test coverage to ensure functionality and prevent future regressions. If a bug or other problem prompted a fix, a test should be added that previously would have failed, especially if the report included a configuration.</li> <li>Follows the Single Responsibility Principle: Every test should have a single responsibility and effectively test that responsibility. This may include individual tests for verifying basic functionality of the resource (Create, Read, Delete), separately verifying using and updating a single attribute in a resource, or separately changing between two attributes to verify two \"modes\"/\"types\" possible with a resource configuration. In following this principle, test configurations should be as simple as possible. For example, not including extra configuration unless it is necessary for the specific test.</li> </ul>"},{"location":"running-and-writing-acceptance-tests/#test-implementation","title":"Test Implementation","text":"<p>Below are the required items that will be noted during the submission review and prevent immediate merging:</p> <ul> <li>Implements CheckDestroy: Resource testing should include a <code>CheckDestroy</code> function (typically named <code>testAccCheck{SERVICE}{RESOURCE}Destroy</code>) that calls the API to verify that the Terraform resource has been deleted or disassociated as appropriate. More information about <code>CheckDestroy</code> functions can be found in the SDKv2 TestCase documentation.</li> <li>Implements Exists Check Function: Resource testing should include a <code>TestCheckFunc</code> function (typically named <code>testAccCheck{SERVICE}{RESOURCE}Exists</code>) that calls the API to verify that the Terraform resource has been created or associated as appropriate. Preferably, this function will also accept a pointer to an API object representing the Terraform resource from the API response that can be set for potential usage in later <code>TestCheckFunc</code>. More information about these functions can be found in the SDKv2 Custom Check Functions documentation.</li> <li>Excludes Provider Declarations: Test configurations should not include <code>provider \"aws\" {...}</code> declarations. If necessary, only the provider declarations in <code>acctest.go</code> should be used for multiple account/region or otherwise specialized testing.</li> <li>Passes in us-west-2 Region: Tests default to running in <code>us-west-2</code> and at a minimum should pass in that region or include necessary <code>PreCheck</code> functions to skip the test when run outside an expected environment.</li> <li>Includes ErrorCheck: All acceptance tests should include a call to the common ErrorCheck (<code>ErrorCheck:   acctest.ErrorCheck(t, names.ExampleServiceID),</code>).</li> <li>Uses resource.ParallelTest: Tests should use <code>resource.ParallelTest()</code> instead of <code>resource.Test()</code> except where serialized testing is absolutely required.</li> <li>[ ] Uses fmt.Sprintf(): Test configurations preferably should be separated into their own functions (typically named <code>testAcc{SERVICE}{RESOURCE}Config{PURPOSE}</code>) that call <code>fmt.Sprintf()</code> for variable injection or a string <code>const</code> for completely static configurations. Test configurations should avoid <code>var</code> or other variable injection functionality such as <code>text/template</code>.</li> <li>Uses Randomized Infrastructure Naming: Test configurations that use resources where a unique name is required should generate a random name. Typically this is created via <code>rName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)</code> in the acceptance test function before generating the configuration.</li> <li>Prevents S3 Bucket Deletion Errors: Test configurations that use <code>aws_s3_bucket</code> resources as a logging destination should include the <code>force_destroy = true</code> configuration. This is to prevent race conditions where logging objects may be written during the testing duration which will cause <code>BucketNotEmpty</code> errors during deletion.</li> </ul> <p>For resources that support import, the additional item below is required that will be noted during the submission review and prevent immediate merging:</p> <ul> <li>Implements ImportState Testing: Tests should include an additional <code>TestStep</code> configuration that verifies resource import via <code>ImportState: true</code> and <code>ImportStateVerify: true</code>. This <code>TestStep</code> should be added to all possible tests for the resource to ensure that all infrastructure configurations are properly imported into Terraform.</li> </ul> <p>The below are style-based items that may be noted during review and are recommended for simplicity, consistency, and quality assurance:</p> <ul> <li>Uses Builtin Check Functions: Tests should use already available check functions, e.g. <code>resource.TestCheckResourceAttr()</code>, to verify values in the Terraform state over creating custom <code>TestCheckFunc</code>. More information about these functions can be found in the SDKv2 Builtin Check Functions documentation.</li> <li>Uses TestCheckResourceAttrPair() for Data Sources: Tests should use <code>resource.TestCheckResourceAttrPair()</code> to verify values in the Terraform state for data sources attributes to compare them with their expected resource attributes.</li> <li>Excludes Timeouts Configurations: Test configurations should not include <code>timeouts {...}</code> configuration blocks except for explicit testing of customizable timeouts (typically very short timeouts with <code>ExpectError</code>).</li> <li>Implements Default and Zero Value Validation: The basic test for a resource (typically named <code>TestAcc{SERVICE}{RESOURCE}_basic</code>) should use available check functions, e.g. <code>resource.TestCheckResourceAttr()</code>, to verify default and zero values in the Terraform state for all attributes. Empty/missing configuration blocks can be verified with <code>resource.TestCheckResourceAttr(resourceName, \"{ATTRIBUTE}.#\", \"0\")</code> and empty maps with <code>resource.TestCheckResourceAttr(resourceName, \"{ATTRIBUTE}.%\", \"0\")</code></li> </ul>"},{"location":"running-and-writing-acceptance-tests/#avoid-hard-coding","title":"Avoid Hard Coding","text":"<p>Avoid hard coding values in acceptance test checks and configurations for consistency and testing flexibility. Resource testing is expected to pass across multiple AWS environments supported by the Terraform AWS Provider (e.g., AWS Standard and AWS GovCloud (US)). Contributors are not expected or required to perform testing outside of AWS Standard, e.g., running only in the <code>us-west-2</code> region is perfectly acceptable. However, contributors are expected to avoid hard coding with these guidelines.</p>"},{"location":"running-and-writing-acceptance-tests/#hardcoded-account-ids","title":"Hardcoded Account IDs","text":"<ul> <li>Uses Account Data Sources: Any hardcoded account numbers in configuration, e.g., <code>137112412989</code>, should be replaced with a data source. Depending on the situation, there are several data sources for account IDs including:<ul> <li><code>aws_caller_identity</code> data source,</li> <li><code>aws_canonical_user_id</code> data source,</li> <li><code>aws_billing_service_account</code> data source, and</li> <li><code>aws_sagemaker_prebuilt_ecr_image</code> data source.</li> </ul> </li> <li>Uses Account Test Checks: Any check required to verify an AWS Account ID of the current testing account or another account should use one of the following available helper functions over the usage of <code>resource.TestCheckResourceAttrSet()</code> and <code>resource.TestMatchResourceAttr()</code>:<ul> <li><code>acctest.CheckResourceAttrAccountID()</code>: Validates the state value equals the AWS Account ID of the current account running the test. This is the most common implementation.</li> <li><code>acctest.MatchResourceAttrAccountID()</code>: Validates the state value matches any AWS Account ID (e.g. a 12 digit number). This is typically only used in data source testing of AWS managed components.</li> </ul> </li> </ul> <p>Here's an example of using <code>aws_caller_identity</code>:</p> <pre><code>data \"aws_caller_identity\" \"current\" {}\n\nresource \"aws_backup_selection\" \"test\" {\nplan_id      = aws_backup_plan.test.id\nname         = \"tf_acc_test_backup_selection_%[1]d\"\niam_role_arn = \"arn:${data.aws_partition.current.partition}:iam::${data.aws_caller_identity.current.account_id}:role/service-role/AWSBackupDefaultServiceRole\"\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#hardcoded-ami-ids","title":"Hardcoded AMI IDs","text":"<ul> <li>Uses aws_ami Data Source: Any hardcoded AMI ID configuration, e.g. <code>ami-12345678</code>, should be replaced with the <code>aws_ami</code> data source pointing to an Amazon Linux image. The package <code>internal/acctest</code> includes test configuration helper functions to simplify these lookups:<ul> <li><code>acctest.ConfigLatestAmazonLinuxHVMEBSAMI()</code>: The recommended AMI for most situations, using Amazon Linux, HVM virtualization, and EBS storage. To reference the AMI ID in the test configuration: <code>data.aws_ami.amzn-ami-minimal-hvm-ebs.id</code>.</li> <li><code>testAccLatestAmazonLinuxHVMInstanceStoreAMIConfig()</code> (EC2): AMI lookup using Amazon Linux, HVM virtualization, and Instance Store storage. Should only be used in testing that requires Instance Store storage rather than EBS. To reference the AMI ID in the test configuration: <code>data.aws_ami.amzn-ami-minimal-hvm-instance-store.id</code>.</li> <li><code>testAccLatestAmazonLinuxPVEBSAMIConfig()</code> (EC2): AMI lookup using Amazon Linux, Paravirtual virtualization, and EBS storage. Should only be used in testing that requires Paravirtual over Hardware Virtual Machine (HVM) virtualization. To reference the AMI ID in the test configuration: <code>data.aws_ami.amzn-ami-minimal-pv-ebs.id</code>.</li> <li><code>configLatestAmazonLinuxPvInstanceStoreAmi</code> (EC2): AMI lookup using Amazon Linux, Paravirtual virtualization, and Instance Store storage. Should only be used in testing that requires Paravirtual virtualization over HVM and Instance Store storage over EBS. To reference the AMI ID in the test configuration: <code>data.aws_ami.amzn-ami-minimal-pv-instance-store.id</code>.</li> <li><code>testAccLatestWindowsServer2016CoreAMIConfig()</code> (EC2): AMI lookup using Windows Server 2016 Core, HVM virtualization, and EBS storage. Should only be used in testing that requires Windows. To reference the AMI ID in the test configuration: <code>data.aws_ami.win2016core-ami.id</code>.</li> </ul> </li> </ul> <p>Here's an example of using <code>acctest.ConfigLatestAmazonLinuxHVMEBSAMI()</code> and <code>data.aws_ami.amzn-ami-minimal-hvm-ebs.id</code>:</p> <pre><code>func testAccLaunchConfigurationDataSourceConfig_basic(rName string) string {\nreturn acctest.ConfigCompose(\nacctest.ConfigLatestAmazonLinuxHVMEBSAMI(),\nfmt.Sprintf(`\nresource \"aws_launch_configuration\" \"test\" {\n  name          = %[1]q\n  image_id      = data.aws_ami.amzn-ami-minimal-hvm-ebs.id\n  instance_type = \"m1.small\"\n}\n`, rName))\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#hardcoded-availability-zones","title":"Hardcoded Availability Zones","text":"<ul> <li>Uses aws_availability_zones Data Source: Any hardcoded AWS Availability Zone configuration, e.g. <code>us-west-2a</code>, should be replaced with the <code>aws_availability_zones</code> data source. Use the convenience function called <code>acctest.ConfigAvailableAZsNoOptIn()</code> (defined in <code>internal/acctest/acctest.go</code>) to declare <code>data \"aws_availability_zones\" \"available\" {...}</code>. You can then reference the data source via <code>data.aws_availability_zones.available.names[0]</code> or <code>data.aws_availability_zones.available.names[count.index]</code> in resources using <code>count</code>.</li> </ul> <p>Here's an example of using <code>acctest.ConfigAvailableAZsNoOptIn()</code> and <code>data.aws_availability_zones.available.names[0]</code>:</p> <pre><code>func testAccInstanceVpcConfigBasic(rName string) string {\nreturn acctest.ConfigCompose(\nacctest.ConfigAvailableAZsNoOptIn(),\nfmt.Sprintf(`\nresource \"aws_subnet\" \"test\" {\n  availability_zone = data.aws_availability_zones.available.names[0]\n  cidr_block        = \"10.0.0.0/24\"\n  vpc_id            = aws_vpc.test.id\n}\n`, rName))\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#hardcoded-database-versions","title":"Hardcoded Database Versions","text":"<ul> <li>Uses Database Version Data Sources: Hardcoded database versions, e.g., RDS MySQL Engine Version <code>5.7.42</code>, should be removed (which means the AWS-defined default version will be used) or replaced with a list of preferred versions using a data source. Because versions change over time and version offerings vary from region to region and partition to partition, using the default version or providing a list of preferences ensures a version will be available. Depending on the situation, there are several data sources for versions, including:<ul> <li><code>aws_rds_engine_version</code> data source,</li> <li><code>aws_docdb_engine_version</code> data source, and</li> <li><code>aws_neptune_engine_version</code> data source.</li> </ul> </li> </ul> <p>Here's an example of using <code>aws_rds_engine_version</code> and <code>data.aws_rds_engine_version.default.version</code>:</p> <pre><code>data \"aws_rds_engine_version\" \"default\" {\nengine = \"mysql\"\n}\n\ndata \"aws_rds_orderable_db_instance\" \"test\" {\nengine                     = data.aws_rds_engine_version.default.engine\nengine_version             = data.aws_rds_engine_version.default.version\npreferred_instance_classes = [\"db.t3.small\", \"db.t2.small\", \"db.t2.medium\"]\n}\n\nresource \"aws_db_instance\" \"bar\" {\nengine               = data.aws_rds_engine_version.default.engine\nengine_version       = data.aws_rds_engine_version.default.version\ninstance_class       = data.aws_rds_orderable_db_instance.test.instance_class\nskip_final_snapshot  = true\nparameter_group_name = \"default.${data.aws_rds_engine_version.default.parameter_group_family}\"\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#hardcoded-direct-connect-locations","title":"Hardcoded Direct Connect Locations","text":"<ul> <li>Uses aws_dx_locations Data Source: Hardcoded AWS Direct Connect locations, e.g., <code>EqSe2</code>, should be replaced with the <code>aws_dx_locations</code> data source.</li> </ul> <p>Here's an example using <code>data.aws_dx_locations.test.location_codes</code>:</p> <pre><code>data \"aws_dx_locations\" \"test\" {}\n\nresource \"aws_dx_lag\" \"test\" {\nname                  = \"Test LAG\"\nconnections_bandwidth = \"1Gbps\"\nlocation              = tolist(data.aws_dx_locations.test.location_codes)[0]\nforce_destroy         = true\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#hardcoded-instance-types","title":"Hardcoded Instance Types","text":"<ul> <li>Uses Instance Type Data Source: Singular hardcoded instance types and classes, e.g., <code>t2.micro</code> and <code>db.t2.micro</code>, should be replaced with a list of preferences using a data source. Because offerings vary from region to region and partition to partition, providing a list of preferences dramatically improves the likelihood that one of the options will be available. Depending on the situation, there are several data sources for instance types and classes, including:<ul> <li><code>aws_ec2_instance_type_offering</code> data source - Convenience functions declare configurations that are referenced with <code>data.aws_ec2_instance_type_offering.available</code> including:<ul> <li>The <code>acctest.AvailableEC2InstanceTypeForAvailabilityZone()</code> function for test configurations using an EC2 Subnet which is inherently within a single Availability Zone</li> <li>The <code>acctest.AvailableEC2InstanceTypeForRegion()</code> function for test configurations that do not include specific Availability Zones</li> </ul> </li> <li><code>aws_rds_orderable_db_instance</code> data source,</li> <li><code>aws_neptune_orderable_db_instance</code> data source, and</li> <li><code>aws_docdb_orderable_db_instance</code> data source.</li> </ul> </li> </ul> <p>Here's an example of using <code>acctest.AvailableEC2InstanceTypeForRegion()</code> and <code>data.aws_ec2_instance_type_offering.available.instance_type</code>:</p> <pre><code>func testAccSpotInstanceRequestConfig(rInt int) string {\nreturn acctest.ConfigCompose(\nacctest.AvailableEC2InstanceTypeForRegion(\"t3.micro\", \"t2.micro\"),\nfmt.Sprintf(`\nresource \"aws_spot_instance_request\" \"test\" {\n  instance_type        = data.aws_ec2_instance_type_offering.available.instance_type\n  spot_price           = \"0.05\"\n  wait_for_fulfillment = true\n}\n`, rInt))\n}\n</code></pre> <p>Here's an example of using <code>aws_rds_orderable_db_instance</code> and <code>data.aws_rds_orderable_db_instance.test.instance_class</code>:</p> <pre><code>data \"aws_rds_orderable_db_instance\" \"test\" {\nengine                     = \"mysql\"\nengine_version             = \"5.7.31\"\npreferred_instance_classes = [\"db.t3.micro\", \"db.t2.micro\", \"db.t3.small\"]\n}\n\nresource \"aws_db_instance\" \"test\" {\nengine              = data.aws_rds_orderable_db_instance.test.engine\nengine_version      = data.aws_rds_orderable_db_instance.test.engine_version\ninstance_class      = data.aws_rds_orderable_db_instance.test.instance_class\nskip_final_snapshot = true\nusername            = \"test\"\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#hardcoded-partition-dns-suffix","title":"Hardcoded Partition DNS Suffix","text":"<ul> <li>Uses aws_partition Data Source: Any hardcoded DNS suffix configuration, e.g., the <code>amazonaws.com</code> in a <code>ec2.amazonaws.com</code> service principal, should be replaced with the <code>aws_partition</code> data source. A common pattern is declaring <code>data \"aws_partition\" \"current\" {}</code> and referencing it via <code>data.aws_partition.current.dns_suffix</code>.</li> </ul> <p>Here's an example of using <code>aws_partition</code> and <code>data.aws_partition.current.dns_suffix</code>:</p> <pre><code>data \"aws_partition\" \"current\" {}\n\nresource \"aws_iam_role\" \"test\" {\nassume_role_policy = &lt;&lt;POLICY\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"cloudtrail.${data.aws_partition.current.dns_suffix}\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\nPOLICY\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#hardcoded-partition-in-arn","title":"Hardcoded Partition in ARN","text":"<ul> <li>Uses aws_partition Data Source: Any hardcoded AWS Partition configuration, e.g. the <code>aws</code> in a <code>arn:aws:SERVICE:REGION:ACCOUNT:RESOURCE</code> ARN, should be replaced with the <code>aws_partition</code> data source. A common pattern is declaring <code>data \"aws_partition\" \"current\" {}</code> and referencing it via <code>data.aws_partition.current.partition</code>.</li> <li> <p>Uses Builtin ARN Check Functions: Tests should use available ARN check functions to validate ARN attribute values in the Terraform state over <code>resource.TestCheckResourceAttrSet()</code> and <code>resource.TestMatchResourceAttr()</code>:</p> <ul> <li><code>acctest.CheckResourceAttrRegionalARN()</code> verifies that an ARN matches the account ID and region of the test execution with an exact resource value</li> <li><code>acctest.MatchResourceAttrRegionalARN()</code> verifies that an ARN matches the account ID and region of the test execution with a regular expression of the resource value</li> <li><code>acctest.CheckResourceAttrGlobalARN()</code> verifies that an ARN matches the account ID of the test execution with an exact resource value</li> <li><code>acctest.MatchResourceAttrGlobalARN()</code> verifies that an ARN matches the account ID of the test execution with a regular expression of the resource value</li> <li><code>acctest.CheckResourceAttrRegionalARNNoAccount()</code> verifies that an ARN has no account ID and matches the current region of the test execution with an exact resource value</li> <li><code>acctest.CheckResourceAttrGlobalARNNoAccount()</code> verifies that an ARN has no account ID and matches an exact resource value</li> <li><code>acctest.CheckResourceAttrRegionalARNAccountID()</code> verifies that an ARN matches a specific account ID and the current region of the test execution with an exact resource value</li> <li><code>acctest.CheckResourceAttrGlobalARNAccountID()</code> verifies that an ARN matches a specific account ID with an exact resource value</li> </ul> </li> </ul> <p>Here's an example of using <code>aws_partition</code> and <code>data.aws_partition.current.partition</code>:</p> <pre><code>data \"aws_partition\" \"current\" {}\n\nresource \"aws_iam_role_policy_attachment\" \"test\" {\npolicy_arn = \"arn:${data.aws_partition.current.partition}:iam::aws:policy/service-role/AmazonRDSEnhancedMonitoringRole\"\nrole       = aws_iam_role.test.name\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#hardcoded-region","title":"Hardcoded Region","text":"<ul> <li>Uses aws_region Data Source: Any hardcoded AWS Region configuration, e.g., <code>us-west-2</code>, should be replaced with the <code>aws_region</code> data source. A common pattern is declaring <code>data \"aws_region\" \"current\" {}</code> and referencing it via <code>data.aws_region.current.region</code></li> </ul> <p>Here's an example of using <code>aws_region</code> and <code>data.aws_region.current.region</code>:</p> <pre><code>data \"aws_region\" \"current\" {}\n\nresource \"aws_route53_zone\" \"test\" {\nvpc {\nvpc_id     = aws_vpc.test.id\nvpc_region = data.aws_region.current.region\n}\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#hardcoded-spot-price","title":"Hardcoded Spot Price","text":"<ul> <li>Uses aws_ec2_spot_price Data Source: Any hardcoded spot prices, e.g., <code>0.05</code>, should be replaced with the <code>aws_ec2_spot_price</code> data source. A common pattern is declaring <code>data \"aws_ec2_spot_price\" \"current\" {}</code> and referencing it via <code>data.aws_ec2_spot_price.current.spot_price</code>.</li> </ul> <p>Here's an example of using <code>aws_ec2_spot_price</code> and <code>data.aws_ec2_spot_price.current.spot_price</code>:</p> <pre><code>data \"aws_ec2_spot_price\" \"current\" {\ninstance_type = \"t3.medium\"\n\nfilter {\nname   = \"product-description\"\nvalues = [\"Linux/UNIX\"]\n}\n}\n\nresource \"aws_spot_fleet_request\" \"test\" {\nspot_price      = data.aws_ec2_spot_price.current.spot_price\ntarget_capacity = 2\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#hardcoded-ssh-keys","title":"Hardcoded SSH Keys","text":"<ul> <li>Uses acctest.RandSSHKeyPair() or RandSSHKeyPairSize() Functions: Any hardcoded SSH keys should be replaced with random SSH keys generated by either the acceptance testing framework's function <code>RandSSHKeyPair()</code> or the provider function <code>RandSSHKeyPairSize()</code>. <code>RandSSHKeyPair()</code> generates 1024-bit keys.</li> </ul> <p>Here's an example using <code>aws_key_pair</code></p> <pre><code>func TestAccKeyPair_basic(t *testing.T) {\n...\n\nrName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\npublicKey, _, err := acctest.RandSSHKeyPair(acctest.DefaultEmailAddress)\nif err != nil {\nt.Fatalf(\"generating random SSH key: %s\", err)\n}\n\nresource.ParallelTest(t, resource.TestCase{\n...\nSteps: []resource.TestStep{\n{\nConfig: testAccKeyPairConfig(rName, publicKey),\n...\n},\n},\n})\n}\n\nfunc testAccKeyPairConfig(rName, publicKey string) string {\nreturn fmt.Sprintf(`\nresource \"aws_key_pair\" \"test\" {\n  key_name   = %[1]q\n  public_key = %[2]q\n}\n`, rName, publicKey)\n}\n</code></pre>"},{"location":"running-and-writing-acceptance-tests/#hardcoded-email-addresses","title":"Hardcoded Email Addresses","text":"<ul> <li>Uses either acctest.DefaultEmailAddress Constant or acctest.RandomEmailAddress() Function: Any hardcoded email addresses should replaced with either the constant <code>acctest.DefaultEmailAddress</code> or the function <code>acctest.RandomEmailAddress()</code>.</li> </ul> <p>Using <code>acctest.DefaultEmailAddress</code> is preferred when using a single email address in an acceptance test.</p> <p>Here's an example using <code>acctest.DefaultEmailAddress</code></p> <pre><code>func TestAccSNSTopicSubscription_email(t *testing.T) {\n...\n\nrName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\n\nresource.ParallelTest(t, resource.TestCase{\n...\nSteps: []resource.TestStep{\n{\nConfig: testAccTopicSubscriptionEmailConfig(rName, acctest.DefaultEmailAddress),\nCheck: resource.ComposeTestCheckFunc(\n...\nresource.TestCheckResourceAttr(resourceName, \"endpoint\", acctest.DefaultEmailAddress),\n),\n},\n},\n})\n}\n</code></pre> <p>Here's an example using <code>acctest.RandomEmailAddress()</code></p> <pre><code>func TestAccPinpointEmailChannel_basic(t *testing.T) {\n...\n\ndomain := acctest.RandomDomainName()\naddress1 := acctest.RandomEmailAddress(domain)\naddress2 := acctest.RandomEmailAddress(domain)\n\nresource.ParallelTest(t, resource.TestCase{\n...\nSteps: []resource.TestStep{\n{\nConfig: testAccEmailChannelConfig_FromAddress(domain, address1),\nCheck: resource.ComposeTestCheckFunc(\n...\nresource.TestCheckResourceAttr(resourceName, \"from_address\", address1),\n),\n},\n{\nConfig: testAccEmailChannelConfig_FromAddress(domain, address2),\nCheck: resource.ComposeTestCheckFunc(\n...\nresource.TestCheckResourceAttr(resourceName, \"from_address\", address2),\n),\n},\n},\n})\n}\n</code></pre>"},{"location":"skaff/","title":"Provider Scaffolding (skaff)","text":"<p><code>skaff</code> is a Terraform AWS Provider scaffolding command line tool. It generates resource, data source, or function source files, along with test files which adhere to the latest best practices. These files are heavily commented with instructions, serving as the best way to get started with provider development.</p>"},{"location":"skaff/#overview-workflow-steps","title":"Overview workflow steps","text":"<ol> <li>Figure out what you're trying to do:<ul> <li>Resource, data source, or function?</li> <li>Name it.</li> </ul> <p>Tip</p> <p>Net-new resources should be implemented with Terraform Plugin Framework (i.e. the default <code>skaff</code> settings). See Terraform Plugin Development Packages and this issue for additional information.</p> </li> <li>Use <code>skaff</code> to generate provider code.</li> <li>Go through the generated code, customizing as necessary.</li> <li>Run, test, refine.</li> <li>Remove \"TIP\" comments.</li> <li>Submit a pull request.</li> </ol>"},{"location":"skaff/#running-skaff","title":"Running <code>skaff</code>","text":"<ol> <li>Clone the Terraform AWS Provider repository.</li> <li> <p>Install <code>skaff</code>.</p> <pre><code>make skaff\n</code></pre> </li> <li> <p>Change into the appropriate directory.</p> <ul> <li>For resources and data sources, this is the service directory where the new entity will reside, e.g. <code>internal/service/mq</code>.</li> <li>For functions, this is <code>internal/functions</code>.</li> </ul> </li> <li>Generate the resource, data source or function. For example,<ul> <li><code>skaff resource --name BrokerReboot</code>.</li> <li><code>skaff datasource --name IAMRole</code>.</li> <li><code>skaff function --name ARNParse</code>.</li> </ul> </li> </ol> <p>To get help, enter <code>skaff</code> without arguments.</p>"},{"location":"skaff/#usage","title":"Usage","text":""},{"location":"skaff/#help","title":"Help","text":"<pre><code>skaff --help\n</code></pre> <pre><code>Usage:\n  skaff [command]\n\nAvailable Commands:\n  completion  Generate the autocompletion script for the specified shell\n  datasource  Create scaffolding for a data source\n  function    Create scaffolding for a function\n  help        Help about any command\n  resource    Create scaffolding for a resource\n\nFlags:\n  -h, --help   help for skaff\n</code></pre>"},{"location":"skaff/#autocompletion","title":"Autocompletion","text":"<p>Generate the autocompletion script for <code>skaff</code> for the specified shell</p> <pre><code>skaff completion --help\n</code></pre> <pre><code>Usage:\n  skaff completion [command]\n\nAvailable Commands:\n  bash        Generate the autocompletion script for bash\n  fish        Generate the autocompletion script for fish\n  powershell  Generate the autocompletion script for powershell\n  zsh         Generate the autocompletion script for zsh\n\nFlags:\n  -h, --help   help for completion\n\nUse \"skaff completion [command] --help\" for more information about a command\n</code></pre>"},{"location":"skaff/#data-source","title":"Data Source","text":"<p>Create scaffolding for a data source</p> <pre><code>skaff datasource --help\n</code></pre> <pre><code>Create scaffolding for a data source\n\nUsage:\n  skaff datasource [flags]\n\nFlags:\n  -c, --clear-comments     do not include instructional comments in source\n  -f, --force              force creation, overwriting existing files\n  -h, --help               help for datasource\n  -t, --include-tags       Indicate that this resource has tags and the code for tagging should be generated\n  -n, --name string        name of the entity\n  -p, --plugin-sdkv2       generate for Terraform Plugin SDK V2\n  -s, --snakename string   if skaff doesn't get it right, explicitly give name in snake case (e.g., db_vpc_instance)\n</code></pre>"},{"location":"skaff/#function","title":"Function","text":"<p>Create scaffolding for a function.</p> <pre><code>skaff function --help\n</code></pre> <pre><code>Create scaffolding for a function\n\nUsage:\n  skaff function [flags]\n\nFlags:\n  -c, --clear-comments       do not include instructional comments in source\n  -d, --description string   description of the function\n  -f, --force                force creation, overwriting existing files\n  -h, --help                 help for function\n  -n, --name string          name of the function\n  -s, --snakename string     if skaff doesn't get it right, explicitly give name in snake case (e.g., arn_build)\n</code></pre>"},{"location":"skaff/#resource","title":"Resource","text":"<p>Create scaffolding for a resource</p> <pre><code>skaff resource --help\n</code></pre> <pre><code>Create scaffolding for a resource\n\nUsage:\n  skaff resource [flags]\n\nFlags:\n  -c, --clear-comments     do not include instructional comments in source\n  -f, --force              force creation, overwriting existing files\n  -h, --help               help for resource\n  -t, --include-tags       Indicate that this resource has tags and the code for tagging should be generated\n  -n, --name string        name of the entity\n  -p, --plugin-sdkv2       generate for Terraform Plugin SDK V2\n  -s, --snakename string   if skaff doesn't get it right, explicitly give name in snake case (e.g., db_vpc_instance)\n</code></pre>"},{"location":"terraform-plugin-development-packages/","title":"Terraform Plugin Development Packages","text":"<p>The Terraform AWS Provider is constructed with HashiCorp-maintained packages for building plugins. All new resources use Terraform Plugin Framework, however, there are many existing resources implemented with Terraform Plugin SDK V2. A thorough comparison of the packages can be found here.</p>"},{"location":"terraform-plugin-development-packages/#which-plugin-version-should-i-use","title":"Which Plugin Version Should I Use?","text":"<p>All net-new contributions are required to use Terraform Plugin Framework. Maintainers will migrate resources to Terraform Plugin Framework during the review process if necessary, but efforts toward submitting the initial implementation in Terraform Plugin Framework are greatly appreciated.</p> <p>Enhancements or bug fixes to existing Plugin SDKV2 based resources do not require migration. The AWS Provider is muxed to allow existing Terraform Plugin SDK V2 resources and data sources to remain alongside newer Plugin Framework based resources.</p> <p><code>skaff</code> will generate Plugin Framework based resources by default. Where applicable, the contributor guide includes code examples using both Terraform Plugin Framework and Terraform Plugin SDK V2.</p>"},{"location":"terraform-plugin-migrations/","title":"Terraform Plugin Migrations","text":"<p>With the introduction of Terraform Plugin Framework there are now two options for creating resource/data-sources in the provider. All new resources/data-sources must use the new Terraform Plugin Framework which is actively maintained and developed. At this time we do not intend to pursue migration of existing resources from terraform-plugin-sdk to terraform-plugin-framework. While they seem functionally identical, they exhibit enough differences in behavior that it has proven difficult to migrate resources of any complexity without introducing breaking changes. That said there are likely to be simple resources for which the migration will work fine, but we would discourage any attempts to migrate resources of any complexity, particularly those that are heavily used.</p>"},{"location":"terraform-plugin-migrations/#migration-tooling","title":"Migration Tooling","text":"<p>Tooling has been created that will scaffold an existing resource into a Framework resource. This tool is meant to be used as a starting point so additional editing will be needed.</p> <p>Build:</p> <pre><code>make tfsdk2fw\n</code></pre> <p>Convert a resource:</p> <p>The following pattern is used to generate a file:  <code>tfsdk2fw [-resource &lt;resource-type&gt;|-data-source &lt;data-source-type&gt;] &lt;package-name&gt; &lt;name&gt; &lt;generated-file&gt;</code></p> <p>Example:</p> <pre><code>tfsdk2fw -resource aws_example_resource examplepackage ResourceName internal/service/examplepackage/resource_name_fw.go\n</code></pre> <p>This command creates a separate file that exists alongside the existing SDKv2 resource. Ultimately, the new file should replace the SDKv2 resource.</p> <p>When done creating the resource using the Framework run <code>make gen</code> to remove the SDK resource and add the Framework resource to the list of generated service packages.</p>"},{"location":"terraform-plugin-migrations/#state-upgrade","title":"State Upgrade","text":"<p>Terraform Plugin Framework introduced <code>null</code> values, which differ from <code>zero</code> values. Since the Plugin SDKv2 marked both <code>null</code> and <code>zero</code> values as the same, it will be necessary to use the State Upgrader.</p> <p>An example of a resource with an upgraded state, while migrating, can be found here.</p>"},{"location":"terraform-plugin-migrations/#custom-types","title":"Custom Types","text":"<p>The Plugin Framework introduced custom types that allow custom validation on basic types. The following attribute types will require a state upgrade to utilize these custom types.</p> <ul> <li>ARNs</li> <li>CIDR Blocks</li> <li>Duration</li> <li>Timestamps</li> </ul> <p>SDKv2 <code>ARN</code> attribute.</p> <pre><code>func ResourceExampleResource {\nreturn &amp;schema.Schema{\n\"arn_attribute\": {      Type:         schema.TypeString,\nOptional:     true,\nValidateFunc: verify.ValidARN,\n},\n\n// other schema attributes\n}\n}\n</code></pre> <p>Framework <code>ARN</code> attribute.</p> <pre><code>func (r *resourceExampleResource) Schema(ctx context.Context, request resource.SchemaRequest, response *resource.SchemaResponse) {\nreturn schema.Schema{\n\"arn_attribute\": schema.StringAttribute{\nCustomType: fwtypes.ARNType,\nOptional:   true,\nPlanModifiers: []planmodifier.String{\nstringplanmodifier.UseStateForUnknown(),\n},\n},\n\n// other schema attributes\n}\n}\n</code></pre>"},{"location":"terraform-plugin-migrations/#tagging","title":"Tagging","text":"<p>Transparent Tagging that is used in SDKv2 also applies to the Framework by using the <code>@Tags</code> decorator when defining the resource.</p> <pre><code>// @FrameworkResource(\"aws_service_example\", name=\"Example Resource\")\n// @Tags(identifierAttribute=\"arn\")\nfunc newResourceExampleResource(_ context.Context) (resource.ResourceWithConfigure, error) {\nr := resourceExampleResource{}\nreturn &amp;r, nil\n}\n</code></pre>"},{"location":"terraform-plugin-migrations/#testing","title":"Testing","text":"<p>It is important to not cause any state diffs that result in breaking changes. Testing will check that the diff before and after the migration presents no changes.</p> <p>Tip</p> <p><code>VersionConstraint</code> should be set to the most recently published version of the AWS Provider.</p> <pre><code>func TestAccExampleResource_MigrateFromPluginSDK(t *testing.T) {\nctx := acctest.Context(t)\nvar example service.ExampleResourceOutput\nresourceName := \"aws_example_resource.test\"\nrName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\n\nresource.ParallelTest(t, resource.TestCase{\nPreCheck:     func() { acctest.PreCheck(ctx, t); testAccPreCheck(ctx, t) },\nErrorCheck:   acctest.ErrorCheck(t, names.ExampleServiceID),\nCheckDestroy: testAccCheckExampleResourceDestroy(ctx),\nSteps: []resource.TestStep{\n{\nExternalProviders: map[string]resource.ExternalProvider{\n\"aws\": {\nSource:            \"hashicorp/aws\",\nVersionConstraint: \"5.23.0\", // always use most recently published version of the Provider\n},\n},\nConfig: testAccExampleResourceConfig_basic(rName),\nCheck: resource.ComposeTestCheckFunc(\ntestAccCheckExampleResourceExists(ctx, resourceName, &amp;example),\n),\n},\n{\nProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories,\nConfig:                   testAccExampleResourceConfig_basic(rName),\nPlanOnly:                 true,\n},\n},\n})\n}\n</code></pre>"},{"location":"unit-tests/","title":"Unit Tests","text":"<p>Unlike acceptance tests, unit tests do not access AWS and are focused on a function or method. Because of this, they are quick and cheap to run.</p> <p>In designing a resource's implementation, isolate complex bits from AWS bits so that they can be tested through a unit test. We encourage more unit tests in the provider.</p>"},{"location":"unit-tests/#in-context","title":"In context","text":"<p>To help place unit testing in context, here is an overview of the Terraform AWS Provider's three types of tests.</p> <ol> <li>Acceptance tests are end-to-end evaluations of interactions with AWS. They validate functionalities like creating, reading, and destroying resources within AWS.</li> <li>Unit tests (You are here!) focus on testing isolated units of code within the software, typically at the function level. They assess functionalities solely within the provider itself.</li> <li>Continuous integration tests encompass a suite of automated tests that are executed on every pull request and include linting, compiling code, running unit tests, and performing static analysis.</li> </ol>"},{"location":"unit-tests/#what-to-test","title":"What to test","text":"<p>Utilitarian functions that carry out processing within the provider and don't need contact with AWS are candidates for unit testing. Any moderate to very complex utilitarian function should have a corresponding unit test.</p> <p>Rather than being a burden, using unit tests often save time and money during development because they can be run quickly and locally. In addition, rather than mentally processing all edge cases, you can use test cases to refine a function's behavior.</p> <p>Cut and dry functions using well-used patterns, like typical flatteners and expanders (flex functions) don't need unit testing. However, if the flex functions are complex or intricate, they should be unit tested.</p>"},{"location":"unit-tests/#where-they-go","title":"Where they go","text":"<p>Unit tests can be placed in a test file with acceptance tests if they mainly relate to a single resource. If a function is used across resources or is significantly complex, the function and it's unit tests can be moved to a standalone file. If unit tests are included with resource acceptance tests, they should be placed at the top before the first acceptance test.</p>"},{"location":"unit-tests/#example","title":"Example","text":"<p>This is an example of a unit test. Its name is prefixed with <code>Test</code> but not <code>TestAcc</code> like an acceptance test.</p> <pre><code>func TestExampleUnitTest(t *testing.T) {\nt.Parallel()\n\ntestCases := []struct {\nTestName string\nInput    string\nExpected string\nError    bool\n}{\n{\nTestName: \"empty\",\nInput:    \"\",\nExpected: \"\",\nError:    true,\n},\n{\nTestName: \"descriptive name\",\nInput:    \"some input\",\nExpected: \"some output\",\nError:    false,\n},\n{\nTestName: \"another descriptive name\",\nInput:    \"more input\",\nExpected: \"more output\",\nError:    false,\n},\n}\n\nfor _, testCase := range testCases {\nt.Run(testCase.TestName, func(t *testing.T) {\nt.Parallel()\ngot, err := tfrds.FunctionFromResource(testCase.Input)\n\nif err != nil &amp;&amp; !testCase.Error {\nt.Errorf(\"got error (%s), expected no error\", err)\n}\n\nif err == nil &amp;&amp; testCase.Error {\nt.Errorf(\"got (%s) and no error, expected error\", got)\n}\n\nif got != testCase.Expected {\nt.Errorf(\"got %s, expected %s\", got, testCase.Expected)\n}\n})\n}\n}\n</code></pre>"},{"location":"ai-agent-guides/arn-based-resource-identity/","title":"Adding Resource Identity to ARN-based Resources","text":"<p>You are working on the Terraform AWS Provider, specifically focused on adding resource identity to existing ARN-based Plugin SDKV2 resources. This Github issue contains the resources that need resource identity support and have an ARN based identity.</p> <p>When adding resource identity, all resources in a service should be done in the same pull request. Follow the steps below to complete this task.</p>"},{"location":"ai-agent-guides/arn-based-resource-identity/#1-prepare-the-branch","title":"1. Prepare the branch","text":"<ul> <li>The feature branch name should begin with <code>f-ri</code> and be suffixed with the name of the service being updated, e.g. <code>f-ri-elbv2</code>. If the current branch does not match this convention, create one.</li> <li>Ensure the feature branch is rebased with the <code>main</code> branch.</li> </ul>"},{"location":"ai-agent-guides/arn-based-resource-identity/#2-add-resource-identity-to-each-resource","title":"2. Add resource identity to each resource","text":"<p>The changes for each individual resource should be done in its own commit. Use the following steps to add resource identity to an existing resource:</p> <ul> <li>Add the <code>@ArnIdentity</code> annotation to the target resource. If the corresponding property is named <code>arn</code>, then the <code>arnAttribute</code> value does not need to be specified.</li> <li>If the resource's test file uses a <code>CheckExists</code> helper function that accepts 3 parameters rather than 2 (you can check this in the resource's test file), add another annotation to the resource file in the format <code>// @Testing(existsType=\"github.com/aws/aws-sdk-go-v2/service/elasticloadbalancingv2/types;types.TrustStore\")</code>, but replacing the type with the correct one for the resource in question. The type should match the third parameter of the CheckExists function.</li> <li>Since we are newly adding identity to this resource, add an annotation indicating the most recent pre-identity version, e.g. <code>@Testing(preIdentityVersion=\"v6.3.0\")</code>. Use <code>CHANGELOG.md</code> at the project root to determine the most recently released version (which will be the last before identity is added).</li> <li>Some resources will have an importer function defined. If that function uses <code>schema.ImportStatePassthroughContext</code> as <code>StateContext</code> value then remove that importer function declaration as it is no longer necessary.</li> <li>If the service does not use generated tag tests, you will need to create template files in the <code>testdata/tmpl</code> directory. For each resource, create a file named <code>&lt;resource&gt;_tags.gtpl</code> (e.g., <code>trust_store_tags.gtpl</code>).</li> <li>Populate each template file with the configuration from the resource's <code>_basic</code> test. If populating from the <code>_basic</code> configuration, be sure to replace any string format directives (e.g. <code>name = %[1]q</code>) with a corresponding reference to a variable (e.g. <code>name = var.rName</code>).</li> <li>The generators will use the template files to generate the resource identity test configuration. These will be located in the <code>testdata</code> directory for the service. Do not manually create test directories or files as they will be generated.</li> <li>The region template must be included inside each resource block in the template files. Add it as the first line after the resource declaration:</li> </ul> <pre><code>resource \"aws_service_thing\" \"test\" {\n{{- template \"region\" }}\n  name = var.rName\n{{- template \"tags\" }}\n}\n</code></pre> <ul> <li>If the test configuration references an <code>aws_region</code> data source, the region template should also be embedded here.</li> </ul> <pre><code>data \"aws_region\" \"current\" {\n{{- template \"region\" }}\n}\n</code></pre>"},{"location":"ai-agent-guides/arn-based-resource-identity/#3-generate-and-test-the-changes","title":"3. Generate and test the changes","text":"<ul> <li>Run the generators for this service. This can be done with the following command (e.g. for the elbv2 package): <code>go generate ./internal/service/elbv2/...</code>. This will generate tests for Resource Identity and any required test files.</li> <li>Run the tests in this order:<ul> <li>First run the basic identity test: <code>make testacc PKG=&lt;service&gt; TESTS=TestAcc&lt;Resource&gt;_Identity_Basic</code></li> <li>Run all identity tests: <code>make testacc PKG=&lt;service&gt; TESTS=TestAcc&lt;Resource&gt;_Identity</code></li> <li>Finally, run all tests for the resource: <code>make testacc PKG=&lt;service&gt; TESTS=TestAcc&lt;Resource&gt;_</code>. Always include the <code>PKG</code> parameter to properly scope the tests to the intended service package.</li> </ul> </li> <li>Ensure the template modifications have not introduced any structural changes that would fail <code>terraform fmt</code>. To verify, run <code>terraform fmt -recursive -check</code>, and confirm there is no output.</li> <li>If all the preceding steps complete successfully commit the changes with an appropriate message, e.g. <code>r/aws_lb_target_group: add resource identity</code>. Ensure the commit message body includes the results of the acceptance test run in the previous step.</li> </ul> <p>Repeat steps 2 and 3 for each resource in the service. When all resources are complete, proceed to the next section.</p>"},{"location":"ai-agent-guides/arn-based-resource-identity/#4-update-import-documentation","title":"4. Update import documentation","text":"<ul> <li>Update the import section of the registry documentation for each resource following the template below.</li> </ul> <pre><code>In Terraform v1.12.0 and later, the [`import` block](https://developer.hashicorp.com/terraform/language/import) can be used with the `identity` attribute. For example:\n\n```terraform\nimport {\nto = &lt;resource-name&gt;.example\nidentity = {\n\"arn\" = &lt;example-arn-value&gt;\n}\n}\n\nresource \"&lt;resource-name&gt;\" \"example\" {\n  ### Configuration omitted for brevity ###\n}\n```\n\n### Identity Schema\n\n#### Required\n\n- `arn` (String) &lt;description here&gt;.\n</code></pre> <ul> <li>The instructions for importing by <code>identity</code>, including the identity schema, should appear before instructions for import blocks with an <code>id</code> argument or importing via the CLI.</li> <li>Refer to <code>website/docs/r/acm_certificate.html.markdown</code> for a reference implementation.</li> </ul>"},{"location":"ai-agent-guides/arn-based-resource-identity/#5-submit-a-pull-request","title":"5. Submit a pull request","text":"<p>!!!Important!!!: Ask for confirmation before proceeding with this step.</p> <ul> <li>Push the changes.</li> <li>Create a draft pull request with the following details:<ul> <li>Title: \"Add ARN-based resource identity to <code>&lt;service-name&gt;</code>\", e.g. \"Add ARN-based resource identity to <code>elbv2</code>\".</li> <li>Use the following template for the body. Be sure to replace the acceptance test results section with the full results</li> </ul> </li> </ul> <pre><code>### Description\nAdd resource identity to ARN-based resources in `&lt;service-name&gt;`. This includes:\n\n&lt;list Terraform resource names here&gt;\n\n### Relations\nRelates #42983\nRelates #42984\n\n### Output from Acceptance Testing\n\n&lt;insert acceptance test results here&gt;\n</code></pre> <ul> <li>Once the pull request is created, fetch the PR number to add changelog entries. Create a new file, <code>.changelog/&lt;pr-number&gt;.txt</code>, and include one enhancement entry per resource. Refer to <code>.changelog/43503.txt</code> for the appropriate formatting.</li> <li>Provide a summary of the completed changes.</li> </ul>"},{"location":"ai-agent-guides/arn-based-resource-identity/#common-issues-and-troubleshooting","title":"Common Issues and Troubleshooting","text":""},{"location":"ai-agent-guides/arn-based-resource-identity/#test-failures","title":"Test Failures","text":"<ul> <li>Ensure <code>PKG</code> parameter is included in test commands</li> <li>Verify template file names match exactly (<code>&lt;resource&gt;_tags.gtpl</code>)</li> <li>Check region template placement is inside resource blocks</li> <li>Don't create test directories manually - let the generator create them</li> <li>If a generated test panics because a <code>testAccCheck*Exists</code> helper function has incorrect arguments, add a <code>@Testing(existsType=\"\")</code> annotation. NEVER modify the function signature of an existing \"exists\" helper function</li> </ul>"},{"location":"ai-agent-guides/arn-based-resource-identity/#generator-issues","title":"Generator Issues","text":"<ul> <li>Remove any manually created test directories before running the generator</li> <li>Ensure template files are in the correct location (<code>testdata/tmpl</code>)</li> <li>Verify template file names match the resource name</li> <li>If identity tests are not generated, verify that the <code>identitytests</code> generator is being called within the service's <code>generate.go</code> file. If it isn't, add the following line to <code>generate.go</code> next to the existing <code>go:generate</code> directives.</li> <li>If a generated test does not reference the <code>var.rName</code> variable, add an <code>// @Testing(generator=false)</code> annotation to remove it from the generated configuration.</li> </ul> <pre><code>//go:generate go run ../../generate/identitytests/main.go\n</code></pre>"},{"location":"ai-agent-guides/arn-based-resource-identity/#resource-updates","title":"Resource Updates","text":"<ul> <li>Check if the resource's check exists helper takes 3 parameters</li> <li>Verify the correct type is used in the <code>existsType</code> annotation</li> <li>Ensure importer is only removed if using <code>ImportStatePassthroughContext</code></li> </ul>"},{"location":"ai-agent-guides/go-vcr-migration/","title":"Adding <code>go-vcr</code> Support","text":"<p>You are working on the Terraform AWS Provider, specifically focused on enabling support for <code>go-vcr</code>.</p> <p>Follow the steps below to enable support for a single service.</p> <ul> <li>The working branch name should begin with <code>f-go-vcr-</code> and be suffixed with the name of the service being updated, e.g. <code>f-go-vcr-s3</code>. If the current branch does not match this convention, create one. Ensure the branch is rebased with the <code>main</code> branch.</li> <li>Follow the steps on this page to enable <code>go-vcr</code> for the target service.</li> <li>Once all acceptace tests are passing, commit the changes with a message like \"service-name: enable <code>go-vcr</code> support\", replacing <code>service-name</code> with the target service. Be sure to include the COMPLETE output from acceptance testing in the commit body, wrapped in a <code>console</code> code block. e.g.</li> </ul> <pre><code>% make testacc PKG=polly VCR_MODE=REPLAY_ONLY VCR_PATH=/tmp/polly-vcr-testdata/\n\nEnables `go-vcr` for the `&lt;service-name&gt;` service.\n\n&lt;-- full results here --&gt;\n</code></pre>"},{"location":"ai-agent-guides/parameterized-resource-identity/","title":"Adding Resource Identity to parameterized Resources","text":"<p>You are working on the Terraform AWS Provider, specifically focused on adding resource identity to Plugin SDKV2 resources whose identity is composed from multiple parameters (parameterized). This Github meta issue contains details and sub-issues related to adding resource identity support.</p> <p>When adding resource identity, a pull request may include all resources in a service or a single resource. Follow the steps below to complete this task.</p>"},{"location":"ai-agent-guides/parameterized-resource-identity/#1-prepare-the-branch","title":"1. Prepare the branch","text":"<ul> <li>The feature branch name should begin with <code>f-ri</code> and be suffixed with the name of the service being updated, e.g. <code>f-ri-elbv2</code>. If the current branch does not match this convention, create one.</li> <li>Ensure the feature branch is rebased with the <code>main</code> branch.</li> </ul>"},{"location":"ai-agent-guides/parameterized-resource-identity/#2-add-resource-identity-to-each-resource","title":"2. Add resource identity to each resource","text":"<p>The changes for each individual resource should be done in its own commit. Use the following steps to add resource identity to an existing resource:</p> <ul> <li>Determine which arguments the resource identity is composed from. This may be a single argument mapping to an AWS-generated identifier, or a combination of multiple arguments. Check for places where the resource ID is set (e.g. <code>d.SetId(&lt;value&gt;)</code>) and infer the relevant parameters.</li> <li>Add an <code>@IdentityAttribute(\"&lt;argument_name&gt;\")</code> annotation to the target resource. For resources where the ID is composed from multiple arguments, add one annotation for each argument.</li> <li>If the <code>id</code> attribute is set to the same value as an identity attribute, add an <code>@Testing(idAttrDuplicates=\"&lt;argument_name&gt;\")</code> annotation.</li> <li>If the resource's test file uses a <code>CheckExists</code> helper function that accepts 3 parameters rather than 2 (you can check this in the resource's test file), add another annotation to the resource file in the format <code>// @Testing(existsType=\"github.com/aws/aws-sdk-go-v2/service/elasticloadbalancingv2/types;types.TrustStore\")</code>, but replacing the type with the correct one for the resource in question. The type should match the third parameter of the CheckExists function.</li> <li>Since we are newly adding identity to this resource, add an annotation indicating the most recent pre-identity version, e.g. <code>@Testing(preIdentityVersion=\"v6.3.0\")</code>. Use <code>CHANGELOG.md</code> at the project root to determine the most recently released version (which will be the last before identity is added).</li> <li>Some resources will have an importer function defined.<ul> <li>If that function uses <code>schema.ImportStatePassthroughContext</code> as <code>StateContext</code> value then remove that importer function declaration as it is no longer necessary.</li> <li>If a custom import function is defined, add a <code>// @CustomImport</code> annotation and include the following at the beginning the custom <code>StateContext</code> function:</li> </ul> </li> </ul> <pre><code>                identitySpec := importer.IdentitySpec(ctx)\nif err := importer.RegionalSingleParameterized(ctx, d, identitySpec, meta.(importer.AWSClient)); err != nil {\nreturn nil, err\n}\n</code></pre> <ul> <li>If the service does not use generated tag tests, you will need to create template files in the <code>testdata/tmpl</code> directory. For each resource, create a file named <code>&lt;resource&gt;_tags.gtpl</code> (e.g., <code>trust_store_tags.gtpl</code>).</li> <li>Populate each template file with the configuration from the resource's <code>_basic</code> test. If populating from the <code>_basic</code> configuration, be sure to replace any string format directives (e.g. <code>name = %[1]q</code>) with a corresponding reference to a variable (e.g. <code>name = var.rName</code>).</li> <li>The generators will use the template files to generate the resource identity test configuration. These will be located in the <code>testdata</code> directory for the service. Do not manually create test directories or files as they will be generated.</li> <li>The region template must be included inside each resource block in the template files. Add it as the first line after the resource declaration:</li> </ul> <pre><code>resource \"aws_service_thing\" \"test\" {\n{{- template \"region\" }}\n  name = var.rName\n{{- template \"tags\" }}\n}\n</code></pre> <ul> <li>If the resource already has a tags template declaration different than the example above, e.g. <code>{{- template \"tags\" . }}</code>, leave it unchanged.</li> <li>If the test configuration references an <code>aws_region</code> data source, the region template should also be embedded here.</li> </ul> <pre><code>data \"aws_region\" \"current\" {\n{{- template \"region\" }}\n}\n</code></pre>"},{"location":"ai-agent-guides/parameterized-resource-identity/#3-generate-and-test-the-changes","title":"3. Generate and test the changes","text":"<ul> <li>Run the generators for this service. This can be done with the following command (e.g. for the elbv2 package): <code>go generate ./internal/service/elbv2/...</code>. This will generate tests for Resource Identity and any required test files.</li> <li>Run the tests in this order:<ul> <li>First run the basic identity test: <code>make testacc PKG=&lt;service&gt; TESTS=TestAcc&lt;Resource&gt;_Identity_Basic</code></li> <li>Run all identity tests: <code>make testacc PKG=&lt;service&gt; TESTS=TestAcc&lt;Resource&gt;_Identity</code></li> <li>Finally, run all tests for the resource: <code>make testacc PKG=&lt;service&gt; TESTS=TestAcc&lt;Resource&gt;_</code>. Always include the <code>PKG</code> parameter to properly scope the tests to the intended service package.</li> </ul> </li> <li>Ensure the template modifications have not introduced any structural changes that would fail <code>terraform fmt</code>. To verify, run <code>terraform fmt -recursive -check</code>, and confirm there is no output.</li> <li>If all the preceding steps complete successfully commit the changes with an appropriate message, e.g. <code>r/aws_lb_target_group: add resource identity</code>. Ensure the commit message body includes the results of the acceptance test run in the previous step.</li> </ul> <p>Repeat steps 2 and 3 for each resource in the service. When all resources are complete, proceed to the next section.</p>"},{"location":"ai-agent-guides/parameterized-resource-identity/#4-update-import-documentation","title":"4. Update import documentation","text":"<ul> <li>Update the import section of the registry documentation for each resource following the template below.</li> </ul> <pre><code>In Terraform v1.12.0 and later, the [`import` block](https://developer.hashicorp.com/terraform/language/import) can be used with the `identity` attribute. For example:\n\n```terraform\nimport {\nto = &lt;resource-name&gt;.example\nidentity = {\n&lt;required key/value pairs here&gt;\n}\n}\n\nresource \"&lt;resource-name&gt;\" \"example\" {\n  ### Configuration omitted for brevity ###\n}\n```\n\n### Identity Schema\n\n#### Required\n\n&lt;required attributes here&gt;\n\n#### Optional\n\n* `account_id` (String) AWS Account where this resource is managed.\n* `region` (String) Region where this resource is managed.\n</code></pre> <ul> <li>The instructions for importing by <code>identity</code>, including the identity schema, should appear before instructions for import blocks with an <code>id</code> argument or importing via the CLI.</li> <li>Refer to <code>website/docs/r/kms_key.html.markdown</code> for a reference implementation.</li> </ul>"},{"location":"ai-agent-guides/parameterized-resource-identity/#5-submit-a-pull-request","title":"5. Submit a pull request","text":"<p>!!!Important!!!: Ask for confirmation before proceeding with this step.</p> <ul> <li>Push the changes.</li> <li>Create a draft pull request with the following details:<ul> <li>Title: \"Add parameterized resource identity to <code>&lt;service-name&gt;</code>\", e.g. \"Add parameterized resource identity to <code>elbv2</code>\". If only a single resource is included, replace service-name with the full Terraform resource name.</li> <li>Use the following template for the body. Be sure to replace the acceptance test results section with the results from the full acceptance test suite run.</li> </ul> </li> </ul> <pre><code>### Description\nAdd resource identity to parameterized resources in `&lt;service-name&gt;`. This includes:\n\n&lt;list Terraform resource names here&gt;\n\n### Relations\nRelates #42983\nRelates #42988\n\n### Output from Acceptance Testing\n\n&lt;insert acceptance test results here&gt;\n</code></pre> <ul> <li>Once the pull request is created, fetch the PR number to add changelog entries. Create a new file, <code>.changelog/&lt;pr-number&gt;.txt</code>, and include one enhancement entry per resource. Refer to <code>.changelog/43503.txt</code> for the appropriate formatting.</li> <li>Provide a summary of the completed changes.</li> </ul>"},{"location":"ai-agent-guides/parameterized-resource-identity/#common-issues-and-troubleshooting","title":"Common Issues and Troubleshooting","text":""},{"location":"ai-agent-guides/parameterized-resource-identity/#test-failures","title":"Test Failures","text":"<ul> <li>Ensure <code>PKG</code> parameter is included in test commands</li> <li>Verify template file names match exactly (<code>&lt;resource&gt;_tags.gtpl</code>)</li> <li>Check region template placement is inside resource blocks</li> <li>Don't create test directories manually - let the generator create them</li> <li>If a generated test panics because a <code>testAccCheck*Exists</code> helper function has incorrect arguments, add a <code>@Testing(existsType=\"\")</code> annotation. NEVER modify the function signature of an existing \"exists\" helper function</li> </ul>"},{"location":"ai-agent-guides/parameterized-resource-identity/#generator-issues","title":"Generator Issues","text":"<ul> <li>Remove any manually created test directories before running the generator</li> <li>Ensure template files are in the correct location (<code>testdata/tmpl</code>)</li> <li>Verify template file names match the resource name</li> <li>If identity tests are not generated, verify that the <code>identitytests</code> generator is being called within the service's <code>generate.go</code> file. If it isn't, add the following line to <code>generate.go</code> next to the existing <code>go:generate</code> directives.</li> <li>If a generated test does not reference the <code>var.rName</code> variable, add an <code>// @Testing(generator=false)</code> annotation to remove it from the generated configuration.</li> </ul> <pre><code>//go:generate go run ../../generate/identitytests/main.go\n</code></pre>"},{"location":"ai-agent-guides/parameterized-resource-identity/#resource-updates","title":"Resource Updates","text":"<ul> <li>Check if the resource's check exists helper takes 3 parameters</li> <li>Verify the correct type is used in the <code>existsType</code> annotation</li> <li>Ensure importer is only removed if using <code>ImportStatePassthroughContext</code></li> </ul>"},{"location":"ai-agent-guides/parameterized-resource-identity/#import-test-failures","title":"Import Test Failures","text":"<ul> <li>If identity tests are failing because they expect an update during import but get a no-op, add an <code>// @Testing(plannableImportAction=\"NoOp\")</code> annotation and re-generate the test files.</li> <li>If identity tests are failing import verification due to missing attribute values, check the <code>_basic</code> test implementation for the presence of an <code>ImportStateVerifyIgnore</code> field in the import test step. If present, add an <code>// @Testing(importIgnore=\"arg1\")</code> annotation where <code>arg1</code> is replaced with the argument name(s) from the verify ignore slice. If mutiple fields are ignored, separate field names with a <code>;</code>, e.g. <code>arg1;arg2</code>.</li> <li>If a region override test is failing and a custom import fuction is configured, ensure the appropriate helper function from the <code>importer</code> package is used.<ul> <li><code>RegionalSingleParameterized</code> - regional resources whose identity is made up of a single parameter.</li> <li><code>GlobalSingleParameterized</code> - global resources whose identity is made up of a single parameter.</li> <li><code>RegionalMultipleParameterized</code> - regional resources whose identity is made up of multiple parameters.</li> <li><code>GlobalMultipleParameterized</code> - global resources whose identity is made up of multiple parameters.</li> </ul> </li> </ul>"},{"location":"ai-agent-guides/smarterr/","title":"smarterr Migration Guide for AI and Human Contributors","text":"<p>This document is designed to enable AI systems (and humans) to fully and accurately migrate Go code in the Terraform AWS Provider from legacy error handling to the <code>smarterr</code>/<code>smerr</code> system. It provides explicit, pattern-based instructions for replacing all legacy error/diagnostic calls and bare error returns with the correct <code>smarterr</code>/<code>smerr</code> usage. Follow these rules exactly for every migration.</p>"},{"location":"ai-agent-guides/smarterr/#what-is-smarterr","title":"What is smarterr?","text":"<p><code>smarterr</code> is a config-driven Go library for formatting and annotating errors in a consistent, helpful, and composable way. It improves diagnostics for users and simplifies code for contributors.</p> <ul> <li>Use <code>smerr</code> (the provider's wrapper) in almost all cases, not <code>smarterr</code> directly.</li> <li><code>smerr</code> injects provider context and simplifies usage for both SDKv2 and Framework resources.</li> </ul>"},{"location":"ai-agent-guides/smarterr/#migration-rules-legacy-smarterrsmerr","title":"Migration Rules: Legacy \u2192 smarterr/smerr","text":""},{"location":"ai-agent-guides/smarterr/#1-replace-all-legacy-diagnosticerror-calls","title":"1. Replace All Legacy Diagnostic/Error Calls","text":"<p>For each of the following legacy calls, replace as shown:</p> Legacy Call Replace With <code>sdkdiag.AppendFromErr(diags, err)</code> <code>smerr.Append(ctx, diags, err, smerr.ID, ...)</code> <code>sdkdiag.AppendErrorf(diags, ..., err)</code> <code>smerr.Append(ctx, diags, err, smerr.ID, ...)</code> <code>create.AppendDiagError(diags, ..., err)</code> <code>smerr.Append(ctx, diags, err, smerr.ID, ...)</code> <code>response.Diagnostics.AddError(..., err.Error())</code> <code>smerr.AddError(ctx, &amp;response.Diagnostics, err, smerr.ID, ...)</code> <code>resp.Diagnostics.AddError(..., err.Error())</code> <code>smerr.AddError(ctx, &amp;resp.Diagnostics, err, smerr.ID, ...)</code> <code>create.AddError(&amp;response.Diagnostics, ..., err)</code> <code>smerr.AddError(ctx, &amp;response.Diagnostics, err, smerr.ID, ...)</code> <code>return nil, err</code> <code>return nil, smarterr.NewError(err)</code> <code>return nil, &amp;retry.NotFoundError{ LastError: err, LastRequest: ..., }</code> <code>return nil, smarterr.NewError(&amp;retry.NotFoundError{ LastError: err, LastRequest: ..., })</code> <code>return nil, tfresource.NewEmptyResultError(...)</code> <code>return nil, smarterr.NewError(tfresource.NewEmptyResultError(...))</code> <code>return tfresource.AssertSingleValueResult(...)</code> <code>return smarterr.Assert(tfresource.AssertSingleValueResult(...))</code> <p>Examples:</p> <ul> <li><code>sdkdiag.AppendFromErr(diags, err)</code> \u2192 <code>smerr.Append(ctx, diags, err)</code></li> <li><code>sdkdiag.AppendErrorf(diags, \"updating EC2 Instance (%s): %s\", d.Id(), err)</code> \u2192 <code>smerr.Append(ctx, diags, err, smerr.ID, d.Id())</code></li> <li><code>sdkdiag.AppendErrorf(diags, \"creating EC2 Instance: %s\", err)</code> \u2192 <code>smerr.Append(ctx, diags, err, smerr.ID, d.Id())</code></li> <li><code>create.AppendDiagError(diags, names.CodeBuild, create.ErrActionCreating, resNameFleet, d.Get(names.AttrName).(string), err)</code> \u2192 <code>smerr.Append(ctx, diags, err, smerr.ID, d.Get(names.AttrName).(string))</code></li> <li><code>response.Diagnostics.AddError(\"creating EC2 EBS Fast Snapshot Restore\", err.Error())</code> \u2192 <code>smerr.AddError(ctx, &amp;response.Diagnostics, err, smerr.ID, new.ID.ValueString())</code></li> <li><code>response.Diagnostics.AddError(fmt.Sprintf(\"updating VPC Security Group Rule (%s)\", new.ID.ValueString()), err.Error())</code> \u2192 <code>smerr.AddError(ctx, &amp;response.Diagnostics, err, smerr.ID, new.ID.ValueString())</code></li> <li><code>resp.Diagnostics.AddError(create.ProblemStandardMessage(..., err), err.Error())</code> \u2192 <code>smerr.AddError(ctx, &amp;resp.Diagnostics, err, smerr.ID, ...)</code></li> <li><code>create.AddError(&amp;response.Diagnostics, names.DRS, create.ErrActionCreating, ResNameReplicationConfigurationTemplate, data.ID.ValueString(), err)</code> \u2192 <code>smerr.AddError(ctx, &amp;response.Diagnostics, err, smerr.ID, data.ID.ValueString())</code></li> </ul> <p>General Rule:</p> <ul> <li>Always pass <code>ctx</code> as the first argument, and the diagnostics object as the second.</li> <li>Always pass the error as the third argument.</li> <li>Always pass <code>smerr.ID</code> and any available resource ID or context as additional arguments.</li> </ul>"},{"location":"ai-agent-guides/smarterr/#including-identifiers","title":"Including identifiers","text":"<p>smarterr's <code>EnrichAppend</code>, <code>AddError</code>, and <code>Append</code> take variadic keyvals. Where possible include <code>smerr.ID</code> (key) and the ID (value) (such as <code>d.Id()</code>, <code>state.RuleName.String()</code>, <code>plan.ResourceArn.String()</code>).</p> <ul> <li>If no ID available (e.g., early in <code>Create</code>), something like <code>smerr.EnrichAppend(ctx, &amp;resp.Diagnostics, req.State.Get(ctx, &amp;state))</code>, without ID, is okay</li> <li>But, if ID is available (e.g., read, update, delete, middle-to-end of create), use something like <code>smerr.EnrichAppend(ctx, &amp;resp.Diagnostics, fwflex.Flatten(ctx, out, &amp;state), smerr.ID, state.RuleName.String())</code>, with the ID</li> <li>IDs may be names, ARNs, IDs, combinations, etc.</li> <li>In SDK, you cannot use <code>d.Id()</code> until after <code>d.SetId()</code></li> <li>The legacy call will often use an ID. If so, use that.</li> <li>If the legacy call doesn't include the ID, but it is available, add it.</li> </ul>"},{"location":"ai-agent-guides/smarterr/#2-replace-all-bare-error-returns","title":"2. Replace All Bare Error Returns","text":"<p>Before:</p> <pre><code>return nil, err\n</code></pre> <p>After:</p> <pre><code>return nil, smarterr.NewError(err)\n</code></pre>"},{"location":"ai-agent-guides/smarterr/#3-wrap-tfresource-helpers","title":"3. Wrap tfresource Helpers","text":"<p>Before:</p> <pre><code>return tfresource.AssertSingleValueResult(...)\n</code></pre> <p>After:</p> <pre><code>return smarterr.Assert(tfresource.AssertSingleValueResult(...))\n</code></pre> <p>Before:</p> <pre><code>return nil, tfresource.NewEmptyResultError(...)\n</code></pre> <p>After:</p> <pre><code>return nil, smarterr.NewError(tfresource.NewEmptyResultError(...))\n</code></pre>"},{"location":"ai-agent-guides/smarterr/#4-replace-all-direct-diagnosticsappend-calls","title":"4. Replace All Direct Diagnostics.Append Calls","text":"<p>Before:</p> <pre><code>resp.Diagnostics.Append(...)\n</code></pre> <p>After:</p> <pre><code>smerr.EnrichAppend(ctx, &amp;resp.Diagnostics, ...)\n</code></pre>"},{"location":"ai-agent-guides/smarterr/#5-framework-vs-sdkv2-how-to-identify","title":"5. Framework vs SDKv2: How to Identify","text":"<ul> <li>Framework-style:<ul> <li>Uses <code>github.com/hashicorp/terraform-plugin-framework/resource</code></li> <li>Has <code>@FrameworkResource</code> or <code>@FrameworkDataSource</code> comments</li> <li>Defines methods like <code>Schema</code>, <code>Create</code>, <code>Read</code>, <code>Update</code>, <code>Delete</code></li> </ul> </li> <li>SDKv2-style:<ul> <li>Uses <code>github.com/hashicorp/terraform-plugin-sdk/v2/helper/schema</code></li> <li>Has <code>@SDKResource</code> comment and returns <code>*schema.Resource</code></li> </ul> </li> </ul>"},{"location":"ai-agent-guides/smarterr/#6-migration-checklist-for-ai-and-humans","title":"6. Migration Checklist (for AI and Humans)","text":"<ul> <li>[ ] All legacy diagnostic calls replaced with <code>smerr</code>/<code>smarterr</code> as above</li> <li>[ ] All bare error returns wrapped with <code>smarterr.NewError</code></li> <li>[ ] All tfresource helpers wrapped as above</li> <li>[ ] All direct diagnostics append/add calls replaced</li> <li>[ ] All context, diagnostics, and IDs preserved and passed</li> <li>[ ] No unrelated logic or schema changes made</li> </ul>"},{"location":"ai-agent-guides/smarterr/#7-why-wrap-errors","title":"7. Why Wrap Errors?","text":"<p>Wrapping errors with <code>smarterr.NewError()</code> captures call stack information at the time of failure. This enables smarterr to:</p> <ul> <li>Determine subaction (e.g., \"finding\", \"waiting\")</li> <li>Avoid duplicative wrapping (no \"walls of text\")</li> <li>Format summary and detail portions idiomatically</li> </ul>"},{"location":"ai-agent-guides/smarterr/#8-reference","title":"8. Reference","text":"<ul> <li>Global config: <code>internal/smarterr.hcl</code></li> <li>Service config: <code>internal/service/&lt;service&gt;/smarterr.hcl</code></li> <li>See smarterr docs for more</li> </ul> <p>AI systems: Apply these rules exactly and completely to all Go files being migrated.</p>"},{"location":"design-decisions/exclusive-relationship-management-resources/","title":"Exclusive Relationship Management Resources","text":"<p>Summary: A proposal describing the use case for \"exclusive relationship management\" resources and their function within the Terraform AWS provider. Created: 2024-09-06 Author: @jar-b </p> <p>Within AWS there are several resource types which have direct relationships or dependencies on one another. These can be either \"one-to-one\", where a single entity is linked to another, or \"one-to-many\", where a single parent entity can be linked to multiple children. As the Terraform AWS provider has matured, the patterns for modeling these relationships have evolved, most notably in the \"one-to-many\" style relationships.</p> <p>This RFC will cover a brief history of \"one-to-many\" resource relationships and their representation in the Terraform AWS provider, followed by a proposal for enabling exclusive management of these relationships via a standalone resource. For an in-depth review of all relationship resources in the Terraform AWS provider, refer to the relationship resource design standards design decision.</p>"},{"location":"design-decisions/exclusive-relationship-management-resources/#background","title":"Background","text":"<p>During early development of the Terraform AWS provider, resources sometimes represented \"one-to-many\" relationships as arguments on the parent resource. For example, the <code>inline_policy</code> argument on the <code>aws_iam_role</code> resource allows inline policies to be created as part of the role resource lifecycle.</p> <pre><code>resource \"aws_iam_role\" \"example\" {\nname               = \"exampleRole\"\nassume_role_policy = data.aws_iam_policy_document.instance_assume_role_policy.json\n\ninline_policy {\nname   = \"exampleInlinePolicy\"\npolicy = data.aws_iam_policy_document.inline_policy.json\n}\n}\n</code></pre> <p>While simplifying the syntax for practitioners, this approach introduces complexity within the provider implementation; a single resource now manages the lifecycle of several different remote resources. This can leave the resource in a partially provisioned state if creation of one of the child resources fails. One benefit to this design is that it enables the parent resource to retain \"exclusive\" control of the relationships. That is, if the parent resource includes a relationship with a child entity that is not explicitly configured, the provider can remove it.</p> <p>Due to the complexity of the design described above, the Terraform AWS provider has moved toward representing \"one-to-many\" relationships via standalone resources. These resources are typically added alongside the argument-based method to preserve backward compatibility. Continuing with the example above, inline policies can now be managed distinctly from the assigned role via the <code>aws_iam_role_policy</code> resource.</p> <pre><code>resource \"aws_iam_role\" \"example\" {\nname               = \"exampleRole\"\nassume_role_policy = data.aws_iam_policy_document.instance_assume_role_policy.json\n}\n\nresource \"aws_iam_role_policy\" \"example\" {\nname = \"exampleInlinePolicy\"\nrole = aws_iam_role.test_role.id\n\npolicy = data.aws_iam_policy_document.inline_policy.json\n}\n</code></pre> <p>While this separates the lifecycle of distinct resource types to more closely align with the underlying AWS APIs, this approach alone does not provide a mechanism for exclusive management of all relationships to the parent. The absence of this benefit is often cited by the community as a reason not to move away from the argument-based definitions to standalone resources. The lack of parity has also prevented maintainers from formally deprecating and removing arguments on the parent resource for configuring relationships.</p>"},{"location":"design-decisions/exclusive-relationship-management-resources/#existing-resources","title":"Existing Resources","text":"<p>There is some precedent in the Terraform AWS provider for a resource which handles only exclusive relationship management in <code>aws_iam_policy_attachment</code>. However, the scope of responsibility in this resource (managing attachments to roles, users, and groups simultaneously), is broader than what this RFC proposes, and cannot be replicated widely due to the unique nature of customer managed IAM policies which can be associated with multiple distinct parent resource types.</p>"},{"location":"design-decisions/exclusive-relationship-management-resources/#proposal","title":"Proposal","text":"<p>To provide practitioners with a consistent and maintainable mechanism for exclusive management of \"one-to-many\" relationships, the Terraform AWS provider will introduce a new pattern for developing \"exclusive relationship management\" resources. Resources with this function will end in the suffix <code>_exclusive</code>, with the purpose of reconciling the relationships present in AWS against what is configured in Terraform, adding and removing relationships as necessary.</p> <p>In general, exclusive relationship management resources should have the following characteristics:</p> <ol> <li>A required argument (typically <code>TypeString</code>) storing the parent resource identifier.</li> <li>A required argument (typically <code>TypeSet</code> with <code>TypeString</code> elements) storing identifiers of all child resources. An empty set should remove all relationships.</li> <li>The ability to read the current state of relationships in AWS and add or remove them as necessary.</li> <li>The ability to \"inherit\" exclusive ownership of existing relationship definitions without destructive action. Adding this resource to a configuration should not result in a destroy/re-create relationship workflow.</li> </ol> <p>As an example, the implementation for inline IAM policies would be named <code>aws_iam_role_policies_exclusive</code>, and used as follows:</p> <pre><code>resource \"aws_iam_role\" \"example\" {\nname               = \"exampleRole\"\nassume_role_policy = data.aws_iam_policy_document.instance_assume_role_policy.json\n}\n\nresource \"aws_iam_role_policy\" \"example\" {\nname = \"exampleInlinePolicy\"\nrole = aws_iam_role.example.id\n\npolicy = data.aws_iam_policy_document.inline_policy.json\n}\n\n# This resource ensures that only `exampleInlinePolicy` is assigned\n# to this role. Any other inline policies will be removed.\nresource \"aws_iam_role_policies_exclusive\" \"example\" {\nrole_name    = aws_iam_role.example.name\npolicy_names = [aws_iam_role_policy.example.name]\n}\n</code></pre> <p>A working implementation can be found on this branch. The <code>_exclusive</code> resource will detect any additional inline policies assigned to the role during <code>plan</code> operations and remove them during <code>apply</code>. This behavior provides parity with the <code>inline_policy</code> argument on the <code>aws_iam_role</code> resource, allowing maintainers to formally deprecate this argument and suggest practitioners migrate to the standalone inline policy resource, optionally including an <code>_exclusive</code> resource when exclusive management of assignments is desired.</p>"},{"location":"design-decisions/exclusive-relationship-management-resources/#potential-resources","title":"Potential Resources","text":"<p>In addition to the IAM inline policy example used throughout this document, there are several other resources with \"one-to-many\" relationships that could benefit from exclusive management of relationships.</p> <p>Note</p> <p>Due to the popularity of the resources in this section, argument deprecations are likely to be \"soft\" deprecations where removal will not happen for several major releases, or until tooling is available to limit the amount of manual changes required to migrate to the preferred pattern. Despite this long removal window, a soft deprecation is still helpful for maintainers to reference when making best practice recommendations to the community.</p>"},{"location":"design-decisions/exclusive-relationship-management-resources/#inline-iam-policies-to-role","title":"Inline IAM policies to role","text":"<p>Manage inline IAM policies assigned to a role. Related resources:</p> <ul> <li><code>aws_iam_role</code></li> <li><code>aws_iam_role_policy</code></li> </ul> <p>Deprecate <code>aws_iam_role.inline_policy</code>.</p>"},{"location":"design-decisions/exclusive-relationship-management-resources/#inline-iam-policies-to-user","title":"Inline IAM policies to user","text":"<p>Manage inline IAM policies assigned to a user. Related resources:</p> <ul> <li><code>aws_iam_user</code></li> <li><code>aws_iam_user_policy</code></li> </ul> <p>There are no arguments to deprecate on <code>aws_iam_user</code>. This may lower the relative priority.</p>"},{"location":"design-decisions/exclusive-relationship-management-resources/#inline-iam-policies-to-group","title":"Inline IAM policies to group","text":"<p>Manage inline IAM policies assigned to a group. Related resources:</p> <ul> <li><code>aws_iam_group</code></li> <li><code>aws_iam_group_policy</code></li> </ul> <p>There are no arguments to deprecate on <code>aws_iam_group</code>. This may lower the relative priority.</p>"},{"location":"design-decisions/exclusive-relationship-management-resources/#customer-managed-iam-policies-to-role","title":"Customer managed IAM policies to role","text":"<p>Manage customer managed IAM policies attached to a role. Related resources:</p> <ul> <li><code>aws_iam_role</code></li> <li><code>aws_iam_policy</code></li> <li><code>aws_iam_role_policy_attachment</code></li> </ul> <p>Deprecate <code>aws_iam_role.managed_policy_arns</code> .</p>"},{"location":"design-decisions/exclusive-relationship-management-resources/#customer-managed-iam-policies-to-user","title":"Customer managed IAM policies to user","text":"<p>Manage customer managed IAM policies attached to a user. Related resources:</p> <ul> <li><code>aws_iam_user</code></li> <li><code>aws_iam_policy</code></li> <li><code>aws_iam_user_policy_attachment</code></li> </ul> <p>There are no arguments to deprecate on <code>aws_iam_user</code>. This may lower the relative priority.</p>"},{"location":"design-decisions/exclusive-relationship-management-resources/#customer-managed-iam-policies-to-group","title":"Customer managed IAM policies to group","text":"<p>Manage customer managed IAM policies attached to a group. Related resources:</p> <ul> <li><code>aws_iam_group</code></li> <li><code>aws_iam_policy</code></li> <li><code>aws_iam_group_policy_attachment</code></li> </ul> <p>There are no arguments to deprecate on <code>aws_iam_group</code>. This may lower the relative priority.</p>"},{"location":"design-decisions/exclusive-relationship-management-resources/#ec2-security-groups-to-network-interface","title":"EC2 security groups to network interface","text":"<p>Manage EC2 security group attachments to network interfaces. Related resources:</p> <ul> <li><code>aws_instance</code></li> <li><code>aws_network_interface</code></li> <li><code>aws_security_group</code></li> <li><code>aws_ec2_network_interface_sg_attachment</code></li> </ul> <p>Deprecate <code>aws_instance.security_groups</code>, <code>aws_instance.vpc_security_group_ids</code>, and <code>aws_network_interface.security_groups</code>.</p>"},{"location":"design-decisions/exclusive-relationship-management-resources/#vpc-security-group-rules-to-security-group","title":"VPC security group rules to security group","text":"<p>Manage ingress and egress rules assigned to a VPC security group. Related resources:</p> <ul> <li><code>aws_security_group</code></li> <li><code>aws_security_group_rule</code></li> <li><code>aws_vpc_security_group_egress_rule</code></li> <li><code>aws_vpc_security_group_ingress_rule</code></li> </ul> <p>Deprecate <code>aws_security_group.ingress</code> and <code>aws_security_group.egress</code>. Consider deprecating <code>aws_security_group_rule</code> entirely.</p>"},{"location":"design-decisions/exclusive-relationship-management-resources/#alternate-naming-conventions","title":"Alternate Naming Conventions","text":"<p>Ideally all resources providing the \"exclusive relationship management\" function should utilize the same suffix. While there are several options to describe this behavior, <code>_exclusive</code> seemed the most concise. The options considered were:</p> Suffix Example <code>_exclusive</code> (selected) <code>aws_iam_role_policies_exclusive</code> <code>_management</code> <code>aws_iam_role_policies_management</code> <code>_lock</code> <code>aws_iam_role_policies_lock</code> <code>_exclusive_lock</code> <code>aws_iam_role_policies_exclusive_lock</code> <code>_exclusive_management</code> <code>aws_iam_role_policies_exclusive_management</code>"},{"location":"design-decisions/exclusive-relationship-management-resources/#next-steps","title":"Next Steps","text":"<p>If this design decision is approved, a meta-issue will be opened to track all resources listed in the Potential Resources section, with linked issues for each individual implementation.</p>"},{"location":"design-decisions/expect-resource-action-with-disappears-tests/","title":"Use <code>plancheck.ExpectResourceAction</code> with disappears acceptance tests","text":"<p>Summary: Acceptance tests exercising out of band deletion (colloquially named \"disappears\" tests) should utilize the terraform-plugin-testing library's plancheck package to assert expected post apply actions. Created: 2025-04-11 Author: @jar-b </p>"},{"location":"design-decisions/expect-resource-action-with-disappears-tests/#background","title":"Background","text":"<p>Resources implemented in the Terraform AWS provider commonly include a \u201cdisappears\u201d acceptance test, which exercises the expected behavior of the resource following an out of band deletion (e.g. deletion outside of Terraform). The goal of these tests is to ensure that the resource\u2019s <code>Read</code> operation correctly detects when the resource cannot be found and removes it from state. The contributor guide includes the following example of a \u201cdisappears\u201d test.</p> <pre><code>func TestAccExampleThing_disappears(t *testing.T) {\n  ctx := acctest.Context(t)\n  rName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\n  resourceName := \"aws_example_thing.test\"\n\n  resource.ParallelTest(t, resource.TestCase{\n    PreCheck:                 func() { acctest.PreCheck(ctx, t) },\n    ErrorCheck:               acctest.ErrorCheck(t, names.ExampleServiceID),\n        ProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories,\n    CheckDestroy:             testAccCheckExampleThingDestroy(ctx),\n    Steps: []resource.TestStep{\n      {\n        Config: testAccExampleThingConfigName(rName),\n        Check: resource.ComposeTestCheckFunc(\n          testAccCheckExampleThingExists(ctx, resourceName),\n          acctest.CheckResourceDisappears(ctx, acctest.Provider, ResourceExampleThing(), resourceName),\n        ),\n        ExpectNonEmptyPlan: true,\n      },\n    },\n  })\n}\n</code></pre> <p>\u201cDisappears\u201d tests are distinctive in that they expect a non-empty plan because a deletion was intentionally triggered during the check phase via the <code>acctest.CheckResourceDisappears</code> helper. The impacted resource should be dropped from state during the post apply refresh, and therefore planned to be re-created. <code>ExpectNonEmptyPlan</code> ensures we don\u2019t fail to detect out of band deletions, however, it doesn\u2019t explicitly verify what changes are planned.</p> <p>In <code>v1.2.0</code>, the <code>terraform-plugin-testing</code> library introduced a <code>plancheck</code> package with an <code>ExpectResourceAction</code> built-in plan check, which asserts that a given resource will have a specific resource change type in the plan. With this check \u201cdisappears\u201d tests can explicitly verify the plan of the deleted resource.</p>"},{"location":"design-decisions/expect-resource-action-with-disappears-tests/#decision","title":"Decision","text":"<p>The contributor guide and <code>skaff</code> will be updated to include a post-apply, post-refresh plan check to verify that the \u201cdisappeared\u201d resource is planned for creation. For example,</p> <pre><code>func TestAccExampleThing_disappears(t *testing.T) {\n  ctx := acctest.Context(t)\n  rName := sdkacctest.RandomWithPrefix(acctest.ResourcePrefix)\n  resourceName := \"aws_example_thing.test\"\n\n  resource.ParallelTest(t, resource.TestCase{\n    PreCheck:                 func() { acctest.PreCheck(ctx, t) },\n    ErrorCheck:               acctest.ErrorCheck(t, names.ExampleServiceID),\n        ProtoV5ProviderFactories: acctest.ProtoV5ProviderFactories,\n    CheckDestroy:             testAccCheckExampleThingDestroy(ctx),\n    Steps: []resource.TestStep{\n      {\n        Config: testAccExampleThingConfigName(rName),\n        Check: resource.ComposeTestCheckFunc(\n          testAccCheckExampleThingExists(ctx, resourceName),\n          acctest.CheckResourceDisappears(ctx, acctest.Provider, ResourceExampleThing(), resourceName),\n        ),\n        ExpectNonEmptyPlan: true,\n        ConfigPlanChecks: resource.ConfigPlanChecks{\n          PostApplyPostRefresh: []plancheck.PlanCheck{\n            plancheck.ExpectResourceAction(resourceName, plancheck.ResourceActionCreate),\n          },\n        },\n      },\n    },\n  })\n}\n</code></pre>"},{"location":"design-decisions/expect-resource-action-with-disappears-tests/#consequencesfuture-work","title":"Consequences/Future Work","text":"<p>Given the additional <code>ConfigPlanCheck</code> has only a single variable reference (<code>resourceName</code>) which is a standard naming convention across all acceptance tests, we should explore automating the addition of this check to existing \u201cdisappears\u201d tests as well.</p>"},{"location":"design-decisions/rds-bluegreen-deployments/","title":"RDS Blue/Green Deployments","text":"<p>Summary: Discussion of which resource types can support RDS Blue/Green Deployments for updates Created: 2023-11-24 Updated: 2024-01-24</p> <p>The Terraform Provider for AWS currently supports RDS Blue/Green Deployments for the resource type <code>aws_db_instance</code>. Extending this support to other RDS resource types is a common request.</p> <p>Due to Terraform's resource model and how RDS implements Blue/Green Deployments, support for Blue/Green Deployments cannot be extended to other resource types.</p>"},{"location":"design-decisions/rds-bluegreen-deployments/#background","title":"Background","text":"<p>Terraform treats each resource as a persistent, self-contained object that can be created, modified, and deleted without interacting with other objects, other than having parameter value dependencies on other resources. In most cases, this more or less corresponds to how objects within AWS interrelate.</p> <p>RDS Blue/Green Deployments are implemented using a temporary Blue/Green Deployment orchestration object, which manages both the old and new RDS Instances, data synchronization, switchover, and deletion of the old Instances. The orchestration object is typically deleted after switchover, though it can be retained to, for example, review the operation logs for the deployment.</p>"},{"location":"design-decisions/rds-bluegreen-deployments/#implementations","title":"Implementations","text":""},{"location":"design-decisions/rds-bluegreen-deployments/#aws_db_instance","title":"<code>aws_db_instance</code>","text":"<p>Because the <code>aws_db_instance</code> resource type represents a single, self-contained object, we are able to fit within the Terraform resource model while using an RDS Blue/Green Deployment to reduce downtime for many Instance updates. For instance, changing the backing EC2 instance type or the engine version in place can cause downtimes of over 15 minutes. When the <code>blue_green_update.enabled</code> parameter is set, the AWS Provider will use a Blue/Green Deployment internally to perform the update, so that the downtime is only the length of the switchover. According to AWS documentation, \"switchover typically takes under a minute\".</p> <p>The update takes place in a single <code>terraform apply</code>, and the Blue/Green Deployment object is not exposed by the provider, since it is only an implementation detail.</p>"},{"location":"design-decisions/rds-bluegreen-deployments/#aws_db_instance-replicas","title":"<code>aws_db_instance</code> Replicas","text":"<p>Creating a replica RDS DB Instance involves one source <code>aws_db_instance</code> resource and one or more replica <code>aws_db_instance</code> resources. Because multiple resources are involved, Terraform cannot treat the source Instance and its replicas as a single unit. Therefore, Terraform cannot use a Blue/Green Deployment for updating an Instance and its replicas.</p>"},{"location":"design-decisions/rds-bluegreen-deployments/#aws_rds_cluster","title":"<code>aws_rds_cluster</code>","text":"<p>The <code>aws_rds_cluster</code> resource type is used to model two types of RDS clusters, either an Aurora cluster or a multi-AZ cluster for a \"traditional\" database engine such as MySQL or PostgreSQL.</p> <p>Blue/Green Deployments are not currently supported for multi-AZ clusters. If, at some point, Blue/Green Deployments are supported, multi-AZ clusters may be a candidate for updates using Blue/Green Deployment for reduced downtime, since the multi-AZ cluster is treated as a single object by both the provider and the AWS API. However, the multi-AZ cluster already performs a rolling update of the individual instances, so there may be no benefit to using a Blue/Green Deployment for updates.</p> <p>Aurora clusters do support Blue/Green Deployments. However, neither the AWS APIs nor the provider treats an Aurora cluster as a self-contained object: A cluster consists of a containing <code>aws_rds_cluster</code> resource and one or more <code>aws_rds_cluster_instance</code> resources. Because of this, a Blue/Green Deployment cannot be used for most updates on an Aurora cluster.</p>"},{"location":"design-decisions/rds-bluegreen-deployments/#standalone-bluegreen-deployment-resource","title":"Standalone Blue/Green Deployment Resource","text":"<p>The RDS Blue/Green Deployment object is a temporary orchestration object which reserves compute and other resources and manages connections. When it is created, it first creates replicas of any existing RDS Instances, configures synchronization, and updates the engine version, DB parameter group, and instance class if needed. Once these operations are complete, any additional updates are performed on the new (or \"Green\") instances. After these updates are complete, the Green instances can be promoted to be the live instances.</p> <p>Using a standalone Blue/Green Deployment resource would require multiple iterations of editing the Terraform configuration, applying the configuration, or importing resources. This isn't a good fit for Terraform.</p> <p>The following example shows the steps that would be needed to use a standalone Blue/Green Deployment resource with a single RDS Instance (<code>aws_db_instance</code>). Using it with an Aurora cluster would be similar, but require changes to all of the resources making up the cluster.</p> <p>We start with an RDS Instance with the name <code>example-db</code>.</p> <pre><code>resource \"aws_db_instance\" \"example\" {\nidentifier = \"example-db\"\n}\n</code></pre> <ol> <li> <p>Edit the Terraform configuration to add the <code>aws_rds_blue_green_deployment</code> resource</p> <pre><code>resource \"aws_db_instance\" \"example\" {\nidentifier = \"example-db\"\n}\n\nresource \"aws_rds_blue_green_deployment\" \"example\" {\nsource = aws_db_instance.example.arn\n}\n</code></pre> </li> <li> <p>Run <code>terraform apply</code>.   This will wait until the Blue/Green Deployment and the \"Green\" RDS Instance  <code>example-db-green</code> are created</p> </li> <li> <p>Edit the Terraform configuration to add an <code>aws_db_instance</code> resource for the RDS Instance <code>example-db-green</code> and make any desired configuration changes</p> <pre><code>resource \"aws_db_instance\" \"example\" {\nidentifier = \"example-db\"\n}\n\nresource \"aws_rds_blue_green_deployment\" \"example\" {\nsource = aws_db_instance.example.arn\n}\n\nresource \"aws_db_instance\" \"example_updated\" {\nidentifier = \"example-db-green\"\n}\n</code></pre> </li> <li> <p>Run <code>terraform</code> to import the RDS Instance <code>example-db-green</code></p> </li> <li>Run <code>terraform apply</code> to update the RDS Instance <code>example-db-green</code></li> <li> <p>Edit the Terraform configuration to trigger the switchover on the <code>aws_rds_blue_green_deployment</code> resource</p> <pre><code>resource \"aws_db_instance\" \"example\" {\nidentifier = \"example-db\"\n}\n\nresource \"aws_rds_blue_green_deployment\" \"example\" {\nsource      = aws_db_instance.example.arn\nswitch_over = true\n}\n\nresource \"aws_db_instance\" \"example_updated\" {\nidentifier = \"example-db-green\"\n}\n</code></pre> </li> <li> <p>Run <code>terraform apply</code> to perform the switchover.   The updated RDS Instance will be renamed from <code>example-db-green</code> to <code>example-db</code> and the original RDS Instance will be renamed from <code>example-db</code> to <code>example-db-old</code>.   This means that the resource <code>aws_db_instance.example</code> will now be pointing to the updated RDS Instance, the resource <code>aws_db_instance.example_updated</code> will be pointing at a non-existent RDS Instance, and the original RDS Instance is not known to Terraform.</p> </li> <li> <p>Edit the Terraform configuration to remove the resource <code>aws_db_instance.example_updated</code> and add an <code>aws_db_instance</code> resource for the RDS Instance <code>example-db-old</code></p> <pre><code>resource \"aws_db_instance\" \"example\" {\nidentifier = \"example-db\"\n}\n\nresource \"aws_rds_blue_green_deployment\" \"example\" {\nsource      = aws_db_instance.example.arn\nswitch_over = true\n}\n\nresource \"aws_db_instance\" \"example_to_delete\" {\nidentifier = \"example-db-old\"\n}\n</code></pre> </li> <li> <p>Run <code>terraform</code> to import the RDS Instance <code>example-db-old</code></p> </li> <li> <p>Edit the Terraform configuration to remove the resource <code>aws_db_instance.example_updated</code> and the <code>aws_rds_blue_green_deployment</code> resource</p> <pre><code>resource \"aws_db_instance\" \"example\" {\nidentifier = \"example-db\"\n}\n</code></pre> </li> <li> <p>Run <code>terraform apply</code> to delete the Blue/Green Deployment and the original RDS Instance</p> </li> </ol>"},{"location":"design-decisions/relationship-resource-design-standards/","title":"Relationship Resource Design Standards","text":"<p>Summary: Align on design standards for relationship management resources in the Terraform AWS Provider. Created: 2022-07-11  </p> <p>The goal of this document is to assess the design of existing \"relationship\" resources in the Terraform AWS Provider and determine if a consistent set of rules can be defined for implementing them. For the purpose of this document, a \"relationship\" resource is defined as a resource which manages either a direct relationship between two standalone resources (\"one-to-one\", ie. <code>aws_ssoadmin_permission_boundary_attachment</code>), or a variable number of child relationships to a parent resource (\"one-to-many\", ie. <code>aws_iam_role_policy_attachment</code>). Resources and AWS APIs with this function will often contain suffixes like \"attachment\", \"assignment\", \"registration\", or \"rule\".</p> <p>A documented standard for implementing relationship-styled resources will inform how new resources are written, and provide guidelines to refer back to when the community requests features which may not align with internal best practices.</p>"},{"location":"design-decisions/relationship-resource-design-standards/#background","title":"Background","text":"<p>The first form of relationship resources (\"one-to-one\") typically have a straightforward, singular API design and provider implementation given only a single relationship exists. The second form of resources (\"one-to-many\") often has to balance two provider design principles:</p> <ul> <li>Resources should represent a single API object</li> <li>Resource and attribute schema should closely match the underlying API</li> </ul> <p>In these cases, the smallest possible piece of infrastructure may be a single parent-child relationship, while the AWS APIs may accept and return lists of parent-child relationships. The first principle would favor a resource representing a single relationship, while the second principle suggests a resource should manage a variable number of relationships. Additionally, practitioners coming from the AWS CLI or SDK might also have expectations about how resource schemas should be shaped compared to CLI flags or SDK inputs.</p> <p>An analysis of existing resources can inform which of these principles maintainers have given precedence to up to this point. The table below documents existing relationship resources in the AWS provider. This table should not be considered exhaustive1, but covers a large majority of the resources implementing the patterns discussed above.</p> Resource Name Form AWS API Terraform aws_alb_target_group_attachment One-to-many Plural Singular aws_autoscaling_attachment (ELB) One-to-many Plural Singular aws_autoscaling_attachment (Target Group ARN) One-to-many Plural Singular aws_autoscaling_traffic_source_attachment One-to-many Plural Singular aws_cognito_identity_pool_roles_attachment2 One-to-many Plural Plural aws_ec2_transit_gateway_peering_attachment One-to-one Singular Singular aws_ec2_transit_gateway_vpc_attachment One-to-one Singular Singular aws_elb_attachment One-to-many Plural Singular aws_iam_group_policy_attachment One-to-many Singular Singular aws_iam_policy_attachment3 One-to-many Singular Plural aws_iam_role_policy_attachment One-to-many Singular Singular aws_iam_user_policy_attachment One-to-many Singular Singular aws_internet_gateway_attachment One-to-one Singular Singular aws_iot_policy_attachment One-to-many Singular Singular aws_iot_thing_principal_attachment One-to-many Singular Singular aws_lb_target_group_attachment One-to-many Plural Singular aws_lightsail_disk_attachment One-to-many Singular Singular aws_lightsail_lb_attachment One-to-many Plural Singular aws_lightsail_lb_certificate_attachment One-to-one Singular Singular aws_lightsail_static_ip_attachment One-to-one Singular Singular aws_network_interface_attachment One-to-many Singular Singular aws_network_interface_sg_attachment One-to-many Plural Singular aws_networkmanager_connect_attachment One-to-one Singular Singular aws_networkmanager_core_network_policy_attachment One-to-one Singular Singular aws_networkmanager_site_to_site_vpn_attachment One-to-one Singular Singular aws_networkmanager_transit_gateway_registration One-to-one Singular Singular aws_networkmanager_transit_gateway_route_table_attachment One-to-one Singular Singular aws_networkmanager_vpc_attachment One-to-one Singular Singular aws_organizations_policy_attachment One-to-many Singular Singular aws_quicksight_iam_policy_assignment One-to-many Plural Plural aws_security_group (Egress)4 One-to-many Plural Plural aws_security_group (Ingress)4 One-to-many Plural Plural aws_security_group_rule (Egress) One-to-many Plural Singular aws_security_group_rule (Ingress) One-to-many Plural Singular aws_sesv2_dedicated_ip_assignment One-to-one Singular Singular aws_ssoadmin_account_assignment One-to-one Singular Singular aws_ssoadmin_customer_managed_policy_attachment One-to-many Singular Singular aws_ssoadmin_managed_policy_attachment One-to-many Singular Singular aws_ssoadmin_permissions_boundary_attachment One-to-one Singular Singular aws_volume_attachment One-to-many Singular Singular aws_vpc_security_group_egress_rule One-to-many Plural Singular aws_vpc_security_group_ingress_rule One-to-many Plural Singular aws_vpclattice_target_group_attachment One-to-many Plural Singular aws_vpn_gateway_attachment One-to-one Singular Singular <p>Of the 44 resources documented above, 29 are of the \"one-to-many\" form and 17 have \"plural\" AWS APIs (ie. accept a list of child resources to be attached to a single parent). Of these 17, 13 resources (76%) use a \"singular\" Terraform implementation, where a list with one item is sent to the Create/Read/Update API, rather than allowing a single resource to manage multiple relationships. Of the remaining 4 with \"plural\" Terraform implementations, 2 do so to exclusively manage child relationships (<code>aws_security_group</code> Ingress/Egress variants), and one requires a \"plural\" implementation simply because of API limitations.</p> <p>These metrics indicate a strong historical preference for representing a single API object over aligning the schema to the underlying AWS API. The primary exceptions to this are when exclusive management of all child resources is desired, such as security group ingress/egress rules, or IAM policy attachments.</p>"},{"location":"design-decisions/relationship-resource-design-standards/#proposal","title":"Proposal","text":"<p>The best practice for net-new \"one-to-many\" relationship resources should be to implement singular versions. Feature requests related to changing the singular nature of an existing relationship resource should be avoided unless necessary for the underlying API to function properly.</p> <p>Variation from this pattern should only be done when:</p> <ol> <li>There is a valid use case for a single resource to retain exclusive management of all parent/child relationships.</li> <li>Manipulating the underlying AWS APIs to work with singular relationships is not possible or introduces unnecessary complexity.</li> </ol>"},{"location":"design-decisions/relationship-resource-design-standards/#one-to-many-design-example","title":"\"One-to-many\" Design Example","text":"<p>Given a fictional \"plural\" API <code>AttachChildren</code> with a request body like:</p> <pre><code>{\n  \"ParentId\": \"string\",\n  \"Children\": [\n    {\n      \"ChildId: \"string\"\n    }\n  ]\n}\n</code></pre> <p>The corresponding Terraform resource would only represent a single parent/child relationship with a configuration like:</p> <pre><code>resource \"aws_parent\" \"example\" {\nname = \"foo\"\n}\n\nresource \"aws_child\" \"example\" {\nname = \"bar\"\n}\n\nresource \"aws_child_attachment\" \"example\" {\nparent_id = aws_parent.example.id\nchild_id  = aws_child.example.id\n}\n</code></pre>"},{"location":"design-decisions/relationship-resource-design-standards/#references","title":"References","text":"<ul> <li>The following feature request and PR initiated the discussion for this analysis:<ul> <li>https://github.com/hashicorp/terraform-provider-aws/issues/9901</li> <li>https://github.com/hashicorp/terraform-provider-aws/pull/32380</li> </ul> </li> </ul> <ol> <li> <p>Due to the volume of resources with \"rule\" in the name (~70), only the prominent security group rule resources were included in the analysis above. While \"rule\" resources often follow the same relationship-style design, the ~40 examples above provided enough initial data to inform design standards.\u00a0\u21a9</p> </li> <li> <p>The structure of this API precludes it from being implemented in a singular fashion.\u00a0\u21a9</p> </li> <li> <p>Creates exclusive attachments.\u00a0\u21a9</p> </li> <li> <p>Creates exclusive rules.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"design-decisions/secretsmanager-secret-target-attachment/","title":"SecretsManager Secret Target Attachment","text":"<p>Summary: Assess the feasibility of replicating the <code>AWS::SecretsManager::SecretTargetAttachment</code> CloudFormation function with Terraform. Created: 2023-10-25</p> <p>The AWS Terraform provider has a prioritized issue requesting a Terraform AWS provider implementation of the CloudFormation <code>AWS::SecretsManager::SecretTargetAttachment</code> function. This document will assess the feasibility of replicating this functionality in the AWS Terraform provider and document the alternative options available using existing SecretsManager resources.</p>"},{"location":"design-decisions/secretsmanager-secret-target-attachment/#background","title":"Background","text":"<p>The <code>AWS::SecretsManager::SecretTargetAttachment</code> function is a convenience helper to supplement an existing SecretsManager secret with database connection information from services like Amazon RDS or Amazon Redshift. This function has no API equivalent (see the SecretsManager API documentation), and appears to operate as an orchestration job working across AWS services. In the absence of public APIs, the AWS provider cannot easily implement proper Terraform lifecycle handling on top of this workflow. However, the existing SecretsManager resources and configuration options on RDS database resources provide options for practitioners to replicate most of this functionality.</p>"},{"location":"design-decisions/secretsmanager-secret-target-attachment/#manual-secret-with-supplemental-connection-information","title":"Manual Secret with Supplemental Connection Information","text":"<p>With this approach the database (RDS Postgres in this example) is initially created with a random password. An empty secret is created at the same time. Once database creation is complete, the username, password, and database connection information are all written to a new version of the existing secret.</p> <pre><code>terraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~&gt; 5.0\"\n    }\n    random = {\n      source  = \"hashicorp/random\"\n      version = \"~&gt; 3.0\"\n    }\n  }\n}\n\nprovider \"aws\" {}\nprovider \"random\" {}\n\nlocals {\n  username = \"foo\"\n}\n\nresource \"random_password\" \"password\" {\n  length  = 20\n  special = false\n}\n\ndata \"aws_rds_orderable_db_instance\" \"test\" {\n  engine         = \"postgres\"\n  engine_version = \"15.3\"\n\n  preferred_instance_classes = [\"db.t3.micro\", \"db.t3.small\"]\n}\n\nresource \"aws_db_instance\" \"test\" {\n  allocated_storage    = 20\n  db_name              = \"testdb\"\n  engine               = data.aws_rds_orderable_db_instance.test.engine\n  engine_version       = data.aws_rds_orderable_db_instance.test.engine_version\n  instance_class       = data.aws_rds_orderable_db_instance.test.instance_class\n  username             = local.username\n  password             = random_password.password.result\n  parameter_group_name = \"default.postgres15\"\n  skip_final_snapshot  = true\n}\n\nresource \"aws_secretsmanager_secret\" \"test\" {\n  name_prefix = \"jb-test\"\n}\n\nresource \"aws_secretsmanager_secret_version\" \"test\" {\n  secret_id = aws_secretsmanager_secret.test.id\n  secret_string = jsonencode({\n    engine   = aws_db_instance.test.engine\n    host     = aws_db_instance.test.address\n    username = local.username\n    password = random_password.password.result\n    dbname   = aws_db_instance.test.db_name\n    port     = aws_db_instance.test.port\n  })\n}\n</code></pre> <p>Manually created secrets require maintaining the lambda function executing the rotation. Specifically, the RotateSecret API requires the <code>LambdaFunctionArn</code> argument to be provided when setting a rotation for a manually created secret (it is technically optional, but can only be omitted for \u201cmanaged\u201d secrets created by AWS). AWS provides Lambda function templates for the most common secret rotation use cases, and the AWS Serverless Application Repository contains many pre-built secret rotation functions. Once a rotation Lambda function is deployed, rotation can be managed with the <code>aws_secretsmanager_secret_rotation</code> resource.</p> <pre><code>resource \"aws_secretsmanager_secret_rotation\" \"test\" {\n  secret_id = aws_secretsmanager_secret.test.id\n\n  # Function templates available in the SecretsManager documentation,\n  # and pre-built functions available in the Serverless Application\n  # Repository.\n  rotation_lambda_arn = aws_lambda_function.secret_rotation.arn\n\n  rotation_rules {\n    automatically_after_days = 30\n  }\n}\n</code></pre>"},{"location":"design-decisions/secretsmanager-secret-target-attachment/#managed-secret","title":"Managed Secret","text":"<p>With this approach the database (RDS Postgres in this example) is initially created with the <code>manage_master_user_password</code> argument set to <code>true</code> (no password required). RDS will create a new \"managed\" secret to store credentials as part of database creation.</p> <pre><code>terraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~&gt; 5.0\"\n    }\n  }\n}\n\nprovider \"aws\" {}\n\nlocals {\n  username = \"foo\"\n}\n\ndata \"aws_rds_orderable_db_instance\" \"test\" {\n  engine         = \"postgres\"\n  engine_version = \"15.3\"\n\n  preferred_instance_classes = [\"db.t3.micro\", \"db.t3.small\"]\n}\n\nresource \"aws_db_instance\" \"test\" {\n  allocated_storage    = 20\n  db_name              = \"testdb\"\n  engine               = data.aws_rds_orderable_db_instance.test.engine\n  engine_version       = data.aws_rds_orderable_db_instance.test.engine_version\n  instance_class       = data.aws_rds_orderable_db_instance.test.instance_class\n  username             = local.username\n  parameter_group_name = \"default.postgres15\"\n  skip_final_snapshot  = true\n\n  manage_master_user_password = true\n}\n\n# Optionally fetch the secret data if attributes need to be used as inputs\n# elsewhere.\ndata \"aws_secretsmanager_secret\" \"test\" {\n  arn = aws_db_instance.test.master_user_secret[0].secret_arn\n}\n</code></pre> <p>If information about the managed secret is required as an input to other resources, the <code>aws_secretsmanager_secret</code> data source can be used. Because this secret is managed by Amazon RDS, the secret value cannot be modified to include supplemental connection information. This may be a limitation the <code>SecretTargetAttachment</code> CloudFormation is able to work around via some internal AWS process, but there is currently no mechanism to implement this with publicly documented APIs.</p> <p>As an alternative, supplemental connection information could be applied as <code>tags</code> if it\u2019s absolutely necessary for the information to exist on the secret itself.</p> <pre><code># Optionally import the managed secret and modify attributes via Terraform\n# (secret value cannot be modified).\nimport {\n  to = aws_secretsmanager_secret.test\n  id = aws_db_instance.test.master_user_secret[0].secret_arn\n}\n\nresource \"aws_secretsmanager_secret\" \"test\" {\n  # customize tags, description, etc.\n}\n</code></pre>"},{"location":"design-decisions/secretsmanager-secret-target-attachment/#proposal","title":"Proposal","text":"<p>Given the currently available AWS APIs, there isn\u2019t a path to implement the workflow from the <code>SecretTargetAttachment</code> CloudFormation function in the AWS Terraform Provider. Existing resources provide options to either fully customize a manual secret with both database credentials and connection information, or to fully offload secret management to AWS. These options cover the core use case of storing and rotating database secrets via AWS SecretsManager.</p> <p>With existing options already available in the provider, and no clear path forward the proposal is to close this issue.</p>"},{"location":"design-decisions/secretsmanager-secret-target-attachment/#consequencesfuture-work","title":"Consequences/Future Work","text":"<p>No work will be done to implement the requested functionality from the original issue. However, the investigation phase did uncover other potential enhancements to improve managed credential workflows in the AWS provider.</p> <ul> <li> <p>The previous implementation of the <code>aws_secretsmanager_secret_rotation</code> resource did not allow for managed secret rotations to be modified (<code>rotation_lambda_arn</code> is required, but managed secrets omit this value on update). An enhancement issue was opened to support this use case, and the implementation was completed with #34180.</p> </li> <li> <p>The <code>aws_redshift_cluster</code> resource did not implement support for managing master passwords. An enhancement issue was opened to support this use case, and the implementation was completed with #34182.</p> </li> </ul>"},{"location":"design-decisions/secretsmanager-secret-target-attachment/#references","title":"References","text":"<p>AWS Documentation</p> <ul> <li>SecretsManager API Reference</li> <li>RDS Managed Secrets Guide</li> <li><code>AWS::SecretsManager::SecretTargetAttachment</code></li> </ul> <p>Terraform Resources</p> <ul> <li><code>aws_secretsmanager_secret</code></li> <li><code>aws_secretsmanager_secret_version</code></li> <li><code>aws_secretsmanager_secret_rotation</code></li> <li><code>aws_db_instance</code></li> </ul>"},{"location":"design-decisions/standardize-use-of-the-id-attribute/","title":"Standardize Use of the <code>id</code> Attribute","text":"<p>Summary: Define a standard for use of the <code>id</code> attribute given improvements to provider development and testing libraries have removed its requirement. Created: 2024-05-24 Author: @jar-b </p> <p>Historically all resources in the Terraform AWS provider have included a read-only <code>id</code> attribute, as Terraform Plugin SDK V2 and its associated acceptance testing library requires it. In most cases, this attribute corresponds to a unique identifier generated by AWS during resource creation. However, for some resources the identifier is a value provided by the user and the resulting <code>id</code> attribute inherently duplicates the value of some other required argument.</p> <p>With the general availability of Terraform Plugin Framework, the separation of the provider testing functionality into its own standalone library (<code>terraform-plugin-testing</code>), and some corresponding enhancements made to this library, resources are no longer required to have an <code>id</code> attribute. This leaves an opportunity for provider developers to define a standard for when to use an <code>id</code> attribute. Broadly, the two options are:</p> <ul> <li>Continue to require an <code>id</code> attribute in all resources for consistency.  </li> <li>Omit the <code>id</code> attribute when it is redundant with some other required argument(s).</li> </ul> <p>This RFC will propose a standard for the Terraform AWS provider. A secondary goal is to gain feedback from other HashiCorp-owned provider teams to determine if a common standard can be shared across the provider ecosystem.</p>"},{"location":"design-decisions/standardize-use-of-the-id-attribute/#background","title":"Background","text":"<p>Prior to general availability of the Terraform Plugin Framework, all providers were built with the Terraform Plugin SDK. As the Terraform Plugin SDK was developed with pre-v1.0 versions of Terraform, an internal core implementation detail requiring the presence of an <code>id</code> attribute was incorporated into the library design. This requirement was accordingly passed along to provider developers in the form of an implicit requirement that all resources have an <code>id</code> attribute.</p> <p>This convention is most noticeable in the method for setting <code>id</code> attributes (<code>d.SetId(\u201cvalue\u201d)</code>, versus <code>d.Set(\u201cfield\u201d, \u201cvalue\u201d)</code>), the special syntax for removing objects from state during a read operation when deleted outside of Terraform (<code>d.SetId(\u201c\u201d)</code>), and the acceptance testing helpers for <code>import</code>, which rely on a populated <code>id</code> attribute to complete the import command and verify the results.</p> <p>The corresponding method for removing Terraform Plugin Framework-based resources from state during read (<code>State.RemoveResource</code>) includes no mention of an ID. In version <code>1.5.0</code> of <code>terraform-plugin-testing</code>, additional fields were added to the <code>TestStep</code> struct to enable testing imports for resources that do not implement an <code>id</code> attribute. With these advancements, providers using Terraform Plugin Framework, either exclusively or via a muxed provider configuration, and <code>terraform-plugin-testing</code> in place of the legacy testing packages embedded in Terraform Plugin SDK V2 are no longer bound to the requirement of including an <code>id</code> attribute in every resource.</p> <p>From a Terraform core perspective, releases after 0.12 no longer treat <code>id</code> as a special attribute. Terraform 0.11 and earlier implicitly required it. Due to the linkage between Terraform 0.12 and protocol version 5, and the fact that supported provider SDKs today only speak protocol version 5 (and 6), the primary Hashicorp-owned providers, like <code>hashicorp/aws</code>, inherently only support Terraform 0.12 and later. Therefore it is safe to remove the <code>id</code> attribute since there is no concern for a Terraform version that requires it.</p>"},{"location":"design-decisions/standardize-use-of-the-id-attribute/#id-attributes-in-the-aws-provider","title":"<code>id</code> Attributes in the AWS Provider","text":"<p>In the AWS provider, the <code>id</code> attribute most commonly corresponds to an identifier generated by AWS during creation of the resource. The Create API typically returns this value as a field named <code>Id</code> or, in some cases, <code>Id</code> with a prefixed with a resource name (eg. <code>InstanceId</code>). When a resource can be referenced by a unique, remotely generated identifier, storing this value in a computed <code>id</code> is a straightforward choice and the meaning of the attribute is easily understood by the practitioner.</p> <p>However, there are cases where AWS does not generate a unique identifier during resource creation, and instead one or more of the arguments supplied to the Create API functions as the identifier. For single value identifiers this is commonly a field like <code>name</code>. For \u201crelationship\u201d resources (ie. resources creating a link between two other resources, such as an IAM policy attachment), the unique identifier may be a \u201cmulti-part key\u201d made up of identifiers from both resources being linked. Historically, multi-part keys have not used a consistent delimiter, further complicating the user experience for practitioners.</p> <p>In these situations, the AWS provider copied the identifying value (or a delimited string of values for multi-part keys) to the <code>id</code> attribute. This approach leaves ambiguity for practitioners regarding which attribute should be used if the exported value is referenced somewhere else in the configuration.</p>"},{"location":"design-decisions/standardize-use-of-the-id-attribute/#proposal","title":"Proposal","text":"<p>Going forward, all net-new resources for which an <code>id</code> attribute would be redundant with an existing argument(s) will omit this attribute. In all other cases the <code>id</code> attribute should continue to be used as it has been historically. When an <code>id</code> is omitted and the unique identifier is a combination of arguments, these should always be delimited with a comma (<code>,</code>), and rely on the internal <code>ExpandResourceID</code> function to handle splitting values within the import method.</p> <p>The added clarity of removing redundant attribute values will directly benefit practitioners, especially for resources with complex multi-part keys.</p> <p>The trade-offs being made with this decision are:</p> <ul> <li>A departure from a historical precedent where all resources include an <code>id</code> attribute, which some practitioners may have come to expect and rely on.</li> <li>A minor UI difference in how resources without an ID are rendered during <code>plan</code> and <code>apply</code> operations. Specifically, when no <code>id</code> attribute is present the logged line for this resource will omit this extra piece of information.</li> </ul> <p>Team discussion around this topic consistently landed on a consensus that the clarity gained by having only a single attribute containing the identifier value outweighs the departure from historical conventions.</p> <p>It should also be noted that this standard is enabled by an existing policy requiring all net-new resources be implemented with Terraform Plugin Framework, and that the AWS provider has already migrated to the standalone <code>terraform-plugin-testing</code> library.</p>"},{"location":"design-decisions/standardize-use-of-the-id-attribute/#import-methods-and-testing","title":"Import Methods and Testing","text":"<p>For resources omitting an <code>id</code> argument, minor changes are required to customize the import method and acceptance test the import operation. For testing specifically, the import verification <code>TestStep</code> will now require the <code>ImportStateVerifyIdentifierAttribute</code> and one of <code>ImportStateID</code> or <code>ImportStateIdFunc</code> be configured. Examples for single non-id identifier and a multi-part key are included below.</p>"},{"location":"design-decisions/standardize-use-of-the-id-attribute/#single-value","title":"Single Value","text":"<p><code>ImportState</code> method:</p> <pre><code>func (r *resourceEndpointPrivateDNS) ImportState(ctx context.Context, req resource.ImportStateRequest, resp *resource.ImportStateResponse) {\nresource.ImportStatePassthroughID(ctx, path.Root(\"vpc_endpoint_id\"), req, resp)\n}\n</code></pre> <p><code>TestStep</code>:</p> <pre><code>{\nResourceName:                         resourceName,\nImportState:                          true,\nImportStateIdFunc:                    testAccVPCEndpointPrivateDNSImportStateIdFunc(resourceName),\nImportStateVerify:                    true,\nImportStateVerifyIdentifierAttribute: \"vpc_endpoint_id\",\n},\n</code></pre> <p><code>ImportStateIdFunc</code>:</p> <pre><code>func testAccVPCEndpointPrivateDNSImportStateIdFunc(resourceName string) resource.ImportStateIdFunc {\nreturn func(s *terraform.State) (string, error) {\nrs, ok := s.RootModule().Resources[resourceName]\nif !ok {\nreturn \"\", fmt.Errorf(\"Not found: %s\", resourceName)\n}\n\nreturn rs.Primary.Attributes[\"vpc_endpoint_id\"], nil\n}\n}\n</code></pre>"},{"location":"design-decisions/standardize-use-of-the-id-attribute/#multi-part-value","title":"Multi-part Value","text":"<p><code>ImportState</code> Method:</p> <pre><code>func (r *resourceRuntimeManagementConfig) ImportState(ctx context.Context, req resource.ImportStateRequest, resp *resource.ImportStateResponse) {\nparts, err := intflex.ExpandResourceId(req.ID, runtimeManagementConfigIDParts, true)\nif err != nil {\nresp.Diagnostics.AddError(\n\"Unexpected Import Identifier\",\nfmt.Sprintf(\"Expected import identifier with format: function_name,qualifier. Got: %q\", req.ID),\n)\nreturn\n}\n\nresp.Diagnostics.Append(resp.State.SetAttribute(ctx, path.Root(\"function_name\"), parts[0])...)\nresp.Diagnostics.Append(resp.State.SetAttribute(ctx, path.Root(\"qualifier\"), parts[1])...)\n}\n</code></pre> <p><code>TestStep</code>:</p> <pre><code>{\nResourceName:                         resourceName,\nImportState:                          true,\nImportStateIdFunc:                    testAccRuntimeManagementConfigImportStateIdFunc(resourceName),\nImportStateVerify:                    true,\nImportStateVerifyIdentifierAttribute: \"function_name\",\n},\n</code></pre> <p><code>ImportStateIdFunc</code>:</p> <pre><code>func testAccRuntimeManagementConfigImportStateIdFunc(resourceName string) resource.ImportStateIdFunc {\nreturn func(s *terraform.State) (string, error) {\nrs, ok := s.RootModule().Resources[resourceName]\nif !ok {\nreturn \"\", fmt.Errorf(\"Not found: %s\", resourceName)\n}\n\nreturn fmt.Sprintf(\"%s,%s\", rs.Primary.Attributes[\"function_name\"], rs.Primary.Attributes[\"qualifier\"]), nil\n}\n}\n</code></pre>"},{"location":"design-decisions/standardize-use-of-the-id-attribute/#abandoned-ideas","title":"Abandoned Ideas","text":"<p>The alternative to omitting <code>id</code> attributes when possible is to continue requiring all resources to include an <code>id</code> attribute. While this approach provides consistency with the historical design of the provider, the drawbacks of redundant attribute values and complex (and often inconsistently delimited) multi-part identifiers remain. Consensus among the team has been that the benefits of clarity outweigh the departure from precedent in this case.</p>"},{"location":"design-decisions/standardize-use-of-the-id-attribute/#references","title":"References","text":"<p>Terraform Plugin Framework</p> <ul> <li>https://developer.hashicorp.com/terraform/plugin/framework/acctests#no-id-found-in-attributes</li> </ul> <p>Terraform Plugin Testing</p> <ul> <li>https://github.com/hashicorp/terraform-plugin-testing/issues/84 </li> <li>https://github.com/hashicorp/terraform-plugin-testing/pull/164</li> </ul> <p>Terraform AWS Provider</p> <ul> <li>Prototype resources implemented without an <code>id</code> attribute  <ul> <li><code>aws_vpc_endpoint_private_dns</code> <ul> <li>The identifier is a single value, <code>vpc_endpoint_id</code> </li> <li>PR </li> <li>Registry documentation </li> </ul> </li> <li><code>aws_lambda_runtime_management_config</code> <ul> <li>The identifier is a multi-part key, made up of the required <code>function_name</code> and optional <code>qualifier</code> arguments  </li> <li>PR </li> <li>Registry documentation</li> </ul> </li> </ul> </li> </ul>"}]}