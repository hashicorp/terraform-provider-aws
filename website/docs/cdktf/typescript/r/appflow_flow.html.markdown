---
subcategory: "AppFlow"
layout: "aws"
page_title: "AWS: aws_appflow_flow"
description: |-
  Provides an AppFlow Flow resource.
---


<!-- Please do not edit this file, it is generated. -->
# Resource: aws_appflow_flow

Provides an AppFlow flow resource.

## Example Usage

```typescript
// DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
import { Construct } from "constructs";
import { Token, TerraformStack } from "cdktf";
/*
 * Provider bindings are generated by running `cdktf get`.
 * See https://cdk.tf/provider-generation for more details.
 */
import { AppflowFlow } from "./.gen/providers/aws/appflow-flow";
import { DataAwsIamPolicyDocument } from "./.gen/providers/aws/data-aws-iam-policy-document";
import { S3Bucket } from "./.gen/providers/aws/s3-bucket";
import { S3BucketPolicy } from "./.gen/providers/aws/s3-bucket-policy";
import { S3Object } from "./.gen/providers/aws/s3-object";
class MyConvertedCode extends TerraformStack {
  constructor(scope: Construct, name: string) {
    super(scope, name);
    const exampleDestination = new S3Bucket(this, "example_destination", {
      bucket: "example-destination",
    });
    const exampleSource = new S3Bucket(this, "example_source", {
      bucket: "example-source",
    });
    new S3Object(this, "example", {
      bucket: exampleSource.id,
      key: "example_source.csv",
      source: "example_source.csv",
    });
    const dataAwsIamPolicyDocumentExampleDestination =
      new DataAwsIamPolicyDocument(this, "example_destination_3", {
        statement: [
          {
            actions: [
              "s3:PutObject",
              "s3:AbortMultipartUpload",
              "s3:ListMultipartUploadParts",
              "s3:ListBucketMultipartUploads",
              "s3:GetBucketAcl",
              "s3:PutObjectAcl",
            ],
            effect: "Allow",
            principals: [
              {
                identifiers: ["appflow.amazonaws.com"],
                type: "Service",
              },
            ],
            resources: [
              "arn:aws:s3:::example-destination",
              "arn:aws:s3:::example-destination/*",
            ],
            sid: "AllowAppFlowDestinationActions",
          },
        ],
      });
    /*This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.*/
    dataAwsIamPolicyDocumentExampleDestination.overrideLogicalId(
      "example_destination"
    );
    const dataAwsIamPolicyDocumentExampleSource = new DataAwsIamPolicyDocument(
      this,
      "example_source_4",
      {
        statement: [
          {
            actions: ["s3:ListBucket", "s3:GetObject"],
            effect: "Allow",
            principals: [
              {
                identifiers: ["appflow.amazonaws.com"],
                type: "Service",
              },
            ],
            resources: [
              "arn:aws:s3:::example-source",
              "arn:aws:s3:::example-source/*",
            ],
            sid: "AllowAppFlowSourceActions",
          },
        ],
      }
    );
    /*This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.*/
    dataAwsIamPolicyDocumentExampleSource.overrideLogicalId("example_source");
    const awsS3BucketPolicyExampleDestination = new S3BucketPolicy(
      this,
      "example_destination_5",
      {
        bucket: exampleDestination.id,
        policy: Token.asString(dataAwsIamPolicyDocumentExampleDestination.json),
      }
    );
    /*This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.*/
    awsS3BucketPolicyExampleDestination.overrideLogicalId(
      "example_destination"
    );
    const awsS3BucketPolicyExampleSource = new S3BucketPolicy(
      this,
      "example_source_6",
      {
        bucket: exampleSource.id,
        policy: Token.asString(dataAwsIamPolicyDocumentExampleSource.json),
      }
    );
    /*This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.*/
    awsS3BucketPolicyExampleSource.overrideLogicalId("example_source");
    const awsAppflowFlowExample = new AppflowFlow(this, "example_7", {
      destinationFlowConfig: [
        {
          connectorType: "S3",
          destinationConnectorProperties: {
            s3: {
              bucketName: Token.asString(
                awsS3BucketPolicyExampleDestination.bucket
              ),
              s3OutputFormatConfig: {
                prefixConfig: {
                  prefixType: "PATH",
                },
              },
            },
          },
        },
      ],
      name: "example",
      sourceFlowConfig: {
        connectorType: "S3",
        sourceConnectorProperties: {
          s3: {
            bucketName: Token.asString(awsS3BucketPolicyExampleSource.bucket),
            bucketPrefix: "example",
          },
        },
      },
      task: [
        {
          connectorOperator: [
            {
              s3: "NO_OP",
            },
          ],
          destinationField: "exampleField",
          sourceFields: ["exampleField"],
          taskType: "Map",
        },
      ],
      triggerConfig: {
        triggerType: "OnDemand",
      },
    });
    /*This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.*/
    awsAppflowFlowExample.overrideLogicalId("example");
  }
}

```

## Argument Reference

This resource supports the following arguments:

* `name` - (Required) Name of the flow.
* `destinationFlowConfig` - (Required) A [Destination Flow Config](#destination-flow-config) that controls how Amazon AppFlow places data in the destination connector.
* `sourceFlowConfig` - (Required) The [Source Flow Config](#source-flow-config) that controls how Amazon AppFlow retrieves data from the source connector.
* `task` - (Required) A [Task](#task) that Amazon AppFlow performs while transferring the data in the flow run.
* `triggerConfig` - (Required) A [Trigger](#trigger-config) that determine how and when the flow runs.
* `description` - (Optional) Description of the flow you want to create.
* `kmsArn` - (Optional) ARN (Amazon Resource Name) of the Key Management Service (KMS) key you provide for encryption. This is required if you do not want to use the Amazon AppFlow-managed KMS key. If you don't provide anything here, Amazon AppFlow uses the Amazon AppFlow-managed KMS key.
* `tags` - (Optional) Key-value mapping of resource tags. If configured with a provider [`defaultTags` configuration block](https://registry.terraform.io/providers/hashicorp/aws/latest/docs#default_tags-configuration-block) present, tags with matching keys will overwrite those defined at the provider-level.

### Destination Flow Config

* `connectorType` - (Required) Type of connector, such as Salesforce, Amplitude, and so on. Valid values are `Salesforce`, `Singular`, `Slack`, `Redshift`, `S3`, `Marketo`, `Googleanalytics`, `Zendesk`, `Servicenow`, `Datadog`, `Trendmicro`, `Snowflake`, `Dynatrace`, `Infornexus`, `Amplitude`, `Veeva`, `EventBridge`, `LookoutMetrics`, `Upsolver`, `Honeycode`, `CustomerProfiles`, `SAPOData`, and `CustomConnector`.
* `destinationConnectorProperties` - (Required) This stores the information that is required to query a particular connector. See [Destination Connector Properties](#destination-connector-properties) for more information.
* `apiVersion` - (Optional) API version that the destination connector uses.
* `connectorProfileName` - (Optional) Name of the connector profile. This name must be unique for each connector profile in the AWS account.

#### Destination Connector Properties

* `customConnector` - (Optional) Properties that are required to query the custom Connector. See [Custom Connector Destination Properties](#custom-connector-destination-properties) for more details.
* `customerProfiles` - (Optional) Properties that are required to query Amazon Connect Customer Profiles. See [Customer Profiles Destination Properties](#customer-profiles-destination-properties) for more details.
* `eventBridge` - (Optional) Properties that are required to query Amazon EventBridge. See [Generic Destination Properties](#generic-destination-properties) for more details.
* `honeycode` - (Optional) Properties that are required to query Amazon Honeycode. See [Generic Destination Properties](#generic-destination-properties) for more details.
* `marketo` - (Optional) Properties that are required to query Marketo. See [Generic Destination Properties](#generic-destination-properties) for more details.
* `redshift` - (Optional) Properties that are required to query Amazon Redshift. See [Redshift Destination Properties](#redshift-destination-properties) for more details.
* `s3` - (Optional) Properties that are required to query Amazon S3. See [S3 Destination Properties](#s3-destination-properties) for more details.
* `salesforce` - (Optional) Properties that are required to query Salesforce. See [Salesforce Destination Properties](#salesforce-destination-properties) for more details.
* `sapoData` - (Optional) Properties that are required to query SAPOData. See [SAPOData Destination Properties](#sapodata-destination-properties) for more details.
* `snowflake` - (Optional) Properties that are required to query Snowflake. See [Snowflake Destination Properties](#snowflake-destination-properties) for more details.
* `upsolver` - (Optional) Properties that are required to query Upsolver. See [Upsolver Destination Properties](#upsolver-destination-properties) for more details.
* `zendesk` - (Optional) Properties that are required to query Zendesk. See [Zendesk Destination Properties](#zendesk-destination-properties) for more details.

##### Generic Destination Properties

EventBridge, Honeycode, and Marketo destination properties all support the following attributes:

* `object` - (Required) Object specified in the flow destination.
* `errorHandlingConfig` - (Optional) Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See [Error Handling Config](#error-handling-config) for more details.

##### Custom Connector Destination Properties

* `entityName` - (Required) Entity specified in the custom connector as a destination in the flow.
* `customProperties` - (Optional) Custom properties that are specific to the connector when it's used as a destination in the flow. Maximum of 50 items.
* `errorHandlingConfig` - (Optional) Settings that determine how Amazon AppFlow handles an error when placing data in the custom connector as destination. See [Error Handling Config](#error-handling-config) for more details.
* `idFieldNames` - (Optional) Name of the field that Amazon AppFlow uses as an ID when performing a write operation such as update, delete, or upsert.
* `writeOperationType` - (Optional) Type of write operation to be performed in the custom connector when it's used as destination. Valid values are `INSERT`, `UPSERT`, `UPDATE`, and `DELETE`.

##### Customer Profiles Destination Properties

* `domainName` - (Required) Unique name of the Amazon Connect Customer Profiles domain.
* `objectTypeName` - (Optional) Object specified in the Amazon Connect Customer Profiles flow destination.

##### Redshift Destination Properties

* `intermediateBucketName` - (Required) Intermediate bucket that Amazon AppFlow uses when moving data into Amazon Redshift.
* `object` - (Required) Object specified in the Amazon Redshift flow destination.
* `bucketPrefix` - (Optional) Object key for the bucket in which Amazon AppFlow places the destination files.
* `errorHandlingConfig` - (Optional) Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See [Error Handling Config](#error-handling-config) for more details.

##### S3 Destination Properties

* `bucketName` - (Required) Amazon S3 bucket name in which Amazon AppFlow places the transferred data.
* `bucketPrefix` - (Optional) Object key for the bucket in which Amazon AppFlow places the destination files.
* `s3OutputFormatConfig` - (Optional) Configuration that determines how Amazon AppFlow should format the flow output data when Amazon S3 is used as the destination. See [S3 Output Format Config](#s3-output-format-config) for more details.

###### S3 Output Format Config

* `aggregationConfig` - (Optional) Aggregation settings that you can use to customize the output format of your flow data. See [Aggregation Config](#aggregation-config) for more details.
* `fileType` - (Optional) File type that Amazon AppFlow places in the Amazon S3 bucket. Valid values are `CSV`, `JSON`, and `PARQUET`.
* `prefixConfig` - (Optional) Determines the prefix that Amazon AppFlow applies to the folder name in the Amazon S3 bucket. You can name folders according to the flow frequency and date. See [Prefix Config](#prefix-config) for more details.
* `preserveSourceDataTyping` - (Optional, Boolean) Whether the data types from the source system need to be preserved (Only valid for `Parquet` file type)

##### Salesforce Destination Properties

* `object` - (Required) Object specified in the flow destination.
* `errorHandlingConfig` - (Optional) Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See [Error Handling Config](#error-handling-config) for more details.
* `idFieldNames` - (Optional) Name of the field that Amazon AppFlow uses as an ID when performing a write operation such as update or delete.
* `writeOperationType` - (Optional) This specifies the type of write operation to be performed in Salesforce. When the value is `UPSERT`, then `idFieldNames` is required. Valid values are `INSERT`, `UPSERT`, `UPDATE`, and `DELETE`.

##### SAPOData Destination Properties

* `objectPath` - (Required) Object path specified in the SAPOData flow destination.
* `errorHandlingConfig` - (Optional) Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See [Error Handling Config](#error-handling-config) for more details.
* `idFieldNames` - (Optional) Name of the field that Amazon AppFlow uses as an ID when performing a write operation such as update or delete.
* `successResponseHandlingConfig` - (Optional) Determines how Amazon AppFlow handles the success response that it gets from the connector after placing data. See [Success Response Handling Config](#success-response-handling-config) for more details.
* `write_operation` - (Optional) Possible write operations in the destination connector. When this value is not provided, this defaults to the `INSERT` operation. Valid values are `INSERT`, `UPSERT`, `UPDATE`, and `DELETE`.

###### Success Response Handling Config

* `bucketName` - (Optional) Name of the Amazon S3 bucket.
* `bucketPrefix` - (Optional) Amazon S3 bucket prefix.

##### Snowflake Destination Properties

* `intermediateBucketName` - (Required) Intermediate bucket that Amazon AppFlow uses when moving data into Amazon Snowflake.
* `object` - (Required) Object specified in the Amazon Snowflake flow destination.
* `bucketPrefix` - (Optional) Object key for the bucket in which Amazon AppFlow places the destination files.
* `errorHandlingConfig` - (Optional) Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See [Error Handling Config](#error-handling-config) for more details.

##### Upsolver Destination Properties

* `bucketName` - (Required) Upsolver Amazon S3 bucket name in which Amazon AppFlow places the transferred data. This must begin with `upsolver-appflow`.
* `bucketPrefix` - (Optional) Object key for the Upsolver Amazon S3 Bucket in which Amazon AppFlow places the destination files.
* `s3OutputFormatConfig` - (Optional) Configuration that determines how Amazon AppFlow should format the flow output data when Upsolver is used as the destination. See [Upsolver S3 Output Format Config](#upsolver-s3-output-format-config) for more details.

###### Upsolver S3 Output Format Config

* `aggregationConfig` - (Optional) Aggregation settings that you can use to customize the output format of your flow data. See [Aggregation Config](#aggregation-config) for more details.
* `fileType` - (Optional) File type that Amazon AppFlow places in the Upsolver Amazon S3 bucket. Valid values are `CSV`, `JSON`, and `PARQUET`.
* `prefixConfig` - (Optional) Determines the prefix that Amazon AppFlow applies to the folder name in the Amazon S3 bucket. You can name folders according to the flow frequency and date. See [Prefix Config](#prefix-config) for more details.

###### Aggregation Config

* `aggregationType` - (Optional) Whether Amazon AppFlow aggregates the flow records into a single file, or leave them unaggregated. Valid values are `None` and `SingleFile`.
* `targetFileSize` - (Optional) The desired file size, in MB, for each output file that Amazon AppFlow writes to the flow destination. Integer value.

###### Prefix Config

* `prefixFormat` - (Optional) Determines the level of granularity that's included in the prefix. Valid values are `YEAR`, `MONTH`, `DAY`, `HOUR`, and `MINUTE`.
* `prefixType` - (Optional) Determines the format of the prefix, and whether it applies to the file name, file path, or both. Valid values are `FILENAME`, `PATH`, and `PATH_AND_FILENAME`.

##### Zendesk Destination Properties

* `object` - (Required) Object specified in the flow destination.
* `errorHandlingConfig` - (Optional) Settings that determine how Amazon AppFlow handles an error when placing data in the destination. See [Error Handling Config](#error-handling-config) for more details.
* `idFieldNames` - (Optional) Name of the field that Amazon AppFlow uses as an ID when performing a write operation such as update or delete.
* `writeOperationType` - (Optional) This specifies the type of write operation to be performed in Zendesk. When the value is `UPSERT`, then `idFieldNames` is required. Valid values are `INSERT`, `UPSERT`, `UPDATE`, and `DELETE`.

###### Error Handling Config

* `bucketName` - (Optional) Name of the Amazon S3 bucket.
* `bucketPrefix` - (Optional) Amazon S3 bucket prefix.
* `failOnFirstDestinationError` - (Optional, boolean) If the flow should fail after the first instance of a failure when attempting to place data in the destination.

### Source Flow Config

* `connectorType` - (Required) Type of connector, such as Salesforce, Amplitude, and so on. Valid values are `Salesforce`, `Singular`, `Slack`, `Redshift`, `S3`, `Marketo`, `Googleanalytics`, `Zendesk`, `Servicenow`, `Datadog`, `Trendmicro`, `Snowflake`, `Dynatrace`, `Infornexus`, `Amplitude`, `Veeva`, `EventBridge`, `LookoutMetrics`, `Upsolver`, `Honeycode`, `CustomerProfiles`, `SAPOData`, and `CustomConnector`.
* `sourceConnectorProperties` - (Required) Information that is required to query a particular source connector. See [Source Connector Properties](#source-connector-properties) for details.
* `apiVersion` - (Optional) API version that the destination connector uses.
* `connectorProfileName` - (Optional) Name of the connector profile. This name must be unique for each connector profile in the AWS account.
* `incrementalPullConfig` - (Optional) Defines the configuration for a scheduled incremental data pull. If a valid configuration is provided, the fields specified in the configuration are used when querying for the incremental data pull. See [Incremental Pull Config](#incremental-pull-config) for more details.

#### Source Connector Properties

* `amplitude` - (Optional) Information that is required for querying Amplitude. See [Generic Source Properties](#generic-source-properties) for more details.
* `customConnector` - (Optional) Properties that are applied when the custom connector is being used as a source. See [Custom Connector Source Properties](#custom-connector-source-properties).
* `datadog` - (Optional) Information that is required for querying Datadog. See [Generic Source Properties](#generic-source-properties) for more details.
* `dynratrace` - (Optional) Information that is required for querying Dynatrace. See [Generic Source Properties](#generic-source-properties) for more details.
* `inforNexus` - (Optional) Information that is required for querying Infor Nexus. See [Generic Source Properties](#generic-source-properties) for more details.
* `marketo` - (Optional) Information that is required for querying Marketo. See [Generic Source Properties](#generic-source-properties) for more details.
* `s3` - (Optional) Information that is required for querying Amazon S3. See [S3 Source Properties](#s3-source-properties) for more details.
* `salesforce` - (Optional) Information that is required for querying Salesforce. See [Salesforce Source Properties](#s3-source-properties) for more details.
* `sapoData` - (Optional) Information that is required for querying SAPOData as a flow source. See [SAPO Source Properties](#sapodata-source-properties) for more details.
* `serviceNow` - (Optional) Information that is required for querying ServiceNow. See [Generic Source Properties](#generic-source-properties) for more details.
* `singular` - (Optional) Information that is required for querying Singular. See [Generic Source Properties](#generic-source-properties) for more details.
* `slack` - (Optional) Information that is required for querying Slack. See [Generic Source Properties](#generic-source-properties) for more details.
* `trend_micro` - (Optional) Information that is required for querying Trend Micro. See [Generic Source Properties](#generic-source-properties) for more details.
* `veeva` - (Optional) Information that is required for querying Veeva. See [Veeva Source Properties](#veeva-source-properties) for more details.
* `zendesk` - (Optional) Information that is required for querying Zendesk. See [Generic Source Properties](#generic-source-properties) for more details.

##### Generic Source Properties

Amplitude, Datadog, Dynatrace, Google Analytics, Infor Nexus, Marketo, ServiceNow, Singular, Slack, Trend Micro, and Zendesk source properties all support the following attributes:

* `object` - (Required) Object specified in the flow source.

##### Custom Connector Source Properties

* `entityName` - (Required) Entity specified in the custom connector as a source in the flow.
* `customProperties` - (Optional) Custom properties that are specific to the connector when it's used as a source in the flow. Maximum of 50 items.

##### S3 Source Properties

* `bucketName` - (Required) Amazon S3 bucket name where the source files are stored.
* `bucketPrefix` - (Optional) Object key for the Amazon S3 bucket in which the source files are stored.
* `s3InputFormatConfig` - (Optional) When you use Amazon S3 as the source, the configuration format that you provide the flow input data. See [S3 Input Format Config](#s3-input-format-config) for details.

###### S3 Input Format Config

* `s3InputFileType` - (Optional) File type that Amazon AppFlow gets from your Amazon S3 bucket. Valid values are `CSV` and `JSON`.

##### Salesforce Source Properties

* `object` - (Required) Object specified in the Salesforce flow source.
* `enableDynamicFieldUpdate` - (Optional, boolean) Flag that enables dynamic fetching of new (recently added) fields in the Salesforce objects while running a flow.
* `includeDeletedRecords` - (Optional, boolean) Whether Amazon AppFlow includes deleted files in the flow run.

##### SAPOData Source Properties

* `objectPath` - (Required) Object path specified in the SAPOData flow source.

##### Veeva Source Properties

* `object` - (Required) Object specified in the Veeva flow source.
* `documentType` - (Optional) Document type specified in the Veeva document extract flow.
* `includeAllVersions` - (Optional, boolean) Boolean value to include All Versions of files in Veeva document extract flow.
* `includeRenditions` - (Optional, boolean) Boolean value to include file renditions in Veeva document extract flow.
* `includeSourceFiles` - (Optional, boolean) Boolean value to include source files in Veeva document extract flow.

#### Incremental Pull Config

* `datetimeTypeFieldName` - (Optional) Field that specifies the date time or timestamp field as the criteria to use when importing incremental records from the source.

### Task

* `sourceFields` - (Required) Source fields to which a particular task is applied.
* `taskType` - (Required) Particular task implementation that Amazon AppFlow performs. Valid values are `Arithmetic`, `Filter`, `Map`, `Map_all`, `Mask`, `Merge`, `Passthrough`, `Truncate`, and `Validate`.
* `connectorOperator` - (Optional) Operation to be performed on the provided source fields. See [Connector Operator](#connector-operator) for details.
* `destinationField` - (Optional) Field in a destination connector, or a field value against which Amazon AppFlow validates a source field.
* `taskProperties` - (Optional) Map used to store task-related information. The execution service looks for particular information based on the `TaskType`. Valid keys are `VALUE`, `VALUES`, `DATA_TYPE`, `UPPER_BOUND`, `LOWER_BOUND`, `SOURCE_DATA_TYPE`, `DESTINATION_DATA_TYPE`, `VALIDATION_ACTION`, `MASK_VALUE`, `MASK_LENGTH`, `TRUNCATE_LENGTH`, `MATH_OPERATION_FIELDS_ORDER`, `CONCAT_FORMAT`, `SUBFIELD_CATEGORY_MAP`, and `EXCLUDE_SOURCE_FIELDS_LIST`.

#### Connector Operator

* `amplitude` - (Optional) Operation to be performed on the provided Amplitude source fields. The only valid value is `BETWEEN`.
* `customConnector` - (Optional) Operators supported by the custom connector. Valid values are `PROJECTION`, `LESS_THAN`, `GREATER_THAN`, `CONTAINS`, `BETWEEN`, `LESS_THAN_OR_EQUAL_TO`, `GREATER_THAN_OR_EQUAL_TO`, `EQUAL_TO`, `NOT_EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
* `datadog` - (Optional) Operation to be performed on the provided Datadog source fields. Valid values are `PROJECTION`, `BETWEEN`, `EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
* `dynatrace` - (Optional) Operation to be performed on the provided Dynatrace source fields. Valid values are `PROJECTION`, `BETWEEN`, `EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
* `googleAnalytics` - (Optional) Operation to be performed on the provided Google Analytics source fields. Valid values are `PROJECTION` and `BETWEEN`.
* `inforNexus` - (Optional) Operation to be performed on the provided Infor Nexus source fields. Valid values are `PROJECTION`, `BETWEEN`, `EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
* `marketo` - (Optional) Operation to be performed on the provided Marketo source fields. Valid values are `PROJECTION`, `BETWEEN`, `EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
* `s3` - (Optional) Operation to be performed on the provided Amazon S3 source fields. Valid values are `PROJECTION`, `LESS_THAN`, `GREATER_THAN`, `BETWEEN`, `LESS_THAN_OR_EQUAL_TO`, `GREATER_THAN_OR_EQUAL_TO`, `EQUAL_TO`, `NOT_EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
* `salesforce` - (Optional) Operation to be performed on the provided Salesforce source fields. Valid values are `PROJECTION`, `LESS_THAN`, `GREATER_THAN`, `CONTAINS`, `BETWEEN`, `LESS_THAN_OR_EQUAL_TO`, `GREATER_THAN_OR_EQUAL_TO`, `EQUAL_TO`, `NOT_EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
* `sapoData` - (Optional) Operation to be performed on the provided SAPOData source fields. Valid values are `PROJECTION`, `LESS_THAN`, `GREATER_THAN`, `CONTAINS`, `BETWEEN`, `LESS_THAN_OR_EQUAL_TO`, `GREATER_THAN_OR_EQUAL_TO`, `EQUAL_TO`, `NOT_EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
* `serviceNow` - (Optional) Operation to be performed on the provided ServiceNow source fields. Valid values are `PROJECTION`, `LESS_THAN`, `GREATER_THAN`, `CONTAINS`, `BETWEEN`, `LESS_THAN_OR_EQUAL_TO`, `GREATER_THAN_OR_EQUAL_TO`, `EQUAL_TO`, `NOT_EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
* `singular` - (Optional) Operation to be performed on the provided Singular source fields. Valid values are `PROJECTION`, `EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
* `slack` - (Optional) Operation to be performed on the provided Slack source fields. Valid values are `PROJECTION`, `LESS_THAN`, `GREATER_THAN`, `BETWEEN`, `LESS_THAN_OR_EQUAL_TO`, `GREATER_THAN_OR_EQUAL_TO`, `EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
* `trendmicro` - (Optional) Operation to be performed on the provided Trend Micro source fields. Valid values are `PROJECTION`, `EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
* `veeva` - (Optional) Operation to be performed on the provided Veeva source fields. Valid values are `PROJECTION`, `LESS_THAN`, `GREATER_THAN`, `CONTAINS`, `BETWEEN`, `LESS_THAN_OR_EQUAL_TO`, `GREATER_THAN_OR_EQUAL_TO`, `EQUAL_TO`, `NOT_EQUAL_TO`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.
* `zendesk` - (Optional) Operation to be performed on the provided Zendesk source fields. Valid values are `PROJECTION`, `GREATER_THAN`, `ADDITION`, `MULTIPLICATION`, `DIVISION`, `SUBTRACTION`, `MASK_ALL`, `MASK_FIRST_N`, `MASK_LAST_N`, `VALIDATE_NON_NULL`, `VALIDATE_NON_ZERO`, `VALIDATE_NON_NEGATIVE`, `VALIDATE_NUMERIC`, and `NO_OP`.

### Trigger Config

* `triggerType` - (Required) Type of flow trigger. Valid values are `Scheduled`, `Event`, and `OnDemand`.
* `triggerProperties` - (Optional) Configuration details of a schedule-triggered flow as defined by the user. Currently, these settings only apply to the `Scheduled` trigger type. See [Scheduled Trigger Properties](#scheduled-trigger-properties) for details.

#### Scheduled Trigger Properties

The `triggerProperties` block only supports one attribute: `scheduled`, a block which in turn supports the following:

* `scheduleExpression` - (Required) Scheduling expression that determines the rate at which the schedule will run, for example `rate(5minutes)`.
* `dataPullMode` - (Optional) Whether a scheduled flow has an incremental data transfer or a complete data transfer for each flow run. Valid values are `Incremental` and `Complete`.
* `firstExecutionFrom` - (Optional) Date range for the records to import from the connector in the first flow run. Must be a valid RFC3339 timestamp.
* `scheduleEndTime` - (Optional) Scheduled end time for a schedule-triggered flow. Must be a valid RFC3339 timestamp.
* `scheduleOffset` - (Optional) Optional offset that is added to the time interval for a schedule-triggered flow. Maximum value of 36000.
* `scheduleStartTime` - (Optional) Scheduled start time for a schedule-triggered flow. Must be a valid RFC3339 timestamp.
* `timezone` - (Optional) Time zone used when referring to the date and time of a scheduled-triggered flow, such as `America/New_York`.

```typescript
// DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
import { Construct } from "constructs";
import { TerraformStack } from "cdktf";
/*
 * Provider bindings are generated by running `cdktf get`.
 * See https://cdk.tf/provider-generation for more details.
 */
import { AppflowFlow } from "./.gen/providers/aws/appflow-flow";
interface MyConfig {
  triggerType: any;
  destinationFlowConfig: any;
  name: any;
  sourceFlowConfig: any;
  task: any;
}
class MyConvertedCode extends TerraformStack {
  constructor(scope: Construct, name: string, config: MyConfig) {
    super(scope, name);
    new AppflowFlow(this, "example", {
      triggerConfig: {
        scheduled: [
          {
            schedule_expression: "rate(1minutes)",
          },
        ],
        triggerType: config.triggerType,
      },
      destinationFlowConfig: config.destinationFlowConfig,
      name: config.name,
      sourceFlowConfig: config.sourceFlowConfig,
      task: config.task,
    });
  }
}

```

## Attribute Reference

This resource exports the following attributes in addition to the arguments above:

* `arn` - Flow's ARN.
* `flowStatus` - The current status of the flow.
* `tagsAll` - Map of tags assigned to the resource, including those inherited from the provider [`defaultTags` configuration block](https://registry.terraform.io/providers/hashicorp/aws/latest/docs#default_tags-configuration-block).

## Import

In Terraform v1.5.0 and later, use an [`import` block](https://developer.hashicorp.com/terraform/language/import) to import AppFlow flows using the `arn`. For example:

```typescript
// DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
import { Construct } from "constructs";
import { TerraformStack } from "cdktf";
/*
 * Provider bindings are generated by running `cdktf get`.
 * See https://cdk.tf/provider-generation for more details.
 */
import { AppflowFlow } from "./.gen/providers/aws/appflow-flow";
class MyConvertedCode extends TerraformStack {
  constructor(scope: Construct, name: string) {
    super(scope, name);
    AppflowFlow.generateConfigForImport(
      this,
      "example",
      "arn:aws:appflow:us-west-2:123456789012:flow/example-flow"
    );
  }
}

```

Using `terraform import`, import AppFlow flows using the `arn`. For example:

```console
% terraform import aws_appflow_flow.example arn:aws:appflow:us-west-2:123456789012:flow/example-flow
```

<!-- cache-key: cdktf-0.20.1 input-0b70032feb9b4bc2846b4d1c09cc18156c67303d01c69b123a45fe5f6c3c976c -->