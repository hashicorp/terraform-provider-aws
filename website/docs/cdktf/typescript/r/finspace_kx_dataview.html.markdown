---
subcategory: "FinSpace"
layout: "aws"
page_title: "AWS: aws_finspace_kx_dataview"
description: |-
  Terraform resource for managing an AWS FinSpace Kx Dataview.
---


<!-- Please do not edit this file, it is generated. -->
# Resource: aws_finspace_kx_dataview

Terraform resource for managing an AWS FinSpace Kx Dataview.

## Example Usage

### Basic Usage

```typescript
// DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
import { Construct } from "constructs";
import { Token, TerraformStack } from "cdktf";
/*
 * Provider bindings are generated by running `cdktf get`.
 * See https://cdk.tf/provider-generation for more details.
 */
import { FinspaceKxDataview } from "./.gen/providers/aws/finspace-kx-dataview";
class MyConvertedCode extends TerraformStack {
  constructor(scope: Construct, name: string) {
    super(scope, name);
    new FinspaceKxDataview(this, "example", {
      autoUpdate: true,
      availabilityZoneId: "use1-az2",
      azMode: "SINGLE",
      databaseName: Token.asString(awsFinspaceKxDatabaseExample.name),
      description: "Terraform managed Kx Dataview",
      environmentId: Token.asString(awsFinspaceKxEnvironmentExample.id),
      name: "my-tf-kx-dataview",
      segmentConfigurations: [
        {
          dbPaths: ["/*"],
          volumeName: Token.asString(awsFinspaceKxVolumeExample.name),
        },
      ],
      timeouts: [
        {
          create: "24h",
          delete: "12h",
          update: "24h",
        },
      ],
    });
  }
}

```

## Argument Reference

The following arguments are required:

* `azMode` - (Required) The number of availability zones you want to assign per cluster. This can be one of the following:
    * `SINGLE` - Assigns one availability zone per cluster.
    * `MULTI` - Assigns all the availability zones per cluster.
* `databaseName` - (Required) The name of the database where you want to create a dataview.
* `environmentId` - (Required) Unique identifier for the KX environment.
* `name` - (Required) A unique identifier for the dataview.

The following arguments are optional:

* `autoUpdate` - (Optional) The option to specify whether you want to apply all the future additions and corrections automatically to the dataview, when you ingest new changesets. The default value is false.
* `availabilityZoneId` - (Optional) The identifier of the availability zones. If attaching a volume, the volume must be in the same availability zone as the dataview that you are attaching to.
* `changesetId` - (Optional) A unique identifier of the changeset of the database that you want to use to ingest data.
* `description` - (Optional) A description for the dataview.
* `readWrite` - (Optional) The option to specify whether you want to make the dataview writable to perform database maintenance. The following are some considerations related to writable dataviews.
    * You cannot create partial writable dataviews. When you create writeable dataviews you must provide the entire database path. You cannot perform updates on a writeable dataview. Hence, `autoUpdate` must be set as `false` if `readWrite` is `true` for a dataview.
    * You must also use a unique volume for creating a writeable dataview. So, if you choose a volume that is already in use by another dataview, the dataview creation fails.
    * Once you create a dataview as writeable, you cannot change it to read-only. So, you cannot update the `readWrite` parameter later.
* `segmentConfigurations` - (Optional) The configuration that contains the database path of the data that you want to place on each selected volume. Each segment must have a unique database path for each volume. If you do not explicitly specify any database path for a volume, they are accessible from the cluster through the default S3/object store segment. See [segment_configurations](#segment_configurations-argument-reference) below.
* `tags` - (Optional) Key-value mapping of resource tags. If configured with a provider [`defaultTags` configuration block](/docs/providers/aws/index.html#default_tags-configuration-block) present, tags with matching keys will overwrite those defined at the provider-level.

### `segmentConfigurations` Argument Reference

* `dbPaths` - (Required) The database path of the data that you want to place on each selected volume. Each segment must have a unique database path for each volume.
* `volumeName` - (Required) The name of the volume that you want to attach to a dataview. This volume must be in the same availability zone as the dataview that you are attaching to.
* `onDemand` - (Optional) Enables on-demand caching on the selected database path when a particular file or a column of the database is accessed. When on demand caching is **True**, dataviews perform minimal loading of files on the filesystem as needed. When it is set to **False**, everything is cached. The default value is **False**.

## Attribute Reference

This resource exports the following attributes in addition to the arguments above:

* `arn` - Amazon Resource Name (ARN) identifier of the KX dataview.
* `createdTimestamp` - Timestamp at which the dataview was created in FinSpace. Value determined as epoch time in milliseconds. For example, the value for Monday, November 1, 2021 12:00:00 PM UTC is specified as 1635768000000.
* `id` - A comma-delimited string joining environment ID, database name and dataview name.
* `lastModifiedTimestamp` - The last time that the dataview was updated in FinSpace. The value is determined as epoch time in milliseconds. For example, the value for Monday, November 1, 2021 12:00:00 PM UTC is specified as 1635768000000.
* `tagsAll` - Map of tags assigned to the resource, including those inherited from the provider [`defaultTags` configuration block](/docs/providers/aws/index.html#default_tags-configuration-block).

## Timeouts

[Configuration options](https://developer.hashicorp.com/terraform/language/resources/syntax#operation-timeouts):

* `create` - (Default `4h`)
* `update` - (Default `4h`)
* `delete` - (Default `4h`)

## Import

In Terraform v1.5.0 and later, use an [`import` block](https://developer.hashicorp.com/terraform/language/import) to import an AWS FinSpace Kx Dataview using the `id` (environment ID, database name and dataview name, comma-delimited). For example:

```typescript
// DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
import { Construct } from "constructs";
import { TerraformStack } from "cdktf";
/*
 * Provider bindings are generated by running `cdktf get`.
 * See https://cdk.tf/provider-generation for more details.
 */
import { FinspaceKxDataview } from "./.gen/providers/aws/finspace-kx-dataview";
class MyConvertedCode extends TerraformStack {
  constructor(scope: Construct, name: string) {
    super(scope, name);
    FinspaceKxDataview.generateConfigForImport(
      this,
      "example",
      "n3ceo7wqxoxcti5tujqwzs,my-tf-kx-database,my-tf-kx-dataview"
    );
  }
}

```

Using `terraform import`, import an AWS FinSpace Kx Cluster using the `id` (environment ID and cluster name, comma-delimited). For example:

```console
% terraform import aws_finspace_kx_dataview.example n3ceo7wqxoxcti5tujqwzs,my-tf-kx-database,my-tf-kx-dataview
```

<!-- cache-key: cdktf-0.20.1 input-5a3271854ee031025c68a96f6e5cacafb6953e695fa8365f00a76a6faf3ac92a -->