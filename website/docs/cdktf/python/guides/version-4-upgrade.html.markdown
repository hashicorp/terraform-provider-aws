---
subcategory: ""
layout: "aws"
page_title: "Terraform AWS Provider Version 4 Upgrade Guide"
description: |-
  Terraform AWS Provider Version 4 Upgrade Guide
---


<!-- Please do not edit this file, it is generated. -->
# Terraform AWS Provider Version 4 Upgrade Guide

Version 4.0.0 of the AWS provider for Terraform is a major release and includes some changes that you will need to consider when upgrading. We intend this guide to help with that process and focus only on changes from version 3.X to version 4.0.0. See the [Version 3 Upgrade Guide](/docs/providers/aws/guides/version-3-upgrade.html) for information about upgrading from 2.X to version 3.0.0.

We previously marked most of the changes we outline in this guide as deprecated in the Terraform plan/apply output throughout previous provider releases. You can find these changes, including deprecation notices, in the [Terraform AWS Provider CHANGELOG](https://github.com/hashicorp/terraform-provider-aws/blob/main/CHANGELOG.md).

~> **NOTE:** Versions 4.0.0 through v4.8.0 of the AWS Provider introduce significant breaking changes to the `aws_s3_bucket` resource. See [S3 Bucket Refactor](#s3-bucket-refactor) for more details.
We recommend upgrading to v4.9.0 or later of the AWS Provider instead, where only non-breaking changes and deprecation notices are introduced to the `aws_s3_bucket`. See  [Changes to S3 Bucket Drift Detection](#changes-to-s3-bucket-drift-detection) for additional considerations when upgrading to v4.9.0 or later.

~> **NOTE:** Version 4.0.0 of the AWS Provider introduces changes to the precedence of some authentication and configuration parameters.
These changes bring the provider in line with the AWS CLI and SDKs.
See [Changes to Authentication](#changes-to-authentication) for more details.

~> **NOTE:** Version 4.0.0 of the AWS Provider will be the last major version to support [EC2-Classic resources](#ec2-classic-resource-and-data-source-support) as AWS plans to fully retire EC2-Classic Networking. See the [AWS News Blog](https://aws.amazon.com/blogs/aws/ec2-classic-is-retiring-heres-how-to-prepare/) for additional details.

~> **NOTE:** Version 4.0.0 of the AWS Provider will be the last major version to support [Macie Classic resources](#macie-classic-resource-support) as AWS plans to fully retire Macie Classic. See the [Amazon Macie Classic FAQs](https://aws.amazon.com/macie/classic-faqs/) for additional details.

Upgrade topics:

<!-- TOC depthFrom:2 depthTo:2 -->

- [Provider Version Configuration](#provider-version-configuration)
- [Changes to Authentication](#changes-to-authentication)
- [New Provider Arguments](#new-provider-arguments)
- [Changes to S3 Bucket Drift Detection](#changes-to-s3-bucket-drift-detection) (**Applicable to v4.9.0 and later of the AWS Provider**)
- [S3 Bucket Refactor](#s3-bucket-refactor) (**Only applicable to v4.0.0 through v4.8.0 of the AWS Provider**)
    - [`acceleration_status` Argument](#acceleration_status-argument)
    - [`acl` Argument](#acl-argument)
    - [`cors_rule` Argument](#cors_rule-argument)
    - [`grant` Argument](#grant-argument)
    - [`lifecycle_rule` Argument](#lifecycle_rule-argument)
    - [`logging` Argument](#logging-argument)
    - [`object_lock_configuration` `rule` Argument](#object_lock_configuration-rule-argument)
    - [`policy` Argument](#policy-argument)
    - [`replication_configuration` Argument](#replication_configuration-argument)
    - [`request_payer` Argument](#request_payer-argument)
    - [`server_side_encryption_configuration` Argument](#server_side_encryption_configuration-argument)
    - [`versioning` Argument](#versioning-argument)
    - [`website`, `website_domain`, and `website_endpoint` Arguments](#website-website_domain-and-website_endpoint-arguments)
- [Full Resource Lifecycle of Default Resources](#full-resource-lifecycle-of-default-resources)
    - [Resource: aws_default_subnet](#resource-aws_default_subnet)
    - [Resource: aws_default_vpc](#resource-aws_default_vpc)
- [Plural Data Source Behavior](#plural-data-source-behavior)
- [Empty Strings Not Valid For Certain Resources](#empty-strings-not-valid-for-certain-resources)
    - [Resource: aws_cloudwatch_event_target (Empty String)](#resource-aws_cloudwatch_event_target-empty-string)
    - [Resource: aws_customer_gateway](#resource-aws_customer_gateway)
    - [Resource: aws_default_network_acl](#resource-aws_default_network_acl)
    - [Resource: aws_default_route_table](#resource-aws_default_route_table)
    - [Resource: aws_default_vpc (Empty String)](#resource-aws_default_vpc-empty-string)
    - [Resource: aws_efs_mount_target](#resource-aws_efs_mount_target)
    - [Resource: aws_elasticsearch_domain](#resource-aws_elasticsearch_domain)
    - [Resource: aws_instance](#resource-aws_instance)
    - [Resource: aws_network_acl](#resource-aws_network_acl)
    - [Resource: aws_route](#resource-aws_route)
    - [Resource: aws_route_table](#resource-aws_route_table)
    - [Resource: aws_vpc](#resource-aws_vpc)
    - [Resource: aws_vpc_ipv6_cidr_block_association](#resource-aws_vpc_ipv6_cidr_block_association)
- [Data Source: aws_cloudwatch_log_group](#data-source-aws_cloudwatch_log_group)
- [Data Source: aws_subnet_ids](#data-source-aws_subnet_ids)
- [Data Source: aws_s3_bucket_object](#data-source-aws_s3_bucket_object)
- [Data Source: aws_s3_bucket_objects](#data-source-aws_s3_bucket_objects)
- [Resource: aws_batch_compute_environment](#resource-aws_batch_compute_environment)
- [Resource: aws_cloudwatch_event_target](#resource-aws_cloudwatch_event_target)
- [Resource: aws_elasticache_cluster](#resource-aws_elasticache_cluster)
- [Resource: aws_elasticache_global_replication_group](#resource-aws_elasticache_global_replication_group)
- [Resource: aws_fsx_ontap_storage_virtual_machine](#resource-aws_fsx_ontap_storage_virtual_machine)
- [Resource: aws_lb_target_group](#resource-aws_lb_target_group)
- [Resource: aws_s3_bucket_object](#resource-aws_s3_bucket_object)

<!-- /TOC -->

Additional Topics:

<!-- TOC depthFrom:2 depthTo:2 -->

- [EC2-Classic resource and data source support](#ec2-classic-resource-and-data-source-support)
- [Macie Classic resource support](#macie-classic-resource-support)

<!-- /TOC -->

## Provider Version Configuration

-> Before upgrading to version 4.0.0, upgrade to the most recent 3.X version of the provider and ensure that your environment successfully runs [`terraform plan`](https://www.terraform.io/docs/commands/plan.html). You should not see changes you don't expect or deprecation notices.

Use [version constraints when configuring Terraform providers](https://www.terraform.io/docs/configuration/providers.html#provider-versions). If you are following that recommendation, update the version constraints in your Terraform configuration and run [`terraform init -upgrade`](https://www.terraform.io/docs/commands/init.html) to download the new version.

For example, given this previous configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.provider import AwsProvider
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        AwsProvider(self, "aws")
```

Update to the latest 4.X version:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.provider import AwsProvider
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        AwsProvider(self, "aws")
```

## Changes to Authentication

The authentication configuration for the AWS Provider has changed in this version to match the behavior of other AWS products, including the AWS SDK and AWS CLI. _This will cause authentication failures in AWS provider configurations where you set a non-empty `profile` in the `provider` configuration but the profile does not correspond to an AWS profile with valid credentials._

Precedence for authentication settings is as follows:

* `provider` configuration
* Environment variables
* Shared credentials and configuration files (_e.g._, `~/.aws/credentials` and `~/.aws/config`)

In previous versions of the provider, you could explicitly set `profile` in the `provider`, and if the profile did not correspond to valid credentials, the provider would use credentials from environment variables. Starting in v4.0, the Terraform AWS provider enforces the precedence shown above, similarly to how the AWS SDK and AWS CLI behave.

In other words, when you explicitly set `profile` in `provider`, the AWS provider will not use environment variables per the precedence shown above. Before v4.0, if `profile` was configured in the `provider` configuration but did not correspond to an AWS profile or valid credentials, the provider would attempt to use environment variables. **This is no longer the case.** An explicitly set profile that does not have valid credentials will cause an authentication error.

For example, with the following, the environment variables will not be used:

```console
$ export AWS_ACCESS_KEY_ID="anaccesskey"
$ export AWS_SECRET_ACCESS_KEY="asecretkey"
```

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.provider import AwsProvider
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        AwsProvider(self, "aws",
            profile="customprofile",
            region="us-west-2"
        )
```

## New Provider Arguments

Version 4.x adds these new `provider` arguments:

* `assume_role.duration` - Assume role duration as a string, _e.g._, `"1h"` or `"1h30s"`. Terraform AWS Provider v4.0.0 deprecates `assume_role.duration_seconds` and a future version will remove it.
* `custom_ca_bundle` - File containing custom root and intermediate certificates. Can also be configured using the `AWS_CA_BUNDLE` environment variable. (Setting `ca_bundle` in the shared config file is not supported.)
* `ec2_metadata_service_endpoint` - Address of the EC2 metadata service (IMDS) endpoint to use. Can also be set with the `AWS_EC2_METADATA_SERVICE_ENDPOINT` environment variable.
* `ec2_metadata_service_endpoint_mode` - Mode to use in communicating with the metadata service. Valid values are `IPv4` and `IPv6`. Can also be set with the `AWS_EC2_METADATA_SERVICE_ENDPOINT_MODE` environment variable.
* `s3_use_path_style` - Replaces `s3_force_path_style`, which has been deprecated in Terraform AWS Provider v4.0.0 and support will be removed in a future version.
* `shared_config_files` - List of paths to AWS shared config files. If not set, the default is `[~/.aws/config]`. A single value can also be set with the `AWS_CONFIG_FILE` environment variable.
* `shared_credentials_files` - List of paths to the shared credentials file. If not set, the default  is `[~/.aws/credentials]`. A single value can also be set with the `AWS_SHARED_CREDENTIALS_FILE` environment variable. Replaces `shared_credentials_file`, which has been deprecated in Terraform AWS Provider v4.0.0 and support will be removed in a future version.
* `sts_region` - Region where AWS STS operations will take place. For example, `us-east-1` and `us-west-2`.
* `use_dualstack_endpoint` - Force the provider to resolve endpoints with DualStack capability. Can also be set with the `AWS_USE_DUALSTACK_ENDPOINT` environment variable or in a shared config file (`use_dualstack_endpoint`).
* `use_fips_endpoint` - Force the provider to resolve endpoints with FIPS capability. Can also be set with the `AWS_USE_FIPS_ENDPOINT` environment variable or in a shared config file (`use_fips_endpoint`).

~> **NOTE:** Using the `AWS_METADATA_URL` environment variable has been deprecated in Terraform AWS Provider v4.0.0 and support will be removed in a future version. Change any scripts or environments using `AWS_METADATA_URL` to instead use `AWS_EC2_METADATA_SERVICE_ENDPOINT`.

For example, in previous versions, to use FIPS endpoints, you would need to provide all the FIPS endpoints that you wanted to use in the `endpoints` configuration block:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.provider import AwsProvider
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        AwsProvider(self, "aws",
            endpoints=[AwsProviderEndpoints(
                ec2="https://ec2-fips.us-west-2.amazonaws.com",
                s3="https://s3-fips.us-west-2.amazonaws.com",
                sts="https://sts-fips.us-west-2.amazonaws.com"
            )
            ]
        )
```

In v4.0.0, you can still set endpoints in the same way. However, you can instead use the `use_fips_endpoint` argument to have the provider automatically resolve FIPS endpoints for all supported services:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.provider import AwsProvider
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        AwsProvider(self, "aws",
            use_fips_endpoint=True
        )
```

Note that the provider can only resolve FIPS endpoints where AWS provides FIPS support. Support depends on the service and may include `us-east-1`, `us-east-2`, `us-west-1`, `us-west-2`, `us-gov-east-1`, `us-gov-west-1`, and `ca-central-1`. For more information, see [Federal Information Processing Standard (FIPS) 140-2](https://aws.amazon.com/compliance/fips/).

## Changes to S3 Bucket Drift Detection

~> **NOTE:** This only applies to v4.9.0 and later of the AWS Provider.

~> **NOTE:** If you are migrating from v3.75.x of the AWS Provider and you have already adopted the standalone S3 bucket resources (e.g. `aws_s3_bucket_lifecycle_configuration`),
a [`lifecycle` configuration block to ignore changes](https://www.terraform.io/language/meta-arguments/lifecycle#ignore_changes) to the internal parameters of the source `aws_s3_bucket` resources will no longer be necessary and can be removed upon upgrade.

~> **NOTE:** In the next major version, v5.0, the parameters listed below will be removed entirely from the `aws_s3_bucket` resource.
For this reason, a deprecation notice is printed in the Terraform CLI for each of the parameters when used in a configuration.

To remediate the breaking changes introduced to the `aws_s3_bucket` resource in v4.0.0 of the AWS Provider,
v4.9.0 and later retain the same configuration parameters of the `aws_s3_bucket` resource as in v3.x and functionality of the `aws_s3_bucket` resource only differs from v3.x
in that Terraform will only perform drift detection for each of the following parameters if a configuration value is provided:

* `acceleration_status`
* `acl`
* `cors_rule`
* `grant`
* `lifecycle_rule`
* `logging`
* `object_lock_configuration`
* `policy`
* `replication_configuration`
* `request_payer`
* `server_side_encryption_configuration`
* `versioning`
* `website`

Thus, if one of these parameters was once configured and then is entirely removed from an `aws_s3_bucket` resource configuration,
Terraform will not pick up on these changes on a subsequent `terraform plan` or `terraform apply`.

For example, given the following configuration with a single `cors_rule`:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3Bucket(self, "example",
            bucket="yournamehere",
            cors_rule=[S3BucketCorsRule(
                allowed_headers=["*"],
                allowed_methods=["PUT", "POST"],
                allowed_origins=["https://s3-website-test.hashicorp.com"],
                expose_headers=["ETag"],
                max_age_seconds=3000
            )
            ]
        )
```

When updated to the following configuration without a `cors_rule`:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3Bucket(self, "example",
            bucket="yournamehere"
        )
```

Terraform CLI with v4.9.0 of the AWS Provider will report back:

```console
aws_s3_bucket.example: Refreshing state... [id=yournamehere]
...
No changes. Your infrastructure matches the configuration.
```

With that said, to manage changes to these parameters in the `aws_s3_bucket` resource, practitioners should configure each parameter's respective standalone resource
and perform updates directly on those new configurations. The parameters are mapped to the standalone resources as follows:

| `aws_s3_bucket` Parameter              | Standalone Resource                                  |
|----------------------------------------|------------------------------------------------------|
| `acceleration_status`                  | `aws_s3_bucket_accelerate_configuration`             |
| `acl`                                  | `aws_s3_bucket_acl`                                  |
| `cors_rule`                            | `aws_s3_bucket_cors_configuration`                   |
| `grant`                                | `aws_s3_bucket_acl`                                  |
| `lifecycle_rule`                       | `aws_s3_bucket_lifecycle_configuration`              |
| `logging`                              | `aws_s3_bucket_logging`                              |
| `object_lock_configuration`            | `aws_s3_bucket_object_lock_configuration`            |
| `policy`                               | `aws_s3_bucket_policy`                               |
| `replication_configuration`            | `aws_s3_bucket_replication_configuration`            |
| `request_payer`                        | `aws_s3_bucket_request_payment_configuration`        |
| `server_side_encryption_configuration` | `aws_s3_bucket_server_side_encryption_configuration` |
| `versioning`                           | `aws_s3_bucket_versioning`                           |
| `website`                              | `aws_s3_bucket_website_configuration`                |

Going back to the earlier example, given the following configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3Bucket(self, "example",
            bucket="yournamehere",
            cors_rule=[S3BucketCorsRule(
                allowed_headers=["*"],
                allowed_methods=["PUT", "POST"],
                allowed_origins=["https://s3-website-test.hashicorp.com"],
                expose_headers=["ETag"],
                max_age_seconds=3000
            )
            ]
        )
```

Practitioners can upgrade to v4.9.0 and then introduce the standalone `aws_s3_bucket_cors_configuration` resource, e.g.

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_cors_configuration import S3BucketCorsConfiguration
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere"
        )
        aws_s3_bucket_cors_configuration_example = S3BucketCorsConfiguration(self, "example_1",
            bucket=example.id,
            cors_rule=[S3BucketCorsConfigurationCorsRule(
                allowed_headers=["*"],
                allowed_methods=["PUT", "POST"],
                allowed_origins=["https://s3-website-test.hashicorp.com"],
                expose_headers=["ETag"],
                max_age_seconds=3000
            )
            ]
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_cors_configuration_example.override_logical_id("example")
```

Depending on the tools available to you, the above configuration can either be directly applied with Terraform or the standalone resource
can be imported into Terraform state. Please refer to each standalone resource's _Import_ documentation for the proper syntax.

Once the standalone resources are managed by Terraform, updates and removal can be performed as needed.

The following sections depict standalone resource adoption per individual parameter. Standalone resource adoption is not required to upgrade but is recommended to ensure drift is detected by Terraform.
The examples below are by no means exhaustive. The aim is to provide important concepts when migrating to a standalone resource whose parameters may not entirely align with the corresponding parameter in the `aws_s3_bucket` resource.

### Migrating to `aws_s3_bucket_accelerate_configuration`

Given this previous configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3Bucket(self, "example",
            acceleration_status="Enabled",
            bucket="yournamehere"
        )
```

Update the configuration to:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_accelerate_configuration import S3BucketAccelerateConfiguration
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere"
        )
        aws_s3_bucket_accelerate_configuration_example =
        S3BucketAccelerateConfiguration(self, "example_1",
            bucket=example.id,
            status="Enabled"
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_accelerate_configuration_example.override_logical_id("example")
```

### Migrating to `aws_s3_bucket_acl`

#### With `acl`

Given this previous configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3Bucket(self, "example",
            acl="private",
            bucket="yournamehere"
        )
```

Update the configuration to:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_acl import S3BucketAcl
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere"
        )
        aws_s3_bucket_acl_example = S3BucketAcl(self, "example_1",
            acl="private",
            bucket=example.id
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_acl_example.override_logical_id("example")
```

#### With `grant`

Given this previous configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import Token, TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3Bucket(self, "example",
            bucket="yournamehere",
            grant=[S3BucketGrant(
                id=Token.as_string(current_user.id),
                permissions=["FULL_CONTROL"],
                type="CanonicalUser"
            ), S3BucketGrant(
                permissions=["READ_ACP", "WRITE"],
                type="Group",
                uri="http://acs.amazonaws.com/groups/s3/LogDelivery"
            )
            ]
        )
```

Update the configuration to:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import Token, TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_acl import S3BucketAcl
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere"
        )
        aws_s3_bucket_acl_example = S3BucketAcl(self, "example_1",
            access_control_policy=S3BucketAclAccessControlPolicy(
                grant=[S3BucketAclAccessControlPolicyGrant(
                    grantee=S3BucketAclAccessControlPolicyGrantGrantee(
                        id=Token.as_string(current_user.id),
                        type="CanonicalUser"
                    ),
                    permission="FULL_CONTROL"
                ), S3BucketAclAccessControlPolicyGrant(
                    grantee=S3BucketAclAccessControlPolicyGrantGrantee(
                        type="Group",
                        uri="http://acs.amazonaws.com/groups/s3/LogDelivery"
                    ),
                    permission="READ_ACP"
                ), S3BucketAclAccessControlPolicyGrant(
                    grantee=S3BucketAclAccessControlPolicyGrantGrantee(
                        type="Group",
                        uri="http://acs.amazonaws.com/groups/s3/LogDelivery"
                    ),
                    permission="WRITE"
                )
                ],
                owner=S3BucketAclAccessControlPolicyOwner(
                    id=Token.as_string(current_user.id)
                )
            ),
            bucket=example.id
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_acl_example.override_logical_id("example")
```

### Migrating to `aws_s3_bucket_cors_configuration`

Given this previous configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3Bucket(self, "example",
            bucket="yournamehere",
            cors_rule=[S3BucketCorsRule(
                allowed_headers=["*"],
                allowed_methods=["PUT", "POST"],
                allowed_origins=["https://s3-website-test.hashicorp.com"],
                expose_headers=["ETag"],
                max_age_seconds=3000
            )
            ]
        )
```

Update the configuration to:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_cors_configuration import S3BucketCorsConfiguration
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere"
        )
        aws_s3_bucket_cors_configuration_example = S3BucketCorsConfiguration(self, "example_1",
            bucket=example.id,
            cors_rule=[S3BucketCorsConfigurationCorsRule(
                allowed_headers=["*"],
                allowed_methods=["PUT", "POST"],
                allowed_origins=["https://s3-website-test.hashicorp.com"],
                expose_headers=["ETag"],
                max_age_seconds=3000
            )
            ]
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_cors_configuration_example.override_logical_id("example")
```

### Migrating to `aws_s3_bucket_lifecycle_configuration`

~> **Note:** In version `3.x` of the provider, the `lifecycle_rule.id` argument was optional, while in version `4.x`, the `aws_s3_bucket_lifecycle_configuration.rule.id` argument required. Use the AWS CLI s3api [get-bucket-lifecycle-configuration](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/s3api/get-bucket-lifecycle-configuration.html) to get the source bucket's lifecycle configuration to determine the ID.

#### For Lifecycle Rules with no `prefix` previously configured

~> **Note:** When configuring the `rule.filter` configuration block in the new `aws_s3_bucket_lifecycle_configuration` resource, use the AWS CLI s3api [get-bucket-lifecycle-configuration](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/s3api/get-bucket-lifecycle-configuration.html)
to get the source bucket's lifecycle configuration and determine if the `Filter` is configured as `"Filter" : {}` or `"Filter" : { "Prefix": "" }`.
If AWS returns the former, configure `rule.filter` as `filter {}`. Otherwise, neither a `rule.filter` nor `rule.prefix` parameter should be configured as shown here:

Given this previous configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3Bucket(self, "example",
            bucket="yournamehere",
            lifecycle_rule=[S3BucketLifecycleRule(
                enabled=True,
                id="Keep previous version 30 days, then in Glacier another 60",
                noncurrent_version_expiration=S3BucketLifecycleRuleNoncurrentVersionExpiration(
                    days=90
                ),
                noncurrent_version_transition=[S3BucketLifecycleRuleNoncurrentVersionTransition(
                    days=30,
                    storage_class="GLACIER"
                )
                ]
            ), S3BucketLifecycleRule(
                abort_incomplete_multipart_upload_days=7,
                enabled=True,
                id="Delete old incomplete multi-part uploads"
            )
            ]
        )
```

Update the configuration to:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_lifecycle_configuration import S3BucketLifecycleConfiguration
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere"
        )
        aws_s3_bucket_lifecycle_configuration_example =
        S3BucketLifecycleConfiguration(self, "example_1",
            bucket=example.id,
            rule=[S3BucketLifecycleConfigurationRule(
                id="Keep previous version 30 days, then in Glacier another 60",
                noncurrent_version_expiration=S3BucketLifecycleConfigurationRuleNoncurrentVersionExpiration(
                    noncurrent_days=90
                ),
                noncurrent_version_transition=[S3BucketLifecycleConfigurationRuleNoncurrentVersionTransition(
                    noncurrent_days=30,
                    storage_class="GLACIER"
                )
                ],
                status="Enabled"
            ), S3BucketLifecycleConfigurationRule(
                abort_incomplete_multipart_upload=S3BucketLifecycleConfigurationRuleAbortIncompleteMultipartUpload(
                    days_after_initiation=7
                ),
                id="Delete old incomplete multi-part uploads",
                status="Enabled"
            )
            ]
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_lifecycle_configuration_example.override_logical_id("example")
```

#### For Lifecycle Rules with `prefix` previously configured as an empty string

Given this previous configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3Bucket(self, "example",
            bucket="yournamehere",
            lifecycle_rule=[S3BucketLifecycleRule(
                enabled=True,
                id="log-expiration",
                prefix="",
                transition=[S3BucketLifecycleRuleTransition(
                    days=30,
                    storage_class="STANDARD_IA"
                ), S3BucketLifecycleRuleTransition(
                    days=180,
                    storage_class="GLACIER"
                )
                ]
            )
            ]
        )
```

Update the configuration to:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_lifecycle_configuration import S3BucketLifecycleConfiguration
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere"
        )
        aws_s3_bucket_lifecycle_configuration_example =
        S3BucketLifecycleConfiguration(self, "example_1",
            bucket=example.id,
            rule=[S3BucketLifecycleConfigurationRule(
                id="log-expiration",
                status="Enabled",
                transition=[S3BucketLifecycleConfigurationRuleTransition(
                    days=30,
                    storage_class="STANDARD_IA"
                ), S3BucketLifecycleConfigurationRuleTransition(
                    days=180,
                    storage_class="GLACIER"
                )
                ]
            )
            ]
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_lifecycle_configuration_example.override_logical_id("example")
```

#### For Lifecycle Rules with `prefix`

Given this previous configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3Bucket(self, "example",
            bucket="yournamehere",
            lifecycle_rule=[S3BucketLifecycleRule(
                enabled=True,
                id="log-expiration",
                prefix="foobar",
                transition=[S3BucketLifecycleRuleTransition(
                    days=30,
                    storage_class="STANDARD_IA"
                ), S3BucketLifecycleRuleTransition(
                    days=180,
                    storage_class="GLACIER"
                )
                ]
            )
            ]
        )
```

Update the configuration to:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_lifecycle_configuration import S3BucketLifecycleConfiguration
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere"
        )
        aws_s3_bucket_lifecycle_configuration_example =
        S3BucketLifecycleConfiguration(self, "example_1",
            bucket=example.id,
            rule=[S3BucketLifecycleConfigurationRule(
                filter=S3BucketLifecycleConfigurationRuleFilter(
                    prefix="foobar"
                ),
                id="log-expiration",
                status="Enabled",
                transition=[S3BucketLifecycleConfigurationRuleTransition(
                    days=30,
                    storage_class="STANDARD_IA"
                ), S3BucketLifecycleConfigurationRuleTransition(
                    days=180,
                    storage_class="GLACIER"
                )
                ]
            )
            ]
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_lifecycle_configuration_example.override_logical_id("example")
```

#### For Lifecycle Rules with `prefix` and `tags`

Given this previous configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3Bucket(self, "example",
            bucket="yournamehere",
            lifecycle_rule=[S3BucketLifecycleRule(
                enabled=True,
                expiration=S3BucketLifecycleRuleExpiration(
                    days=90
                ),
                id="log",
                prefix="log/",
                tags={
                    "autoclean": "true",
                    "rule": "log"
                },
                transition=[S3BucketLifecycleRuleTransition(
                    days=30,
                    storage_class="STANDARD_IA"
                ), S3BucketLifecycleRuleTransition(
                    days=60,
                    storage_class="GLACIER"
                )
                ]
            ), S3BucketLifecycleRule(
                enabled=True,
                expiration=S3BucketLifecycleRuleExpiration(
                    date="2022-12-31"
                ),
                id="tmp",
                prefix="tmp/"
            )
            ]
        )
```

Update the configuration to:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_lifecycle_configuration import S3BucketLifecycleConfiguration
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere"
        )
        aws_s3_bucket_lifecycle_configuration_example =
        S3BucketLifecycleConfiguration(self, "example_1",
            bucket=example.id,
            rule=[S3BucketLifecycleConfigurationRule(
                expiration=S3BucketLifecycleConfigurationRuleExpiration(
                    days=90
                ),
                filter=S3BucketLifecycleConfigurationRuleFilter(
                    and=S3BucketLifecycleConfigurationRuleFilterAnd(
                        prefix="log/",
                        tags={
                            "autoclean": "true",
                            "rule": "log"
                        }
                    )
                ),
                id="log",
                status="Enabled",
                transition=[S3BucketLifecycleConfigurationRuleTransition(
                    days=30,
                    storage_class="STANDARD_IA"
                ), S3BucketLifecycleConfigurationRuleTransition(
                    days=60,
                    storage_class="GLACIER"
                )
                ]
            ), S3BucketLifecycleConfigurationRule(
                expiration=S3BucketLifecycleConfigurationRuleExpiration(
                    date="2022-12-31T00:00:00Z"
                ),
                filter=S3BucketLifecycleConfigurationRuleFilter(
                    prefix="tmp/"
                ),
                id="tmp",
                status="Enabled"
            )
            ]
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_lifecycle_configuration_example.override_logical_id("example")
```

### Migrating to `aws_s3_bucket_logging`

Given this previous configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        log_bucket = S3Bucket(self, "log_bucket",
            bucket="example-log-bucket"
        )
        S3Bucket(self, "example",
            bucket="yournamehere",
            logging=S3BucketLogging(
                target_bucket=log_bucket.id,
                target_prefix="log/"
            )
        )
```

Update the configuration to:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_logging import S3BucketLoggingA
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere"
        )
        log_bucket = S3Bucket(self, "log_bucket",
            bucket="example-log-bucket"
        )
        aws_s3_bucket_logging_example = S3BucketLoggingA(self, "example_2",
            bucket=example.id,
            target_bucket=log_bucket.id,
            target_prefix="log/"
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_logging_example.override_logical_id("example")
```

### Migrating to `aws_s3_bucket_object_lock_configuration`

Given this previous configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3Bucket(self, "example",
            bucket="yournamehere",
            object_lock_configuration=S3BucketObjectLockConfiguration(
                object_lock_enabled="Enabled",
                rule=S3BucketObjectLockConfigurationRule(
                    default_retention=S3BucketObjectLockConfigurationRuleDefaultRetention(
                        days=3,
                        mode="COMPLIANCE"
                    )
                )
            )
        )
```

Update the configuration to:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_object_lock_configuration import S3BucketObjectLockConfigurationA
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere",
            object_lock_enabled=True
        )
        aws_s3_bucket_object_lock_configuration_example =
        S3BucketObjectLockConfigurationA(self, "example_1",
            bucket=example.id,
            rule=S3BucketObjectLockConfigurationRuleA(
                default_retention=S3BucketObjectLockConfigurationRuleDefaultRetentionA(
                    days=3,
                    mode="COMPLIANCE"
                )
            )
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_object_lock_configuration_example.override_logical_id("example")
```

### Migrating to `aws_s3_bucket_policy`

Given this previous configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3Bucket(self, "example",
            bucket="yournamehere",
            policy="{\n  \"Id\": \"Policy1446577137248\",\n  \"Statement\": [\n    {\n      \"Action\": \"s3:PutObject\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"${" + current.arn + "}\"\n      },\n      \"Resource\": \"arn:${" + data_aws_partition_current.partition + "}:s3:::yournamehere/*\",\n      \"Sid\": \"Stmt1446575236270\"\n    }\n  ],\n  \"Version\": \"2012-10-17\"\n}\n\n"
        )
```

Update the configuration to:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_policy import S3BucketPolicy
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere"
        )
        aws_s3_bucket_policy_example = S3BucketPolicy(self, "example_1",
            bucket=example.id,
            policy="{\n  \"Id\": \"Policy1446577137248\",\n  \"Statement\": [\n    {\n      \"Action\": \"s3:PutObject\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"${" + current.arn + "}\"\n      },\n      \"Resource\": \"${" + example.arn + "}/*\",\n      \"Sid\": \"Stmt1446575236270\"\n    }\n  ],\n  \"Version\": \"2012-10-17\"\n}\n\n"
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_policy_example.override_logical_id("example")
```

### Migrating to `aws_s3_bucket_replication_configuration`

Given this previous configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3Bucket(self, "example",
            bucket="yournamehere",
            provider=central,
            replication_configuration=S3BucketReplicationConfiguration(
                role=replication.arn,
                rules=[S3BucketReplicationConfigurationRules(
                    destination=S3BucketReplicationConfigurationRulesDestination(
                        bucket=destination.arn,
                        metrics=S3BucketReplicationConfigurationRulesDestinationMetrics(
                            minutes=15,
                            status="Enabled"
                        ),
                        replication_time=S3BucketReplicationConfigurationRulesDestinationReplicationTime(
                            minutes=15,
                            status="Enabled"
                        ),
                        storage_class="STANDARD"
                    ),
                    filter=S3BucketReplicationConfigurationRulesFilter(
                        tags={}
                    ),
                    id="foobar",
                    status="Enabled"
                )
                ]
            )
        )
```

Update the configuration to:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_replication_configuration import S3BucketReplicationConfigurationA
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3Bucket(self, "example",
            bucket="yournamehere",
            provider=central
        )
        aws_s3_bucket_replication_configuration_example =
        S3BucketReplicationConfigurationA(self, "example_1",
            bucket=source.id,
            role=replication.arn,
            rule=[S3BucketReplicationConfigurationRule(
                delete_marker_replication=S3BucketReplicationConfigurationRuleDeleteMarkerReplication(
                    status="Enabled"
                ),
                destination=S3BucketReplicationConfigurationRuleDestination(
                    bucket=destination.arn,
                    metrics=S3BucketReplicationConfigurationRuleDestinationMetrics(
                        event_threshold=S3BucketReplicationConfigurationRuleDestinationMetricsEventThreshold(
                            minutes=15
                        ),
                        status="Enabled"
                    ),
                    replication_time=S3BucketReplicationConfigurationRuleDestinationReplicationTime(
                        status="Enabled",
                        time=S3BucketReplicationConfigurationRuleDestinationReplicationTimeTime(
                            minutes=15
                        )
                    ),
                    storage_class="STANDARD"
                ),
                filter=S3BucketReplicationConfigurationRuleFilter(),
                id="foobar",
                status="Enabled"
            )
            ]
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_replication_configuration_example.override_logical_id("example")
```

### Migrating to `aws_s3_bucket_request_payment_configuration`

Given this previous configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3Bucket(self, "example",
            bucket="yournamehere",
            request_payer="Requester"
        )
```

Update the configuration to:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_request_payment_configuration import S3BucketRequestPaymentConfiguration
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere"
        )
        aws_s3_bucket_request_payment_configuration_example =
        S3BucketRequestPaymentConfiguration(self, "example_1",
            bucket=example.id,
            payer="Requester"
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_request_payment_configuration_example.override_logical_id("example")
```

### Migrating to `aws_s3_bucket_server_side_encryption_configuration`

Given this previous configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3Bucket(self, "example",
            bucket="yournamehere",
            server_side_encryption_configuration=S3BucketServerSideEncryptionConfiguration(
                rule=S3BucketServerSideEncryptionConfigurationRule(
                    apply_server_side_encryption_by_default=S3BucketServerSideEncryptionConfigurationRuleApplyServerSideEncryptionByDefault(
                        kms_master_key_id=mykey.arn,
                        sse_algorithm="aws:kms"
                    )
                )
            )
        )
```

Update the configuration to:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_server_side_encryption_configuration import S3BucketServerSideEncryptionConfigurationA
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere"
        )
        aws_s3_bucket_server_side_encryption_configuration_example =
        S3BucketServerSideEncryptionConfigurationA(self, "example_1",
            bucket=example.id,
            rule=[S3BucketServerSideEncryptionConfigurationRuleA(
                apply_server_side_encryption_by_default=S3BucketServerSideEncryptionConfigurationRuleApplyServerSideEncryptionByDefaultA(
                    kms_master_key_id=mykey.arn,
                    sse_algorithm="aws:kms"
                )
            )
            ]
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_server_side_encryption_configuration_example.override_logical_id("example")
```

### Migrating to `aws_s3_bucket_versioning`

~> **NOTE:** As `aws_s3_bucket_versioning` is a separate resource, any S3 objects for which versioning is important (_e.g._, a truststore for mutual TLS authentication) must implicitly or explicitly depend on the `aws_s3_bucket_versioning` resource. Otherwise, the S3 objects may be created before versioning has been set. [See below](#ensure-objects-depend-on-versioning) for an example. Also note that AWS recommends waiting 15 minutes after enabling versioning on a bucket before putting or deleting objects in/from the bucket.

#### Buckets With Versioning Enabled

Given this previous configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3Bucket(self, "example",
            bucket="yournamehere",
            versioning=S3BucketVersioning(
                enabled=True
            )
        )
```

Update the configuration to:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_versioning import S3BucketVersioningA
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere"
        )
        aws_s3_bucket_versioning_example = S3BucketVersioningA(self, "example_1",
            bucket=example.id,
            versioning_configuration=S3BucketVersioningVersioningConfiguration(
                status="Enabled"
            )
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_versioning_example.override_logical_id("example")
```

#### Buckets With Versioning Disabled or Suspended

Depending on the version of the Terraform AWS Provider you are migrating from, the interpretation of `versioning.enabled = false`
in your `aws_s3_bucket` resource will differ and thus the migration to the `aws_s3_bucket_versioning` resource will also differ as follows.

If you are migrating from the Terraform AWS Provider `v3.70.0` or later:

* For new S3 buckets, `enabled = false` is synonymous to `Disabled`.
* For existing S3 buckets, `enabled = false` is synonymous to `Suspended`.

If you are migrating from an earlier version of the Terraform AWS Provider:

* For both new and existing S3 buckets, `enabled = false` is synonymous to `Suspended`.

Given this previous configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3Bucket(self, "example",
            bucket="yournamehere",
            versioning=S3BucketVersioning(
                enabled=False
            )
        )
```

Update the configuration to one of the following:

* If migrating from Terraform AWS Provider `v3.70.0` or later and bucket versioning was never enabled:

  ```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_versioning import S3BucketVersioningA
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere"
        )
        aws_s3_bucket_versioning_example = S3BucketVersioningA(self, "example_1",
            bucket=example.id,
            versioning_configuration=S3BucketVersioningVersioningConfiguration(
                status="Disabled"
            )
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_versioning_example.override_logical_id("example")
```

* If migrating from Terraform AWS Provider `v3.70.0` or later and bucket versioning was enabled at one point:

  ```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_versioning import S3BucketVersioningA
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere"
        )
        aws_s3_bucket_versioning_example = S3BucketVersioningA(self, "example_1",
            bucket=example.id,
            versioning_configuration=S3BucketVersioningVersioningConfiguration(
                status="Suspended"
            )
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_versioning_example.override_logical_id("example")
```

* If migrating from an earlier version of Terraform AWS Provider:

  ```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_versioning import S3BucketVersioningA
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere"
        )
        aws_s3_bucket_versioning_example = S3BucketVersioningA(self, "example_1",
            bucket=example.id,
            versioning_configuration=S3BucketVersioningVersioningConfiguration(
                status="Suspended"
            )
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_versioning_example.override_logical_id("example")
```

#### Ensure Objects Depend on Versioning

When you create an object whose `version_id` you need and an `aws_s3_bucket_versioning` resource in the same configuration, you are more likely to have success by ensuring the `s3_object` depends either implicitly (see below) or explicitly (i.e., using `depends_on = [aws_s3_bucket_versioning.example]`) on the `aws_s3_bucket_versioning` resource.

~> **NOTE:** For critical and/or production S3 objects, do not create a bucket, enable versioning, and create an object in the bucket within the same configuration. Doing so will not allow the AWS-recommended 15 minutes between enabling versioning and writing to the bucket.

This example shows the `aws_s3_object.example` depending implicitly on the versioning resource through the reference to `aws_s3_bucket_versioning.example.id` to define `bucket`:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import Token, TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_versioning import S3BucketVersioningA
from imports.aws.s3_object import S3Object
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yotto"
        )
        aws_s3_bucket_versioning_example = S3BucketVersioningA(self, "example_1",
            bucket=example.id,
            versioning_configuration=S3BucketVersioningVersioningConfiguration(
                status="Enabled"
            )
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_versioning_example.override_logical_id("example")
        aws_s3_object_example = S3Object(self, "example_2",
            bucket=Token.as_string(aws_s3_bucket_versioning_example.id),
            key="droeloe",
            source="example.txt"
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_object_example.override_logical_id("example")
```

### Migrating to `aws_s3_bucket_website_configuration`

Given this previous configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3Bucket(self, "example",
            bucket="yournamehere",
            website=S3BucketWebsite(
                error_document="error.html",
                index_document="index.html"
            )
        )
```

Update the configuration to:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_website_configuration import S3BucketWebsiteConfiguration
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere"
        )
        aws_s3_bucket_website_configuration_example =
        S3BucketWebsiteConfiguration(self, "example_1",
            bucket=example.id,
            error_document=S3BucketWebsiteConfigurationErrorDocument(
                key="error.html"
            ),
            index_document=S3BucketWebsiteConfigurationIndexDocument(
                suffix="index.html"
            )
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_website_configuration_example.override_logical_id("example")
```

Given this previous configuration that uses the `aws_s3_bucket` parameter `website_domain` with `aws_route53_record`:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.route53_record import Route53Record
from imports.aws.route53_zone import Route53Zone
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        main = Route53Zone(self, "main",
            name="domain.test"
        )
        website = S3Bucket(self, "website",
            website=S3BucketWebsite(
                error_document="error.html",
                index_document="index.html"
            )
        )
        Route53Record(self, "alias",
            alias=Route53RecordAlias(
                evaluate_target_health=True,
                name=website.website_domain,
                zone_id=website.hosted_zone_id
            ),
            name="www",
            type="A",
            zone_id=main.zone_id
        )
```

Update the configuration to use the `aws_s3_bucket_website_configuration` resource and its `website_domain` parameter:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.route53_record import Route53Record
from imports.aws.route53_zone import Route53Zone
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_website_configuration import S3BucketWebsiteConfiguration
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        main = Route53Zone(self, "main",
            name="domain.test"
        )
        website = S3Bucket(self, "website")
        example = S3BucketWebsiteConfiguration(self, "example",
            bucket=website.id,
            index_document=S3BucketWebsiteConfigurationIndexDocument(
                suffix="index.html"
            )
        )
        Route53Record(self, "alias",
            alias=Route53RecordAlias(
                evaluate_target_health=True,
                name=example.website_domain,
                zone_id=website.hosted_zone_id
            ),
            name="www",
            type="A",
            zone_id=main.zone_id
        )
```

## S3 Bucket Refactor

~> **NOTE:** This only applies to v4.0.0 through v4.8.0 of the AWS Provider, which introduce significant breaking
changes to the `aws_s3_bucket` resource. We recommend upgrading to v4.9.0 of the AWS Provider instead. See the section above, [Changes to S3 Bucket Drift Detection](#changes-to-s3-bucket-drift-detection), for additional upgrade considerations.

To help distribute the management of S3 bucket settings via independent resources, various arguments and attributes in the `aws_s3_bucket` resource have become **read-only**.

Configurations dependent on these arguments should be updated to use the corresponding `aws_s3_bucket_*` resource in order to prevent Terraform from reporting unconfigurable attribute errors for read-only arguments. Once updated, it is recommended to import new `aws_s3_bucket_*` resources into Terraform state.

In the event practitioners do not anticipate future modifications to the S3 bucket settings associated with these read-only arguments or drift detection is not needed, these read-only arguments should be removed from `aws_s3_bucket` resource configurations in order to prevent Terraform from reporting unconfigurable attribute errors; the states of these arguments will be preserved but are subject to change with modifications made outside Terraform.

~> **NOTE:** Each of the new `aws_s3_bucket_*` resources relies on S3 API calls that utilize a `PUT` action in order to modify the target S3 bucket. These calls follow standard HTTP methods for REST APIs, and therefore **should** handle situations where the target configuration already exists. While it is not strictly necessary to import new `aws_s3_bucket_*` resources where the updated configuration matches the configuration used in previous versions of the AWS provider, skipping this step will lead to a diff in the first plan after a configuration change indicating that any new `aws_s3_bucket_*` resources will be created, making it more difficult to determine whether the appropriate actions will be taken.

### `acceleration_status` Argument

Switch your Terraform configuration to the [`aws_s3_bucket_accelerate_configuration` resource](/docs/providers/aws/r/s3_bucket_accelerate_configuration.html) instead.

For example, given this previous configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3Bucket(self, "example",
            acceleration_status="Enabled",
            bucket="yournamehere"
        )
```

You will get the following error after upgrading:

```
 Error: Value for unconfigurable attribute

   with aws_s3_bucket.example,
   on main.tf line 1, in resource "aws_s3_bucket" "example":
    1: resource "aws_s3_bucket" "example" {

 Can't configure a value for "acceleration_status": its value will be decided automatically based on the result of applying this configuration.
```

Since `acceleration_status` is now read only, update your configuration to use the `aws_s3_bucket_accelerate_configuration`
resource and remove `acceleration_status` in the `aws_s3_bucket` resource:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_accelerate_configuration import S3BucketAccelerateConfiguration
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere"
        )
        aws_s3_bucket_accelerate_configuration_example =
        S3BucketAccelerateConfiguration(self, "example_1",
            bucket=example.id,
            status="Enabled"
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_accelerate_configuration_example.override_logical_id("example")
```

Run `terraform import` on each new resource, _e.g._,

```console
$ terraform import aws_s3_bucket_accelerate_configuration.example yournamehere
aws_s3_bucket_accelerate_configuration.example: Importing from ID "yournamehere"...
aws_s3_bucket_accelerate_configuration.example: Import prepared!
  Prepared aws_s3_bucket_accelerate_configuration for import
aws_s3_bucket_accelerate_configuration.example: Refreshing state... [id=yournamehere]

Import successful!

The resources that were imported are shown above. These resources are now in
your Terraform state and will henceforth be managed by Terraform.
```

### `acl` Argument

Switch your Terraform configuration to the [`aws_s3_bucket_acl` resource](/docs/providers/aws/r/s3_bucket_acl.html) instead.

For example, given this previous configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3Bucket(self, "example",
            acl="private",
            bucket="yournamehere"
        )
```

You will get the following error after upgrading:

```
 Error: Value for unconfigurable attribute

   with aws_s3_bucket.example,
   on main.tf line 1, in resource "aws_s3_bucket" "example":
    1: resource "aws_s3_bucket" "example" {

 Can't configure a value for "acl": its value will be decided automatically based on the result of applying this configuration.
```

Since `acl` is now read only, update your configuration to use the `aws_s3_bucket_acl`
resource and remove the `acl` argument in the `aws_s3_bucket` resource:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_acl import S3BucketAcl
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere"
        )
        aws_s3_bucket_acl_example = S3BucketAcl(self, "example_1",
            acl="private",
            bucket=example.id
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_acl_example.override_logical_id("example")
```

~> **NOTE:** When importing into `aws_s3_bucket_acl`, make sure you use the S3 bucket name (_e.g._, `yournamehere` in the example above) as part of the ID, and _not_ the Terraform bucket configuration name (_e.g._, `example` in the example above).

Run `terraform import` on each new resource, _e.g._,

```console
$ terraform import aws_s3_bucket_acl.example yournamehere,private
aws_s3_bucket_acl.example: Importing from ID "yournamehere,private"...
aws_s3_bucket_acl.example: Import prepared!
  Prepared aws_s3_bucket_acl for import
aws_s3_bucket_acl.example: Refreshing state... [id=yournamehere,private]

Import successful!

The resources that were imported are shown above. These resources are now in
your Terraform state and will henceforth be managed by Terraform.
```

### `cors_rule` Argument

Switch your Terraform configuration to the [`aws_s3_bucket_cors_configuration` resource](/docs/providers/aws/r/s3_bucket_cors_configuration.html) instead.

For example, given this previous configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3Bucket(self, "example",
            bucket="yournamehere",
            cors_rule=[S3BucketCorsRule(
                allowed_headers=["*"],
                allowed_methods=["PUT", "POST"],
                allowed_origins=["https://s3-website-test.hashicorp.com"],
                expose_headers=["ETag"],
                max_age_seconds=3000
            )
            ]
        )
```

You will get the following error after upgrading:

```
 Error: Value for unconfigurable attribute

   with aws_s3_bucket.example,
   on main.tf line 1, in resource "aws_s3_bucket" "example":
    1: resource "aws_s3_bucket" "example" {

 Can't configure a value for "cors_rule": its value will be decided automatically based on the result of applying this configuration.
```

Since `cors_rule` is now read only, update your configuration to use the `aws_s3_bucket_cors_configuration`
resource and remove `cors_rule` and its nested arguments in the `aws_s3_bucket` resource:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_cors_configuration import S3BucketCorsConfiguration
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere"
        )
        aws_s3_bucket_cors_configuration_example = S3BucketCorsConfiguration(self, "example_1",
            bucket=example.id,
            cors_rule=[S3BucketCorsConfigurationCorsRule(
                allowed_headers=["*"],
                allowed_methods=["PUT", "POST"],
                allowed_origins=["https://s3-website-test.hashicorp.com"],
                expose_headers=["ETag"],
                max_age_seconds=3000
            )
            ]
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_cors_configuration_example.override_logical_id("example")
```

Run `terraform import` on each new resource, _e.g._,

```console
$ terraform import aws_s3_bucket_cors_configuration.example yournamehere
aws_s3_bucket_cors_configuration.example: Importing from ID "yournamehere"...
aws_s3_bucket_cors_configuration.example: Import prepared!
  Prepared aws_s3_bucket_cors_configuration for import
aws_s3_bucket_cors_configuration.example: Refreshing state... [id=yournamehere]

Import successful!

The resources that were imported are shown above. These resources are now in
your Terraform state and will henceforth be managed by Terraform.
```

### `grant` Argument

Switch your Terraform configuration to the [`aws_s3_bucket_acl` resource](/docs/providers/aws/r/s3_bucket_acl.html) instead.

For example, given this previous configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import Token, TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3Bucket(self, "example",
            bucket="yournamehere",
            grant=[S3BucketGrant(
                id=Token.as_string(current_user.id),
                permissions=["FULL_CONTROL"],
                type="CanonicalUser"
            ), S3BucketGrant(
                permissions=["READ_ACP", "WRITE"],
                type="Group",
                uri="http://acs.amazonaws.com/groups/s3/LogDelivery"
            )
            ]
        )
```

You will get the following error after upgrading:

```
 Error: Value for unconfigurable attribute

   with aws_s3_bucket.example,
   on main.tf line 1, in resource "aws_s3_bucket" "example":
    1: resource "aws_s3_bucket" "example" {

 Can't configure a value for "grant": its value will be decided automatically based on the result of applying this configuration.
```

Since `grant` is now read only, update your configuration to use the `aws_s3_bucket_acl`
resource and remove `grant` in the `aws_s3_bucket` resource:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import Token, TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_acl import S3BucketAcl
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere"
        )
        aws_s3_bucket_acl_example = S3BucketAcl(self, "example_1",
            access_control_policy=S3BucketAclAccessControlPolicy(
                grant=[S3BucketAclAccessControlPolicyGrant(
                    grantee=S3BucketAclAccessControlPolicyGrantGrantee(
                        id=Token.as_string(current_user.id),
                        type="CanonicalUser"
                    ),
                    permission="FULL_CONTROL"
                ), S3BucketAclAccessControlPolicyGrant(
                    grantee=S3BucketAclAccessControlPolicyGrantGrantee(
                        type="Group",
                        uri="http://acs.amazonaws.com/groups/s3/LogDelivery"
                    ),
                    permission="READ_ACP"
                ), S3BucketAclAccessControlPolicyGrant(
                    grantee=S3BucketAclAccessControlPolicyGrantGrantee(
                        type="Group",
                        uri="http://acs.amazonaws.com/groups/s3/LogDelivery"
                    ),
                    permission="WRITE"
                )
                ],
                owner=S3BucketAclAccessControlPolicyOwner(
                    id=Token.as_string(current_user.id)
                )
            ),
            bucket=example.id
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_acl_example.override_logical_id("example")
```

Run `terraform import` on each new resource, _e.g._,

```console
$ terraform import aws_s3_bucket_acl.example yournamehere
aws_s3_bucket_acl.example: Importing from ID "yournamehere"...
aws_s3_bucket_acl.example: Import prepared!
  Prepared aws_s3_bucket_acl for import
aws_s3_bucket_acl.example: Refreshing state... [id=yournamehere]

Import successful!

The resources that were imported are shown above. These resources are now in
your Terraform state and will henceforth be managed by Terraform.
```

### `lifecycle_rule` Argument

Switch your Terraform configuration to the [`aws_s3_bucket_lifecycle_configuration` resource](/docs/providers/aws/r/s3_bucket_lifecycle_configuration.html) instead.

#### For Lifecycle Rules with no `prefix` previously configured

For example, given this previous configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3Bucket(self, "example",
            bucket="yournamehere",
            lifecycle_rule=[S3BucketLifecycleRule(
                enabled=True,
                id="Keep previous version 30 days, then in Glacier another 60",
                noncurrent_version_expiration=S3BucketLifecycleRuleNoncurrentVersionExpiration(
                    days=90
                ),
                noncurrent_version_transition=[S3BucketLifecycleRuleNoncurrentVersionTransition(
                    days=30,
                    storage_class="GLACIER"
                )
                ]
            ), S3BucketLifecycleRule(
                abort_incomplete_multipart_upload_days=7,
                enabled=True,
                id="Delete old incomplete multi-part uploads"
            )
            ]
        )
```

You will receive the following error after upgrading:

```
 Error: Value for unconfigurable attribute

   with aws_s3_bucket.example,
   on main.tf line 1, in resource "aws_s3_bucket" "example":
    1: resource "aws_s3_bucket" "example" {

 Can't configure a value for "lifecycle_rule": its value will be decided automatically based on the result of applying this configuration.
```

Since the `lifecycle_rule` argument changed to read-only, update the configuration to use the `aws_s3_bucket_lifecycle_configuration`
resource and remove `lifecycle_rule` and its nested arguments in the `aws_s3_bucket` resource.

~> **Note:** When configuring the `rule.filter` configuration block in the new `aws_s3_bucket_lifecycle_configuration` resource, use the AWS CLI s3api [get-bucket-lifecycle-configuration](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/s3api/get-bucket-lifecycle-configuration.html)
to get the source bucket's lifecycle configuration and determine if the `Filter` is configured as `"Filter" : {}` or `"Filter" : { "Prefix": "" }`.
If AWS returns the former, configure `rule.filter` as `filter {}`. Otherwise, neither a `rule.filter` nor `rule.prefix` parameter should be configured as shown here:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_lifecycle_configuration import S3BucketLifecycleConfiguration
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere"
        )
        aws_s3_bucket_lifecycle_configuration_example =
        S3BucketLifecycleConfiguration(self, "example_1",
            bucket=example.id,
            rule=[S3BucketLifecycleConfigurationRule(
                id="Keep previous version 30 days, then in Glacier another 60",
                noncurrent_version_expiration=S3BucketLifecycleConfigurationRuleNoncurrentVersionExpiration(
                    noncurrent_days=90
                ),
                noncurrent_version_transition=[S3BucketLifecycleConfigurationRuleNoncurrentVersionTransition(
                    noncurrent_days=30,
                    storage_class="GLACIER"
                )
                ],
                status="Enabled"
            ), S3BucketLifecycleConfigurationRule(
                abort_incomplete_multipart_upload=S3BucketLifecycleConfigurationRuleAbortIncompleteMultipartUpload(
                    days_after_initiation=7
                ),
                id="Delete old incomplete multi-part uploads",
                status="Enabled"
            )
            ]
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_lifecycle_configuration_example.override_logical_id("example")
```

Run `terraform import` on each new resource, _e.g._,

```console
$ terraform import aws_s3_bucket_lifecycle_configuration.example yournamehere
aws_s3_bucket_lifecycle_configuration.example: Importing from ID "yournamehere"...
aws_s3_bucket_lifecycle_configuration.example: Import prepared!
  Prepared aws_s3_bucket_lifecycle_configuration for import
aws_s3_bucket_lifecycle_configuration.example: Refreshing state... [id=yournamehere]

Import successful!

The resources that were imported are shown above. These resources are now in
your Terraform state and will henceforth be managed by Terraform.
```

#### For Lifecycle Rules with `prefix` previously configured as an empty string

For example, given this configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3Bucket(self, "example",
            bucket="yournamehere",
            lifecycle_rule=[S3BucketLifecycleRule(
                enabled=True,
                id="log-expiration",
                prefix="",
                transition=[S3BucketLifecycleRuleTransition(
                    days=30,
                    storage_class="STANDARD_IA"
                ), S3BucketLifecycleRuleTransition(
                    days=180,
                    storage_class="GLACIER"
                )
                ]
            )
            ]
        )
```

You will receive the following error after upgrading:

```
 Error: Value for unconfigurable attribute

   with aws_s3_bucket.example,
   on main.tf line 1, in resource "aws_s3_bucket" "example":
    1: resource "aws_s3_bucket" "example" {

 Can't configure a value for "lifecycle_rule": its value will be decided automatically based on the result of applying this configuration.
```

Since the `lifecycle_rule` argument changed to read-only, update the configuration to use the `aws_s3_bucket_lifecycle_configuration`
resource and remove `lifecycle_rule` and its nested arguments in the `aws_s3_bucket` resource:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_lifecycle_configuration import S3BucketLifecycleConfiguration
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere"
        )
        aws_s3_bucket_lifecycle_configuration_example =
        S3BucketLifecycleConfiguration(self, "example_1",
            bucket=example.id,
            rule=[S3BucketLifecycleConfigurationRule(
                id="log-expiration",
                status="Enabled",
                transition=[S3BucketLifecycleConfigurationRuleTransition(
                    days=30,
                    storage_class="STANDARD_IA"
                ), S3BucketLifecycleConfigurationRuleTransition(
                    days=180,
                    storage_class="GLACIER"
                )
                ]
            )
            ]
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_lifecycle_configuration_example.override_logical_id("example")
```

Run `terraform import` on each new resource, _e.g._,

```console
$ terraform import aws_s3_bucket_lifecycle_configuration.example yournamehere
aws_s3_bucket_lifecycle_configuration.example: Importing from ID "yournamehere"...
aws_s3_bucket_lifecycle_configuration.example: Import prepared!
  Prepared aws_s3_bucket_lifecycle_configuration for import
aws_s3_bucket_lifecycle_configuration.example: Refreshing state... [id=yournamehere]

Import successful!

The resources that were imported are shown above. These resources are now in
your Terraform state and will henceforth be managed by Terraform.
```

#### For Lifecycle Rules with `prefix`

For example, given this configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3Bucket(self, "example",
            bucket="yournamehere",
            lifecycle_rule=[S3BucketLifecycleRule(
                enabled=True,
                id="log-expiration",
                prefix="foobar",
                transition=[S3BucketLifecycleRuleTransition(
                    days=30,
                    storage_class="STANDARD_IA"
                ), S3BucketLifecycleRuleTransition(
                    days=180,
                    storage_class="GLACIER"
                )
                ]
            )
            ]
        )
```

You will receive the following error after upgrading:

```
 Error: Value for unconfigurable attribute

   with aws_s3_bucket.example,
   on main.tf line 1, in resource "aws_s3_bucket" "example":
    1: resource "aws_s3_bucket" "example" {

 Can't configure a value for "lifecycle_rule": its value will be decided automatically based on the result of applying this configuration.
```

Since the `lifecycle_rule` argument changed to read-only, update the configuration to use the `aws_s3_bucket_lifecycle_configuration`
resource and remove `lifecycle_rule` and its nested arguments in the `aws_s3_bucket` resource:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_lifecycle_configuration import S3BucketLifecycleConfiguration
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere"
        )
        aws_s3_bucket_lifecycle_configuration_example =
        S3BucketLifecycleConfiguration(self, "example_1",
            bucket=example.id,
            rule=[S3BucketLifecycleConfigurationRule(
                filter=S3BucketLifecycleConfigurationRuleFilter(
                    prefix="foobar"
                ),
                id="log-expiration",
                status="Enabled",
                transition=[S3BucketLifecycleConfigurationRuleTransition(
                    days=30,
                    storage_class="STANDARD_IA"
                ), S3BucketLifecycleConfigurationRuleTransition(
                    days=180,
                    storage_class="GLACIER"
                )
                ]
            )
            ]
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_lifecycle_configuration_example.override_logical_id("example")
```

Run `terraform import` on each new resource, _e.g._,

```console
$ terraform import aws_s3_bucket_lifecycle_configuration.example yournamehere
aws_s3_bucket_lifecycle_configuration.example: Importing from ID "yournamehere"...
aws_s3_bucket_lifecycle_configuration.example: Import prepared!
  Prepared aws_s3_bucket_lifecycle_configuration for import
aws_s3_bucket_lifecycle_configuration.example: Refreshing state... [id=yournamehere]

Import successful!

The resources that were imported are shown above. These resources are now in
your Terraform state and will henceforth be managed by Terraform.
```

#### For Lifecycle Rules with `prefix` and `tags`

For example, given this previous configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3Bucket(self, "example",
            bucket="yournamehere",
            lifecycle_rule=[S3BucketLifecycleRule(
                enabled=True,
                expiration=S3BucketLifecycleRuleExpiration(
                    days=90
                ),
                id="log",
                prefix="log/",
                tags={
                    "autoclean": "true",
                    "rule": "log"
                },
                transition=[S3BucketLifecycleRuleTransition(
                    days=30,
                    storage_class="STANDARD_IA"
                ), S3BucketLifecycleRuleTransition(
                    days=60,
                    storage_class="GLACIER"
                )
                ]
            ), S3BucketLifecycleRule(
                enabled=True,
                expiration=S3BucketLifecycleRuleExpiration(
                    date="2022-12-31"
                ),
                id="tmp",
                prefix="tmp/"
            )
            ]
        )
```

You will get the following error after upgrading:

```
 Error: Value for unconfigurable attribute

   with aws_s3_bucket.example,
   on main.tf line 1, in resource "aws_s3_bucket" "example":
    1: resource "aws_s3_bucket" "example" {

 Can't configure a value for "lifecycle_rule": its value will be decided automatically based on the result of applying this configuration.
```

Since `lifecycle_rule` is now read only, update your configuration to use the `aws_s3_bucket_lifecycle_configuration`
resource and remove `lifecycle_rule` and its nested arguments in the `aws_s3_bucket` resource:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_lifecycle_configuration import S3BucketLifecycleConfiguration
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere"
        )
        aws_s3_bucket_lifecycle_configuration_example =
        S3BucketLifecycleConfiguration(self, "example_1",
            bucket=example.id,
            rule=[S3BucketLifecycleConfigurationRule(
                expiration=S3BucketLifecycleConfigurationRuleExpiration(
                    days=90
                ),
                filter=S3BucketLifecycleConfigurationRuleFilter(
                    and=S3BucketLifecycleConfigurationRuleFilterAnd(
                        prefix="log/",
                        tags={
                            "autoclean": "true",
                            "rule": "log"
                        }
                    )
                ),
                id="log",
                status="Enabled",
                transition=[S3BucketLifecycleConfigurationRuleTransition(
                    days=30,
                    storage_class="STANDARD_IA"
                ), S3BucketLifecycleConfigurationRuleTransition(
                    days=60,
                    storage_class="GLACIER"
                )
                ]
            ), S3BucketLifecycleConfigurationRule(
                expiration=S3BucketLifecycleConfigurationRuleExpiration(
                    date="2022-12-31T00:00:00Z"
                ),
                filter=S3BucketLifecycleConfigurationRuleFilter(
                    prefix="tmp/"
                ),
                id="tmp",
                status="Enabled"
            )
            ]
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_lifecycle_configuration_example.override_logical_id("example")
```

Run `terraform import` on each new resource, _e.g._,

```console
$ terraform import aws_s3_bucket_lifecycle_configuration.example yournamehere
aws_s3_bucket_lifecycle_configuration.example: Importing from ID "yournamehere"...
aws_s3_bucket_lifecycle_configuration.example: Import prepared!
  Prepared aws_s3_bucket_lifecycle_configuration for import
aws_s3_bucket_lifecycle_configuration.example: Refreshing state... [id=yournamehere]

Import successful!

The resources that were imported are shown above. These resources are now in
your Terraform state and will henceforth be managed by Terraform.
```

### `logging` Argument

Switch your Terraform configuration to the [`aws_s3_bucket_logging` resource](/docs/providers/aws/r/s3_bucket_logging.html) instead.

For example, given this previous configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        log_bucket = S3Bucket(self, "log_bucket",
            bucket="example-log-bucket"
        )
        S3Bucket(self, "example",
            bucket="yournamehere",
            logging=S3BucketLogging(
                target_bucket=log_bucket.id,
                target_prefix="log/"
            )
        )
```

You will get the following error after upgrading:

```
 Error: Value for unconfigurable attribute

   with aws_s3_bucket.example,
   on main.tf line 1, in resource "aws_s3_bucket" "example":
    1: resource "aws_s3_bucket" "example" {

 Can't configure a value for "logging": its value will be decided automatically based on the result of applying this configuration.
```

Since `logging` is now read only, update your configuration to use the `aws_s3_bucket_logging`
resource and remove `logging` and its nested arguments in the `aws_s3_bucket` resource:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_logging import S3BucketLoggingA
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere"
        )
        log_bucket = S3Bucket(self, "log_bucket",
            bucket="example-log-bucket"
        )
        aws_s3_bucket_logging_example = S3BucketLoggingA(self, "example_2",
            bucket=example.id,
            target_bucket=log_bucket.id,
            target_prefix="log/"
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_logging_example.override_logical_id("example")
```

Run `terraform import` on each new resource, _e.g._,

```console
$ terraform import aws_s3_bucket_logging.example yournamehere
aws_s3_bucket_logging.example: Importing from ID "yournamehere"...
aws_s3_bucket_logging.example: Import prepared!
  Prepared aws_s3_bucket_logging for import
aws_s3_bucket_logging.example: Refreshing state... [id=yournamehere]

Import successful!

The resources that were imported are shown above. These resources are now in
your Terraform state and will henceforth be managed by Terraform.
```

### `object_lock_configuration` `rule` Argument

Switch your Terraform configuration to the [`aws_s3_bucket_object_lock_configuration` resource](/docs/providers/aws/r/s3_bucket_object_lock_configuration.html) instead.

For example, given this previous configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3Bucket(self, "example",
            bucket="yournamehere",
            object_lock_configuration=S3BucketObjectLockConfiguration(
                object_lock_enabled="Enabled",
                rule=S3BucketObjectLockConfigurationRule(
                    default_retention=S3BucketObjectLockConfigurationRuleDefaultRetention(
                        days=3,
                        mode="COMPLIANCE"
                    )
                )
            )
        )
```

You will get the following error after upgrading:

```
 Error: Value for unconfigurable attribute

   with aws_s3_bucket.example,
   on main.tf line 1, in resource "aws_s3_bucket" "example":
    1: resource "aws_s3_bucket" "example" {

 Can't configure a value for "object_lock_configuration.0.rule": its value will be decided automatically based on the result of applying this configuration.
```

Since the `rule` argument of the `object_lock_configuration` configuration block changed to read-only, update your configuration to use the `aws_s3_bucket_object_lock_configuration`
resource and remove `rule` and its nested arguments in the `aws_s3_bucket` resource:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_object_lock_configuration import S3BucketObjectLockConfigurationA
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere",
            object_lock_enabled=True
        )
        aws_s3_bucket_object_lock_configuration_example =
        S3BucketObjectLockConfigurationA(self, "example_1",
            bucket=example.id,
            rule=S3BucketObjectLockConfigurationRuleA(
                default_retention=S3BucketObjectLockConfigurationRuleDefaultRetentionA(
                    days=3,
                    mode="COMPLIANCE"
                )
            )
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_object_lock_configuration_example.override_logical_id("example")
```

Run `terraform import` on each new resource, _e.g._,

```console
$ terraform import aws_s3_bucket_object_lock_configuration.example yournamehere
aws_s3_bucket_object_lock_configuration.example: Importing from ID "yournamehere"...
aws_s3_bucket_object_lock_configuration.example: Import prepared!
  Prepared aws_s3_bucket_object_lock_configuration for import
aws_s3_bucket_object_lock_configuration.example: Refreshing state... [id=yournamehere]

Import successful!

The resources that were imported are shown above. These resources are now in
your Terraform state and will henceforth be managed by Terraform.
```

### `policy` Argument

Switch your Terraform configuration to the [`aws_s3_bucket_policy` resource](/docs/providers/aws/r/s3_bucket_policy.html) instead.

For example, given this previous configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3Bucket(self, "example",
            bucket="yournamehere",
            policy="{\n  \"Id\": \"Policy1446577137248\",\n  \"Statement\": [\n    {\n      \"Action\": \"s3:PutObject\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"${" + current.arn + "}\"\n      },\n      \"Resource\": \"arn:${" + data_aws_partition_current.partition + "}:s3:::yournamehere/*\",\n      \"Sid\": \"Stmt1446575236270\"\n    }\n  ],\n  \"Version\": \"2012-10-17\"\n}\n\n"
        )
```

You will get the following error after upgrading:

```
 Error: Value for unconfigurable attribute

   with aws_s3_bucket.example,
   on main.tf line 1, in resource "aws_s3_bucket" "example":
    1: resource "aws_s3_bucket" "example" {

 Can't configure a value for "policy": its value will be decided automatically based on the result of applying this configuration.
```

Since `policy` is now read only, update your configuration to use the `aws_s3_bucket_policy`
resource and remove `policy` in the `aws_s3_bucket` resource:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_policy import S3BucketPolicy
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere"
        )
        aws_s3_bucket_policy_example = S3BucketPolicy(self, "example_1",
            bucket=example.id,
            policy="{\n  \"Id\": \"Policy1446577137248\",\n  \"Statement\": [\n    {\n      \"Action\": \"s3:PutObject\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"${" + current.arn + "}\"\n      },\n      \"Resource\": \"${" + example.arn + "}/*\",\n      \"Sid\": \"Stmt1446575236270\"\n    }\n  ],\n  \"Version\": \"2012-10-17\"\n}\n\n"
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_policy_example.override_logical_id("example")
```

Run `terraform import` on each new resource, _e.g._,

```console
$ terraform import aws_s3_bucket_policy.example yournamehere
aws_s3_bucket_policy.example: Importing from ID "yournamehere"...
aws_s3_bucket_policy.example: Import prepared!
  Prepared aws_s3_bucket_policy for import
aws_s3_bucket_policy.example: Refreshing state... [id=yournamehere]

Import successful!

The resources that were imported are shown above. These resources are now in
your Terraform state and will henceforth be managed by Terraform.
```

### `replication_configuration` Argument

Switch your Terraform configuration to the [`aws_s3_bucket_replication_configuration` resource](/docs/providers/aws/r/s3_bucket_replication_configuration.html) instead.

For example, given this previous configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3Bucket(self, "example",
            bucket="yournamehere",
            provider=central,
            replication_configuration=S3BucketReplicationConfiguration(
                role=replication.arn,
                rules=[S3BucketReplicationConfigurationRules(
                    destination=S3BucketReplicationConfigurationRulesDestination(
                        bucket=destination.arn,
                        metrics=S3BucketReplicationConfigurationRulesDestinationMetrics(
                            minutes=15,
                            status="Enabled"
                        ),
                        replication_time=S3BucketReplicationConfigurationRulesDestinationReplicationTime(
                            minutes=15,
                            status="Enabled"
                        ),
                        storage_class="STANDARD"
                    ),
                    filter=S3BucketReplicationConfigurationRulesFilter(
                        tags={}
                    ),
                    id="foobar",
                    status="Enabled"
                )
                ]
            )
        )
```

You will get the following error after upgrading:

```
 Error: Value for unconfigurable attribute

   with aws_s3_bucket.example,
   on main.tf line 1, in resource "aws_s3_bucket" "example":
    1: resource "aws_s3_bucket" "example" {

 Can't configure a value for "replication_configuration": its value will be decided automatically based on the result of applying this configuration.
```

Since `replication_configuration` is now read only, update your configuration to use the `aws_s3_bucket_replication_configuration`
resource and remove `replication_configuration` and its nested arguments in the `aws_s3_bucket` resource:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_replication_configuration import S3BucketReplicationConfigurationA
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere",
            provider=central
        )
        aws_s3_bucket_replication_configuration_example =
        S3BucketReplicationConfigurationA(self, "example_1",
            bucket=example.id,
            role=replication.arn,
            rule=[S3BucketReplicationConfigurationRule(
                delete_marker_replication=S3BucketReplicationConfigurationRuleDeleteMarkerReplication(
                    status="Enabled"
                ),
                destination=S3BucketReplicationConfigurationRuleDestination(
                    bucket=destination.arn,
                    metrics=S3BucketReplicationConfigurationRuleDestinationMetrics(
                        event_threshold=S3BucketReplicationConfigurationRuleDestinationMetricsEventThreshold(
                            minutes=15
                        ),
                        status="Enabled"
                    ),
                    replication_time=S3BucketReplicationConfigurationRuleDestinationReplicationTime(
                        status="Enabled",
                        time=S3BucketReplicationConfigurationRuleDestinationReplicationTimeTime(
                            minutes=15
                        )
                    ),
                    storage_class="STANDARD"
                ),
                filter=S3BucketReplicationConfigurationRuleFilter(),
                id="foobar",
                status="Enabled"
            )
            ]
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_replication_configuration_example.override_logical_id("example")
```

Run `terraform import` on each new resource, _e.g._,

```console
$ terraform import aws_s3_bucket_replication_configuration.example yournamehere
aws_s3_bucket_replication_configuration.example: Importing from ID "yournamehere"...
aws_s3_bucket_replication_configuration.example: Import prepared!
  Prepared aws_s3_bucket_replication_configuration for import
aws_s3_bucket_replication_configuration.example: Refreshing state... [id=yournamehere]

Import successful!

The resources that were imported are shown above. These resources are now in
your Terraform state and will henceforth be managed by Terraform.
```

### `request_payer` Argument

Switch your Terraform configuration to the [`aws_s3_bucket_request_payment_configuration` resource](/docs/providers/aws/r/s3_bucket_request_payment_configuration.html) instead.

For example, given this previous configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3Bucket(self, "example",
            bucket="yournamehere",
            request_payer="Requester"
        )
```

You will get the following error after upgrading:

```
 Error: Value for unconfigurable attribute

   with aws_s3_bucket.example,
   on main.tf line 1, in resource "aws_s3_bucket" "example":
    1: resource "aws_s3_bucket" "example" {

 Can't configure a value for "request_payer": its value will be decided automatically based on the result of applying this configuration.
```

Since `request_payer` is now read only, update your configuration to use the `aws_s3_bucket_request_payment_configuration`
resource and remove `request_payer` in the `aws_s3_bucket` resource:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_request_payment_configuration import S3BucketRequestPaymentConfiguration
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere"
        )
        aws_s3_bucket_request_payment_configuration_example =
        S3BucketRequestPaymentConfiguration(self, "example_1",
            bucket=example.id,
            payer="Requester"
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_request_payment_configuration_example.override_logical_id("example")
```

Run `terraform import` on each new resource, _e.g._,

```console
$ terraform import aws_s3_bucket_request_payment_configuration.example yournamehere
aws_s3_bucket_request_payment_configuration.example: Importing from ID "yournamehere"...
aws_s3_bucket_request_payment_configuration.example: Import prepared!
  Prepared aws_s3_bucket_request_payment_configuration for import
aws_s3_bucket_request_payment_configuration.example: Refreshing state... [id=yournamehere]

Import successful!

The resources that were imported are shown above. These resources are now in
your Terraform state and will henceforth be managed by Terraform.
```

### `server_side_encryption_configuration` Argument

Switch your Terraform configuration to the [`aws_s3_bucket_server_side_encryption_configuration` resource](/docs/providers/aws/r/s3_bucket_server_side_encryption_configuration.html) instead.

For example, given this previous configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3Bucket(self, "example",
            bucket="yournamehere",
            server_side_encryption_configuration=S3BucketServerSideEncryptionConfiguration(
                rule=S3BucketServerSideEncryptionConfigurationRule(
                    apply_server_side_encryption_by_default=S3BucketServerSideEncryptionConfigurationRuleApplyServerSideEncryptionByDefault(
                        kms_master_key_id=mykey.arn,
                        sse_algorithm="aws:kms"
                    )
                )
            )
        )
```

You will get the following error after upgrading:

```
 Error: Value for unconfigurable attribute

   with aws_s3_bucket.example,
   on main.tf line 1, in resource "aws_s3_bucket" "example":
    1: resource "aws_s3_bucket" "example" {

 Can't configure a value for "server_side_encryption_configuration": its value will be decided automatically based on the result of applying this configuration.
```

Since `server_side_encryption_configuration` is now read only, update your configuration to use the `aws_s3_bucket_server_side_encryption_configuration`
resource and remove `server_side_encryption_configuration` and its nested arguments in the `aws_s3_bucket` resource:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_server_side_encryption_configuration import S3BucketServerSideEncryptionConfigurationA
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere"
        )
        aws_s3_bucket_server_side_encryption_configuration_example =
        S3BucketServerSideEncryptionConfigurationA(self, "example_1",
            bucket=example.id,
            rule=[S3BucketServerSideEncryptionConfigurationRuleA(
                apply_server_side_encryption_by_default=S3BucketServerSideEncryptionConfigurationRuleApplyServerSideEncryptionByDefaultA(
                    kms_master_key_id=mykey.arn,
                    sse_algorithm="aws:kms"
                )
            )
            ]
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_server_side_encryption_configuration_example.override_logical_id("example")
```

Run `terraform import` on each new resource, _e.g._,

```console
$ terraform import aws_s3_bucket_server_side_encryption_configuration.example yournamehere
aws_s3_bucket_server_side_encryption_configuration.example: Importing from ID "yournamehere"...
aws_s3_bucket_server_side_encryption_configuration.example: Import prepared!
  Prepared aws_s3_bucket_server_side_encryption_configuration for import
aws_s3_bucket_server_side_encryption_configuration.example: Refreshing state... [id=yournamehere]

Import successful!

The resources that were imported are shown above. These resources are now in
your Terraform state and will henceforth be managed by Terraform.
```

### `versioning` Argument

Switch your Terraform configuration to the [`aws_s3_bucket_versioning` resource](/docs/providers/aws/r/s3_bucket_versioning.html) instead.

~> **NOTE:** As `aws_s3_bucket_versioning` is a separate resource, any S3 objects for which versioning is important (_e.g._, a truststore for mutual TLS authentication) must implicitly or explicitly depend on the `aws_s3_bucket_versioning` resource. Otherwise, the S3 objects may be created before versioning has been set. [See below](#ensure-objects-depend-on-versioning) for an example. Also note that AWS recommends waiting 15 minutes after enabling versioning on a bucket before putting or deleting objects in/from the bucket.

#### Buckets With Versioning Enabled

Given this previous configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3Bucket(self, "example",
            bucket="yournamehere",
            versioning=S3BucketVersioning(
                enabled=True
            )
        )
```

You will get the following error after upgrading:

```
 Error: Value for unconfigurable attribute

   with aws_s3_bucket.example,
   on main.tf line 1, in resource "aws_s3_bucket" "example":
    1: resource "aws_s3_bucket" "example" {

 Can't configure a value for "versioning": its value will be decided automatically based on the result of applying this configuration.
```

Since `versioning` is now read only, update your configuration to use the `aws_s3_bucket_versioning`
resource and remove `versioning` and its nested arguments in the `aws_s3_bucket` resource:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_versioning import S3BucketVersioningA
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere"
        )
        aws_s3_bucket_versioning_example = S3BucketVersioningA(self, "example_1",
            bucket=example.id,
            versioning_configuration=S3BucketVersioningVersioningConfiguration(
                status="Enabled"
            )
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_versioning_example.override_logical_id("example")
```

Run `terraform import` on each new resource, _e.g._,

```console
$ terraform import aws_s3_bucket_versioning.example yournamehere
aws_s3_bucket_versioning.example: Importing from ID "yournamehere"...
aws_s3_bucket_versioning.example: Import prepared!
  Prepared aws_s3_bucket_versioning for import
aws_s3_bucket_versioning.example: Refreshing state... [id=yournamehere]

Import successful!

The resources that were imported are shown above. These resources are now in
your Terraform state and will henceforth be managed by Terraform.
```

#### Buckets With Versioning Disabled or Suspended

Depending on the version of the Terraform AWS Provider you are migrating from, the interpretation of `versioning.enabled = false`
in your `aws_s3_bucket` resource will differ and thus the migration to the `aws_s3_bucket_versioning` resource will also differ as follows.

If you are migrating from the Terraform AWS Provider `v3.70.0` or later:

* For new S3 buckets, `enabled = false` is synonymous to `Disabled`.
* For existing S3 buckets, `enabled = false` is synonymous to `Suspended`.

If you are migrating from an earlier version of the Terraform AWS Provider:

* For both new and existing S3 buckets, `enabled = false` is synonymous to `Suspended`.

Given this previous configuration :

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3Bucket(self, "example",
            bucket="yournamehere",
            versioning=S3BucketVersioning(
                enabled=False
            )
        )
```

You will get the following error after upgrading:

```
 Error: Value for unconfigurable attribute

   with aws_s3_bucket.example,
   on main.tf line 1, in resource "aws_s3_bucket" "example":
    1: resource "aws_s3_bucket" "example" {

 Can't configure a value for "versioning": its value will be decided automatically based on the result of applying this configuration.
```

Since `versioning` is now read only, update your configuration to use the `aws_s3_bucket_versioning`
resource and remove `versioning` and its nested arguments in the `aws_s3_bucket` resource.

* If migrating from Terraform AWS Provider `v3.70.0` or later and bucket versioning was never enabled:

  ```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_versioning import S3BucketVersioningA
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere"
        )
        aws_s3_bucket_versioning_example = S3BucketVersioningA(self, "example_1",
            bucket=example.id,
            versioning_configuration=S3BucketVersioningVersioningConfiguration(
                status="Disabled"
            )
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_versioning_example.override_logical_id("example")
```

* If migrating from Terraform AWS Provider `v3.70.0` or later and bucket versioning was enabled at one point:

  ```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_versioning import S3BucketVersioningA
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere"
        )
        aws_s3_bucket_versioning_example = S3BucketVersioningA(self, "example_1",
            bucket=example.id,
            versioning_configuration=S3BucketVersioningVersioningConfiguration(
                status="Suspended"
            )
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_versioning_example.override_logical_id("example")
```

* If migrating from an earlier version of Terraform AWS Provider:

  ```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_versioning import S3BucketVersioningA
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere"
        )
        aws_s3_bucket_versioning_example = S3BucketVersioningA(self, "example_1",
            bucket=example.id,
            versioning_configuration=S3BucketVersioningVersioningConfiguration(
                status="Suspended"
            )
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_versioning_example.override_logical_id("example")
```

Run `terraform import` on each new resource, _e.g._,

```console
$ terraform import aws_s3_bucket_versioning.example yournamehere
aws_s3_bucket_versioning.example: Importing from ID "yournamehere"...
aws_s3_bucket_versioning.example: Import prepared!
  Prepared aws_s3_bucket_versioning for import
aws_s3_bucket_versioning.example: Refreshing state... [id=yournamehere]

Import successful!

The resources that were imported are shown above. These resources are now in
your Terraform state and will henceforth be managed by Terraform.
```

#### Ensure Objects Depend on Versioning

When you create an object whose `version_id` you need and an `aws_s3_bucket_versioning` resource in the same configuration, you are more likely to have success by ensuring the `s3_object` depends either implicitly (see below) or explicitly (i.e., using `depends_on = [aws_s3_bucket_versioning.example]`) on the `aws_s3_bucket_versioning` resource.

~> **NOTE:** For critical and/or production S3 objects, do not create a bucket, enable versioning, and create an object in the bucket within the same configuration. Doing so will not allow the AWS-recommended 15 minutes between enabling versioning and writing to the bucket.

This example shows the `aws_s3_object.example` depending implicitly on the versioning resource through the reference to `aws_s3_bucket_versioning.example.bucket` to define `bucket`:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import Token, TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_versioning import S3BucketVersioningA
from imports.aws.s3_object import S3Object
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yotto"
        )
        aws_s3_bucket_versioning_example = S3BucketVersioningA(self, "example_1",
            bucket=example.id,
            versioning_configuration=S3BucketVersioningVersioningConfiguration(
                status="Enabled"
            )
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_versioning_example.override_logical_id("example")
        aws_s3_object_example = S3Object(self, "example_2",
            bucket=Token.as_string(aws_s3_bucket_versioning_example.id),
            key="droeloe",
            source="example.txt"
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_object_example.override_logical_id("example")
```

### `website`, `website_domain`, and `website_endpoint` Arguments

Switch your Terraform configuration to the [`aws_s3_bucket_website_configuration` resource](/docs/providers/aws/r/s3_bucket_website_configuration.html) instead.

For example, given this previous configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3Bucket(self, "example",
            bucket="yournamehere",
            website=S3BucketWebsite(
                error_document="error.html",
                index_document="index.html"
            )
        )
```

You will get the following error after upgrading:

```
 Error: Value for unconfigurable attribute

   with aws_s3_bucket.example,
   on main.tf line 1, in resource "aws_s3_bucket" "example":
    1: resource "aws_s3_bucket" "example" {

 Can't configure a value for "website": its value will be decided automatically based on the result of applying this configuration.
```

Since `website` is now read only, update your configuration to use the `aws_s3_bucket_website_configuration`
resource and remove `website` and its nested arguments in the `aws_s3_bucket` resource:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_website_configuration import S3BucketWebsiteConfiguration
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="yournamehere"
        )
        aws_s3_bucket_website_configuration_example =
        S3BucketWebsiteConfiguration(self, "example_1",
            bucket=example.id,
            error_document=S3BucketWebsiteConfigurationErrorDocument(
                key="error.html"
            ),
            index_document=S3BucketWebsiteConfigurationIndexDocument(
                suffix="index.html"
            )
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_website_configuration_example.override_logical_id("example")
```

Run `terraform import` on each new resource, _e.g._,

```console
$ terraform import aws_s3_bucket_website_configuration.example yournamehere
aws_s3_bucket_website_configuration.example: Importing from ID "yournamehere"...
aws_s3_bucket_website_configuration.example: Import prepared!
  Prepared aws_s3_bucket_website_configuration for import
aws_s3_bucket_website_configuration.example: Refreshing state... [id=yournamehere]

Import successful!

The resources that were imported are shown above. These resources are now in
your Terraform state and will henceforth be managed by Terraform.
```

For example, if you use the `aws_s3_bucket` attribute `website_domain` with `aws_route53_record`, as shown below, you will need to update your configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.route53_record import Route53Record
from imports.aws.route53_zone import Route53Zone
from imports.aws.s3_bucket import S3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        main = Route53Zone(self, "main",
            name="domain.test"
        )
        website = S3Bucket(self, "website",
            website=S3BucketWebsite(
                error_document="error.html",
                index_document="index.html"
            )
        )
        Route53Record(self, "alias",
            alias=Route53RecordAlias(
                evaluate_target_health=True,
                name=website.website_domain,
                zone_id=website.hosted_zone_id
            ),
            name="www",
            type="A",
            zone_id=main.zone_id
        )
```

Instead, you will now use the `aws_s3_bucket_website_configuration` resource and its `website_domain` attribute:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.route53_record import Route53Record
from imports.aws.route53_zone import Route53Zone
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_website_configuration import S3BucketWebsiteConfiguration
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        main = Route53Zone(self, "main",
            name="domain.test"
        )
        website = S3Bucket(self, "website")
        example = S3BucketWebsiteConfiguration(self, "example",
            bucket=website.id,
            index_document=S3BucketWebsiteConfigurationIndexDocument(
                suffix="index.html"
            )
        )
        Route53Record(self, "alias",
            alias=Route53RecordAlias(
                evaluate_target_health=True,
                name=example.website_domain,
                zone_id=website.hosted_zone_id
            ),
            name="www",
            type="A",
            zone_id=main.zone_id
        )
```

## Full Resource Lifecycle of Default Resources

Default subnets and vpcs can now do full resource lifecycle operations such that resource
creation and deletion are now supported.

### Resource: aws_default_subnet

The `aws_default_subnet` resource behaves differently from normal resources in that if a default subnet exists in the specified Availability Zone, Terraform does not _create_ this resource, but instead "adopts" it into management.
If no default subnet exists, Terraform creates a new default subnet.
By default, `terraform destroy` does not delete the default subnet but does remove the resource from Terraform state.
Set the `force_destroy` argument to `true` to delete the default subnet.

For example, given this previous configuration with no existing default subnet:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.default_subnet import DefaultSubnet
from imports.aws.provider import AwsProvider
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name, *, availabilityZone):
        super().__init__(scope, name)
        AwsProvider(self, "aws",
            region="eu-west-2"
        )
        DefaultSubnet(self, "default",
            availability_zone=availability_zone
        )
```

The following error was thrown on `terraform apply`:

```
 Error: Default subnet not found.

   with aws_default_subnet.default,
   on main.tf line 5, in resource "aws_default_subnet" "default":
    5: resource "aws_default_subnet" "default" {}
```

Now after upgrading, the above configuration will apply successfully.

To delete the default subnet, the above configuration should be updated as follows:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.default_subnet import DefaultSubnet
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name, *, availabilityZone):
        super().__init__(scope, name)
        DefaultSubnet(self, "default",
            force_destroy=True,
            availability_zone=availability_zone
        )
```

### Resource: aws_default_vpc

The `aws_default_vpc` resource behaves differently from normal resources in that if a default VPC exists, Terraform does not _create_ this resource, but instead "adopts" it into management.
If no default VPC exists, Terraform creates a new default VPC, which leads to the implicit creation of [other resources](https://docs.aws.amazon.com/vpc/latest/userguide/default-vpc.html#default-vpc-components).
By default, `terraform destroy` does not delete the default VPC but does remove the resource from Terraform state.
Set the `force_destroy` argument to `true` to delete the default VPC.

For example, given this previous configuration with no existing default VPC:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.default_vpc import DefaultVpc
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        DefaultVpc(self, "default")
```

The following error was thrown on `terraform apply`:

```
 Error: No default VPC found in this region.

   with aws_default_vpc.default,
   on main.tf line 5, in resource "aws_default_vpc" "default":
    5: resource "aws_default_vpc" "default" {}
```

Now after upgrading, the above configuration will apply successfully.

To delete the default VPC, the above configuration should be updated to:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.default_vpc import DefaultVpc
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        DefaultVpc(self, "default",
            force_destroy=True
        )
```

## Plural Data Source Behavior

The following plural data sources are now consistent with [Provider Design](https://hashicorp.github.io/terraform-provider-aws/provider-design/#plural-data-sources)
such that they no longer return an error if zero results are found.

* [aws_cognito_user_pools](/docs/providers/aws/d/cognito_user_pools.html)
* [aws_db_event_categories](/docs/providers/aws/d/db_event_categories.html)
* [aws_ebs_volumes](/docs/providers/aws/d/ebs_volumes.html)
* [aws_ec2_coip_pools](/docs/providers/aws/d/ec2_coip_pools.html)
* [aws_ec2_local_gateway_route_tables](/docs/providers/aws/d/ec2_local_gateway_route_tables.html)
* [aws_ec2_local_gateway_virtual_interface_groups](/docs/providers/aws/d/ec2_local_gateway_virtual_interface_groups.html)
* [aws_ec2_local_gateways](/docs/providers/aws/d/ec2_local_gateways.html)
* [aws_ec2_transit_gateway_route_tables](/docs/providers/aws/d/ec2_transit_gateway_route_tables.html)
* [aws_efs_access_points](/docs/providers/aws/d/efs_access_points.html)
* [aws_emr_release_labels](/docs/providers/aws/d/emr_release_labels.html)
* [aws_inspector_rules_packages](/docs/providers/aws/d/inspector_rules_packages.html)
* [aws_ip_ranges](/docs/providers/aws/d/ip_ranges.html)
* [aws_network_acls](/docs/providers/aws/d/network_acls.html)
* [aws_route_tables](/docs/providers/aws/d/route_tables.html)
* [aws_security_groups](/docs/providers/aws/d/security_groups.html)
* [aws_ssoadmin_instances](/docs/providers/aws/d/ssoadmin_instances.html)
* [aws_vpcs](/docs/providers/aws/d/vpcs.html)
* [aws_vpc_peering_connections](/docs/providers/aws/d/vpc_peering_connections.html)

## Empty Strings Not Valid For Certain Resources

First, this is a breaking change but should affect very few configurations.

Second, the motivation behind this change is that previously, you might set an argument to `""` to explicitly convey it is empty. However, with the introduction of `null` in Terraform 0.12 and to prepare for continuing enhancements that distinguish between unset arguments and those that have a value, including an empty string (`""`), we are moving away from this use of zero values. We ask practitioners to either use `null` instead or remove the arguments that are set to `""`.

### Resource: aws_cloudwatch_event_target (Empty String)

Previously, you could set `ecs_target.0.launch_type` to `""`. However, the value `""` is no longer valid. Now, set the argument to `null` (_e.g._, `launch_type = null`) or remove the empty-string configuration.

For example, this type of configuration is now not valid:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.cloudwatch_event_target import CloudwatchEventTarget
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name, *, arn, rule):
        super().__init__(scope, name)
        CloudwatchEventTarget(self, "example",
            ecs_target=CloudwatchEventTargetEcsTarget(
                launch_type="",
                task_count=1,
                task_definition_arn=task.arn
            ),
            arn=arn,
            rule=rule
        )
```

We fix this configuration by setting `launch_type` to `null`:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.cloudwatch_event_target import CloudwatchEventTarget
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name, *, arn, rule):
        super().__init__(scope, name)
        CloudwatchEventTarget(self, "example",
            ecs_target=CloudwatchEventTargetEcsTarget(
                launch_type=[null],
                task_count=1,
                task_definition_arn=task.arn
            ),
            arn=arn,
            rule=rule
        )
```

### Resource: aws_customer_gateway

Previously, you could set `ip_address` to `""`, which would result in an AWS error. However, the provider now also gives an error.

### Resource: aws_default_network_acl

Previously, you could set `egress.*.cidr_block`, `egress.*.ipv6_cidr_block`, `ingress.*.cidr_block`, or `ingress.*.ipv6_cidr_block` to `""`. However, the value `""` is no longer valid. Now, set the argument to `null` (_e.g._, `ipv6_cidr_block = null`) or remove the empty-string configuration.

For example, this type of configuration is now not valid:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.default_network_acl import DefaultNetworkAcl
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name, *, action, fromPort, protocol, ruleNo, toPort, defaultNetworkAclId):
        super().__init__(scope, name)
        DefaultNetworkAcl(self, "example",
            egress=[DefaultNetworkAclEgress(
                cidr_block="0.0.0.0/0",
                ipv6_cidr_block="",
                action=action,
                from_port=from_port,
                protocol=protocol,
                rule_no=rule_no,
                to_port=to_port
            )
            ],
            default_network_acl_id=default_network_acl_id
        )
```

To fix this configuration, we remove the empty-string configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.default_network_acl import DefaultNetworkAcl
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name, *, action, fromPort, protocol, ruleNo, toPort, defaultNetworkAclId):
        super().__init__(scope, name)
        DefaultNetworkAcl(self, "example",
            egress=[DefaultNetworkAclEgress(
                cidr_block="0.0.0.0/0",
                action=action,
                from_port=from_port,
                protocol=protocol,
                rule_no=rule_no,
                to_port=to_port
            )
            ],
            default_network_acl_id=default_network_acl_id
        )
```

### Resource: aws_default_route_table

Previously, you could set `route.*.cidr_block` or `route.*.ipv6_cidr_block` to `""`. However, the value `""` is no longer valid. Now, set the argument to `null` (_e.g._, `ipv6_cidr_block = null`) or remove the empty-string configuration.

For example, this type of configuration is now not valid:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import conditional, Token, TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.default_route_table import DefaultRouteTable
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name, *, defaultRouteTableId):
        super().__init__(scope, name)
        DefaultRouteTable(self, "example",
            route=[DefaultRouteTableRoute(
                cidr_block=Token.as_string(conditional(ipv6, "", destination)),
                ipv6_cidr_block=Token.as_string(conditional(ipv6, destination_ipv6, ""))
            )
            ],
            default_route_table_id=default_route_table_id
        )
```

We fix this configuration by using `null` instead of an empty string (`""`):

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import conditional, Token, TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.default_route_table import DefaultRouteTable
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name, *, defaultRouteTableId):
        super().__init__(scope, name)
        DefaultRouteTable(self, "example",
            route=[DefaultRouteTableRoute(
                cidr_block=Token.as_string(conditional(ipv6, "null", destination)),
                ipv6_cidr_block=Token.as_string(
                    conditional(ipv6, destination_ipv6, "null"))
            )
            ],
            default_route_table_id=default_route_table_id
        )
```

### Resource: aws_default_vpc (Empty String)

Previously, you could set `ipv6_cidr_block` to `""`. However, the value `""` is no longer valid. Now, set the argument to `null` (_e.g._, `ipv6_cidr_block = null`) or remove the empty-string configuration.

### Resource: aws_instance

Previously, you could set `private_ip` to `""`. However, the value `""` is no longer valid. Now, set the argument to `null` (_e.g._, `private_ip = null`) or remove the empty-string configuration.

For example, this type of configuration is now not valid:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.instance import Instance
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        Instance(self, "example",
            instance_type="t2.micro",
            private_ip=""
        )
```

We fix this configuration by removing the empty-string configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.instance import Instance
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        Instance(self, "example",
            instance_type="t2.micro"
        )
```

### Resource: aws_efs_mount_target

Previously, you could set `ip_address` to `""`. However, the value `""` is no longer valid. Now, set the argument to `null` (_e.g._, `ip_address = null`) or remove the empty-string configuration.

For example, this type of configuration is now not valid: `ip_address = ""`.

### Resource: aws_elasticsearch_domain

Previously, you could set `ebs_options.0.volume_type` to `""`. However, the value `""` is no longer valid. Now, set the argument to `null` (_e.g._, `volume_type = null`) or remove the empty-string configuration.

For example, this type of configuration is now not valid:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import Op, conditional, Token, TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.elasticsearch_domain import ElasticsearchDomain
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name, *, domainName):
        super().__init__(scope, name)
        ElasticsearchDomain(self, "example",
            ebs_options=ElasticsearchDomainEbsOptions(
                ebs_enabled=True,
                volume_size=volume_size.number_value,
                volume_type=Token.as_string(
                    conditional(Op.gt(volume_size.value, 0), volume_type, ""))
            ),
            domain_name=domain_name
        )
```

We fix this configuration by using `null` instead of `""`:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import Op, conditional, Token, TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.elasticsearch_domain import ElasticsearchDomain
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name, *, domainName):
        super().__init__(scope, name)
        ElasticsearchDomain(self, "example",
            ebs_options=ElasticsearchDomainEbsOptions(
                ebs_enabled=True,
                volume_size=volume_size.number_value,
                volume_type=Token.as_string(
                    conditional(Op.gt(volume_size.value, 0), volume_type, "null"))
            ),
            domain_name=domain_name
        )
```

### Resource: aws_network_acl

Previously, `egress.*.cidr_block`, `egress.*.ipv6_cidr_block`, `ingress.*.cidr_block`, and `ingress.*.ipv6_cidr_block` could be set to `""`. However, the value `""` is no longer valid. Now, set the argument to `null` (_e.g._, `ipv6_cidr_block = null`) or remove the empty-string configuration.

For example, this type of configuration is now not valid:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.network_acl import NetworkAcl
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name, *, vpcId):
        super().__init__(scope, name)
        NetworkAcl(self, "example",
            egress=[NetworkAclEgress(
                cidr_block="0.0.0.0/0",
                ipv6_cidr_block=""
            )
            ],
            vpc_id=vpc_id
        )
```

We fix this configuration by removing the empty-string configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.network_acl import NetworkAcl
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name, *, vpcId):
        super().__init__(scope, name)
        NetworkAcl(self, "example",
            egress=[NetworkAclEgress(
                cidr_block="0.0.0.0/0"
            )
            ],
            vpc_id=vpc_id
        )
```

### Resource: aws_route

Previously, `destination_cidr_block` and `destination_ipv6_cidr_block` could be set to `""`. However, the value `""` is no longer valid. Now, set the argument to `null` (_e.g._, `destination_ipv6_cidr_block = null`) or remove the empty-string configuration.

In addition, now exactly one of `destination_cidr_block`, `destination_ipv6_cidr_block`, and `destination_prefix_list_id` can be set.

For example, this type of configuration for `aws_route` is now not valid:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import conditional, Token, TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.route import Route
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        Route(self, "example",
            destination_cidr_block=Token.as_string(conditional(ipv6, "", destination)),
            destination_ipv6_cidr_block=Token.as_string(
                conditional(ipv6, destination_ipv6, "")),
            gateway_id=Token.as_string(aws_internet_gateway_example.id),
            route_table_id=Token.as_string(aws_route_table_example.id)
        )
```

We fix this configuration by using `null` instead of an empty-string (`""`):

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import conditional, Token, TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.route import Route
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        Route(self, "example",
            destination_cidr_block=Token.as_string(
                conditional(ipv6, "null", destination)),
            destination_ipv6_cidr_block=Token.as_string(
                conditional(ipv6, destination_ipv6, "null")),
            gateway_id=Token.as_string(aws_internet_gateway_example.id),
            route_table_id=Token.as_string(aws_route_table_example.id)
        )
```

### Resource: aws_route_table

Previously, `route.*.cidr_block` and `route.*.ipv6_cidr_block` could be set to `""`. However, the value `""` is no longer valid. Now, set the argument to `null` (_e.g._, `ipv6_cidr_block = null`) or remove the empty-string configuration.

For example, this type of configuration is now not valid:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import conditional, Token, TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.route_table import RouteTable
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name, *, vpcId):
        super().__init__(scope, name)
        RouteTable(self, "example",
            route=[RouteTableRoute(
                cidr_block=Token.as_string(conditional(ipv6, "", destination)),
                ipv6_cidr_block=Token.as_string(conditional(ipv6, destination_ipv6, ""))
            )
            ],
            vpc_id=vpc_id
        )
```

We fix this configuration by usingd `null` instead of an empty-string (`""`):

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import conditional, Token, TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.route_table import RouteTable
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name, *, vpcId):
        super().__init__(scope, name)
        RouteTable(self, "example",
            route=[RouteTableRoute(
                cidr_block=Token.as_string(conditional(ipv6, "null", destination)),
                ipv6_cidr_block=Token.as_string(
                    conditional(ipv6, destination_ipv6, "null"))
            )
            ],
            vpc_id=vpc_id
        )
```

### Resource: aws_vpc

Previously, `ipv6_cidr_block` could be set to `""`. However, the value `""` is no longer valid. Now, set the argument to `null` (_e.g._, `ipv6_cidr_block = null`) or remove the empty-string configuration.

For example, this type of configuration is now not valid:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.vpc import Vpc
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        Vpc(self, "example",
            cidr_block="10.1.0.0/16",
            ipv6_cidr_block=""
        )
```

We fix this configuration by removing `ipv6_cidr_block`:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.vpc import Vpc
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        Vpc(self, "example",
            cidr_block="10.1.0.0/16"
        )
```

### Resource: aws_vpc_ipv6_cidr_block_association

Previously, `ipv6_cidr_block` could be set to `""`. However, the value `""` is no longer valid. Now, set the argument to `null` (_e.g._, `ipv6_cidr_block = null`) or remove the empty-string configuration.

## Data Source: aws_cloudwatch_log_group

### Removal of arn Wildcard Suffix

Previously, the data source returned the ARN directly from the API, which included a `:*` suffix to denote all CloudWatch Log Streams under the CloudWatch Log Group. Most other AWS resources that return ARNs and many other AWS services do not use the `:*` suffix. The suffix is now automatically removed. For example, the data source previously returned an ARN such as `arn:aws:logs:us-east-1:123456789012:log-group:/example:*` but will now return `arn:aws:logs:us-east-1:123456789012:log-group:/example`.

Workarounds, such as using `replace()` as shown below, should be removed:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import Fn, Token, TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.data_aws_cloudwatch_log_group import DataAwsCloudwatchLogGroup
from imports.aws.datasync_task import DatasyncTask
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name, *, destinationLocationArn, sourceLocationArn):
        super().__init__(scope, name)
        example = DataAwsCloudwatchLogGroup(self, "example",
            name="example"
        )
        aws_datasync_task_example = DatasyncTask(self, "example_1",
            cloudwatch_log_group_arn=Token.as_string(
                Fn.replace(Token.as_string(example.arn), ":*", "")),
            destination_location_arn=destination_location_arn,
            source_location_arn=source_location_arn
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_datasync_task_example.override_logical_id("example")
```

Removing the `:*` suffix is a breaking change for some configurations. Fix these configurations using string interpolations as demonstrated below. For example, this configuration is now broken:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import Token, TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.data_aws_iam_policy_document import DataAwsIamPolicyDocument
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        DataAwsIamPolicyDocument(self, "ad-log-policy",
            statement=[DataAwsIamPolicyDocumentStatement(
                actions=["logs:CreateLogStream", "logs:PutLogEvents"],
                effect="Allow",
                principals=[DataAwsIamPolicyDocumentStatementPrincipals(
                    identifiers=["ds.amazonaws.com"],
                    type="Service"
                )
                ],
                resources=[Token.as_string(example.arn)]
            )
            ]
        )
```

An updated configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.data_aws_iam_policy_document import DataAwsIamPolicyDocument
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        DataAwsIamPolicyDocument(self, "ad-log-policy",
            statement=[DataAwsIamPolicyDocumentStatement(
                actions=["logs:CreateLogStream", "logs:PutLogEvents"],
                effect="Allow",
                principals=[DataAwsIamPolicyDocumentStatementPrincipals(
                    identifiers=["ds.amazonaws.com"],
                    type="Service"
                )
                ],
                resources=["${" + example.arn + "}:*"]
            )
            ]
        )
```

## Data Source: aws_subnet_ids

The `aws_subnet_ids` data source has been deprecated and will be removed in a future version. Use the `aws_subnets` data source instead.

For example, change a configuration such as

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import Token, TerraformIterator, TerraformOutput, TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws. import DataAwsSubnetIds
from imports.aws.data_aws_subnet import DataAwsSubnet
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = DataAwsSubnetIds(self, "example",
            vpc_id=vpc_id.value
        )
        # In most cases loops should be handled in the programming language context and
        #     not inside of the Terraform context. If you are looping over something external, e.g. a variable or a file input
        #     you should consider using a for loop. If you are looping over something only known to Terraform, e.g. a result of a data source
        #     you need to keep this like it is.
        example_for_each_iterator = TerraformIterator.from_list(
            Token.as_any(example.ids))
        data_aws_subnet_example = DataAwsSubnet(self, "example_1",
            id=Token.as_string(example_for_each_iterator.value),
            for_each=example_for_each_iterator
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        data_aws_subnet_example.override_logical_id("example")
        TerraformOutput(self, "subnet_cidr_blocks",
            value="${[ for s in ${" + data_aws_subnet_example.fqn + "} : s.cidr_block]}"
        )
```

to

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import Token, TerraformIterator, TerraformOutput, TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.data_aws_subnet import DataAwsSubnet
from imports.aws.data_aws_subnets import DataAwsSubnets
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = DataAwsSubnets(self, "example",
            filter=[DataAwsSubnetsFilter(
                name="vpc-id",
                values=[vpc_id.string_value]
            )
            ]
        )
        # In most cases loops should be handled in the programming language context and
        #     not inside of the Terraform context. If you are looping over something external, e.g. a variable or a file input
        #     you should consider using a for loop. If you are looping over something only known to Terraform, e.g. a result of a data source
        #     you need to keep this like it is.
        example_for_each_iterator = TerraformIterator.from_list(
            Token.as_any(example.ids))
        data_aws_subnet_example = DataAwsSubnet(self, "example_1",
            id=Token.as_string(example_for_each_iterator.value),
            for_each=example_for_each_iterator
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        data_aws_subnet_example.override_logical_id("example")
        TerraformOutput(self, "subnet_cidr_blocks",
            value="${[ for s in ${" + data_aws_subnet_example.fqn + "} : s.cidr_block]}"
        )
```

## Data Source: aws_s3_bucket_object

Version 4.x deprecates the `aws_s3_bucket_object` data source. Maintainers will remove it in a future version. Use `aws_s3_object` instead, where new features and fixes will be added.

## Data Source: aws_s3_bucket_objects

Version 4.x deprecates the `aws_s3_bucket_objects` data source. Maintainers will remove it in a future version. Use `aws_s3_objects` instead, where new features and fixes will be added.

## Resource: aws_batch_compute_environment

You can no longer specify `compute_resources` when `type` is `UNMANAGED`.

Previously, you could apply this configuration and the provider would ignore any compute resources:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import Token, TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.batch_compute_environment import BatchComputeEnvironment
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        BatchComputeEnvironment(self, "test",
            compute_environment_name="test",
            compute_resources=BatchComputeEnvironmentComputeResources(
                instance_role=ecs_instance.arn,
                instance_type=["c4.large"],
                max_vcpus=16,
                min_vcpus=0,
                security_group_ids=[Token.as_string(aws_security_group_test.id)],
                subnets=[Token.as_string(aws_subnet_test.id)],
                type="EC2"
            ),
            service_role=batch_service.arn,
            type="UNMANAGED"
        )
```

Now, this configuration is invalid and will result in an error during plan.

To resolve this error, simply remove or comment out the `compute_resources` configuration block.

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.batch_compute_environment import BatchComputeEnvironment
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        BatchComputeEnvironment(self, "test",
            compute_environment_name="test",
            service_role=batch_service.arn,
            type="UNMANAGED"
        )
```

## Resource: aws_cloudwatch_event_target

### Removal of `ecs_target` `launch_type` default value

Previously, the provider assigned `ecs_target` `launch_type` the default value of `EC2` if you did not configure a value. However, the provider no longer assigns a default value.

For example, previously you could workaround the default value by using an empty string (`""`), as shown:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import Token, TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.cloudwatch_event_target import CloudwatchEventTarget
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        CloudwatchEventTarget(self, "test",
            arn=Token.as_string(aws_ecs_cluster_test.id),
            ecs_target=CloudwatchEventTargetEcsTarget(
                launch_type="",
                network_configuration=CloudwatchEventTargetEcsTargetNetworkConfiguration(
                    subnets=[subnet.id]
                ),
                task_count=1,
                task_definition_arn=task.arn
            ),
            role_arn=Token.as_string(aws_iam_role_test.arn),
            rule=Token.as_string(aws_cloudwatch_event_rule_test.id)
        )
```

This is no longer necessary. We fix the configuration by removing the empty string assignment:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import Token, TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.cloudwatch_event_target import CloudwatchEventTarget
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        CloudwatchEventTarget(self, "test",
            arn=Token.as_string(aws_ecs_cluster_test.id),
            ecs_target=CloudwatchEventTargetEcsTarget(
                network_configuration=CloudwatchEventTargetEcsTargetNetworkConfiguration(
                    subnets=[subnet.id]
                ),
                task_count=1,
                task_definition_arn=task.arn
            ),
            role_arn=Token.as_string(aws_iam_role_test.arn),
            rule=Token.as_string(aws_cloudwatch_event_rule_test.id)
        )
```

## Resource: aws_elasticache_cluster

### Error raised if neither `engine` nor `replication_group_id` is specified

Previously, when you did not specify either `engine` or `replication_group_id`, Terraform would not prevent you from applying the invalid configuration.
Now, this will produce an error similar to the one below:

```
Error: Invalid combination of arguments

          with aws_elasticache_cluster.example,
          on terraform_plugin_test.tf line 2, in resource "aws_elasticache_cluster" "example":
           2: resource "aws_elasticache_cluster" "example" {

        "replication_group_id": one of `engine,replication_group_id` must be
        specified

        Error: Invalid combination of arguments

          with aws_elasticache_cluster.example,
          on terraform_plugin_test.tf line 2, in resource "aws_elasticache_cluster" "example":
           2: resource "aws_elasticache_cluster" "example" {

        "engine": one of `engine,replication_group_id` must be specified
```

Update your configuration to supply one of `engine` or `replication_group_id`.

## Resource: aws_elasticache_global_replication_group

### actual_engine_version Attribute removal

Switch your Terraform configuration from using `actual_engine_version` to use the `engine_version_actual` attribute instead.

For example, given this previous configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformOutput, TerraformStack
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        TerraformOutput(self, "elasticache_global_replication_group_version_result",
            value=example.actual_engine_version
        )
```

An updated configuration:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformOutput, TerraformStack
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        TerraformOutput(self, "elasticache_global_replication_group_version_result",
            value=example.engine_version_actual
        )
```

## Resource: aws_fsx_ontap_storage_virtual_machine

We removed the misspelled argument `active_directory_configuration.0.self_managed_active_directory_configuration.0.organizational_unit_distinguidshed_name` that we previously deprecated. Use `active_directory_configuration.0.self_managed_active_directory_configuration.0.organizational_unit_distinguished_name` now instead. Terraform will automatically migrate the state to `active_directory_configuration.0.self_managed_active_directory_configuration.0.organizational_unit_distinguished_name` during planning.

## Resource: aws_lb_target_group

For `protocol = "TCP"`, you can no longer set `stickiness.type` to `lb_cookie` even when `enabled = false`. Instead, either change the `protocol` to `"HTTP"` or `"HTTPS"`, or change `stickiness.type` to `"source_ip"`.

For example, this configuration is no longer valid:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import Token, TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.lb_target_group import LbTargetGroup
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        LbTargetGroup(self, "test",
            port=25,
            protocol="TCP",
            stickiness=LbTargetGroupStickiness(
                enabled=False,
                type="lb_cookie"
            ),
            vpc_id=Token.as_string(aws_vpc_test.id)
        )
```

To fix this, we change the `stickiness.type` to `"source_ip"`.

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import Token, TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.lb_target_group import LbTargetGroup
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        LbTargetGroup(self, "test",
            port=25,
            protocol="TCP",
            stickiness=LbTargetGroupStickiness(
                enabled=False,
                type="source_ip"
            ),
            vpc_id=Token.as_string(aws_vpc_test.id)
        )
```

## Resource: aws_s3_bucket_object

Version 4.x deprecates the `aws_s3_bucket_object` and maintainers will remove it in a future version. Use `aws_s3_object` instead, where new features and fixes will be added.

When replacing `aws_s3_bucket_object` with `aws_s3_object` in your configuration, on the next apply, Terraform will recreate the object. If you prefer to not have Terraform recreate the object, import the object using `aws_s3_object`.

For example, the following will import an S3 object into state, assuming the configuration exists, as `aws_s3_object.example`:

```console
% terraform import aws_s3_object.example s3://some-bucket-name/some/key.txt
```

~> **CAUTION:** We do not recommend modifying the state file manually. If you do, you can make it unusable. However, if you accept that risk, some community members have upgraded to the new resource by searching and replacing `"type": "aws_s3_bucket_object",` with `"type": "aws_s3_object",` in the state file, and then running `terraform apply -refresh-only`.

## EC2-Classic Resource and Data Source Support

While an upgrade to this major version will not directly impact EC2-Classic resources configured with Terraform,
it is important to keep in the mind the following AWS Provider resources will eventually no longer
be compatible with EC2-Classic as AWS completes their EC2-Classic networking retirement (expected around August 15, 2022).

* Running or stopped [EC2 instances](/docs/providers/aws/r/instance.html)
* Running or stopped [RDS database instances](/docs/providers/aws/r/db_instance.html)
* [Elastic IP addresses](/docs/providers/aws/r/eip.html)
* [Classic Load Balancers](/docs/providers/aws/r/lb.html)
* [Redshift clusters](/docs/providers/aws/r/redshift_cluster.html)
* [Elastic Beanstalk environments](/docs/providers/aws/r/elastic_beanstalk_environment.html)
* [EMR clusters](/docs/providers/aws/r/emr_cluster.html)
* [AWS Data Pipelines pipelines](/docs/providers/aws/r/datapipeline_pipeline.html)
* [ElastiCache clusters](/docs/providers/aws/r/elasticache_cluster.html)
* [Spot Requests](/docs/providers/aws/r/spot_instance_request.html)
* [Capacity Reservations](/docs/providers/aws/r/ec2_capacity_reservation.html)

## Macie Classic Resource Support

These resources should be considered deprecated and will be removed in version 5.0.0.

* Macie Member Account Association
* Macie S3 Bucket Association

<!-- cache-key: cdktf-0.19.0 input-3ef88020b37e35d78babc46d26306cbef9b0d542745d0afa1cec66e81ef43304 -->