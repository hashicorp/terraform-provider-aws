---
subcategory: "AMP (Managed Prometheus)"
layout: "aws"
page_title: "AWS: aws_prometheus_scraper"
description: |-
  Manages an Amazon Managed Service for Prometheus (AMP) Scraper.
---


<!-- Please do not edit this file, it is generated. -->
# Resource: aws_prometheus_scraper

-> **Note:** You cannot update a scraper. If you change any attribute, Terraform
will delete the current and create a new one.

Provides an Amazon Managed Service for Prometheus fully managed collector
(scraper).

Read more in the [Amazon Managed Service for Prometheus user guide](https://docs.aws.amazon.com/prometheus/latest/userguide/AMP-collector.html).

## Example Usage

### Basic Usage

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import Token, Fn, TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.prometheus_scraper import PrometheusScraper
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        PrometheusScraper(self, "example",
            destination=[PrometheusScraperDestination(
                amp=[PrometheusScraperDestinationAmp(
                    workspace_arn=Token.as_string(aws_prometheus_workspace_example.arn)
                )
                ]
            )
            ],
            scrape_configuration="global:\n  scrape_interval: 30s\nscrape_configs:\n  # pod metrics\n  - job_name: pod_exporter\n    kubernetes_sd_configs:\n      - role: pod\n  # container metrics\n  - job_name: cadvisor\n    scheme: https\n    authorization:\n      credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n    kubernetes_sd_configs:\n      - role: node\n    relabel_configs:\n      - action: labelmap\n        regex: __meta_kubernetes_node_label_(.+)\n      - replacement: kubernetes.default.svc:443\n        target_label: __address__\n      - source_labels: [__meta_kubernetes_node_name]\n        regex: (.+)\n        target_label: __metrics_path__\n        replacement: /api/v1/nodes/$1/proxy/metrics/cadvisor\n  # apiserver metrics\n  - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n    job_name: kubernetes-apiservers\n    kubernetes_sd_configs:\n    - role: endpoints\n    relabel_configs:\n    - action: keep\n      regex: default;kubernetes;https\n      source_labels:\n      - __meta_kubernetes_namespace\n      - __meta_kubernetes_service_name\n      - __meta_kubernetes_endpoint_port_name\n    scheme: https\n  # kube proxy metrics\n  - job_name: kube-proxy\n    honor_labels: true\n    kubernetes_sd_configs:\n    - role: pod\n    relabel_configs:\n    - action: keep\n      source_labels:\n      - __meta_kubernetes_namespace\n      - __meta_kubernetes_pod_name\n      separator: '/'\n      regex: 'kube-system/kube-proxy.+'\n    - source_labels:\n      - __address__\n      action: replace\n      target_label: __address__\n      regex: (.+?)(\\\\:\\\\d+)?\n      replacement: $1:10249\n\n",
            source=[PrometheusScraperSource(
                eks=[PrometheusScraperSourceEks(
                    cluster_arn=Token.as_string(data_aws_eks_cluster_example.arn),
                    subnet_ids=Token.as_list(
                        Fn.lookup_nested(data_aws_eks_cluster_example.vpc_config, ["0", "subnet_ids"
                        ]))
                )
                ]
            )
            ]
        )
```

### Use default EKS scraper configuration

You can use the data source `aws_prometheus_scraper_configuration` to use a
service managed scrape configuration.

-> **Note:** If the configuration is updated, this will trigger a replacement
of your scraper.

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import Token, Fn, TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.data_aws_prometheus_default_scraper_configuration import DataAwsPrometheusDefaultScraperConfiguration
from imports.aws.prometheus_scraper import PrometheusScraper
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        PrometheusScraper(self, "example",
            destination=[PrometheusScraperDestination(
                amp=[PrometheusScraperDestinationAmp(
                    workspace_arn=Token.as_string(aws_prometheus_workspace_example.arn)
                )
                ]
            )
            ],
            scrape_configuration=Token.as_string(data_aws_prometheus_scraper_configuration_example.configuration),
            source=[PrometheusScraperSource(
                eks=[PrometheusScraperSourceEks(
                    cluster_arn=Token.as_string(data_aws_eks_cluster_example.arn),
                    subnet_ids=Token.as_list(
                        Fn.lookup_nested(data_aws_eks_cluster_example.vpc_config, ["0", "subnet_ids"
                        ]))
                )
                ]
            )
            ]
        )
        data_aws_prometheus_default_scraper_configuration_example =
        DataAwsPrometheusDefaultScraperConfiguration(self, "example_1")
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        data_aws_prometheus_default_scraper_configuration_example.override_logical_id("example")
```

### Ignoring changes to Prometheus Workspace destination

A managed scraper will add a `AMPAgentlessScraper` tag to its Prometheus workspace
destination. To avoid Terraform state forcing removing the tag from the workspace,
you can add this tag to the destination workspace (preferred) or ignore tags
changes with `lifecycle`. See example below.

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import Token, Fn, TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.data_aws_eks_cluster import DataAwsEksCluster
from imports.aws.prometheus_scraper import PrometheusScraper
from imports.aws.prometheus_workspace import PrometheusWorkspace
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = PrometheusWorkspace(self, "example",
            tags={
                "AMPAgentlessScraper": ""
            }
        )
        DataAwsEksCluster(self, "this",
            name="example"
        )
        aws_prometheus_scraper_example = PrometheusScraper(self, "example_2",
            destination=[PrometheusScraperDestination(
                amp=[PrometheusScraperDestinationAmp(
                    workspace_arn=example.arn
                )
                ]
            )
            ],
            scrape_configuration="...",
            source=[PrometheusScraperSource(
                eks=[PrometheusScraperSourceEks(
                    cluster_arn=Token.as_string(data_aws_eks_cluster_example.arn),
                    subnet_ids=Token.as_list(
                        Fn.lookup_nested(data_aws_eks_cluster_example.vpc_config, ["0", "subnet_ids"
                        ]))
                )
                ]
            )
            ]
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_prometheus_scraper_example.override_logical_id("example")
```

### Configure aws-auth

Your source Amazon EKS cluster must be configured to allow the scraper to access
metrics. Follow the [user guide](https://docs.aws.amazon.com/prometheus/latest/userguide/AMP-collector-how-to.html#AMP-collector-eks-setup)
to setup the appropriate Kubernetes permissions.

## Argument Reference

The following arguments are required:

* `destination` - (Required) Configuration block for the managed scraper to send metrics to. See [`destination`](#destination).
* `scrape_configuration` - (Required) The configuration file to use in the new scraper. For more information, see [Scraper configuration](https://docs.aws.amazon.com/prometheus/latest/userguide/AMP-collector-how-to.html#AMP-collector-configuration).
* `source` - (Required) Configuration block to specify where the managed scraper will collect metrics from. See [`source`](#source).

The following arguments are optional:

* `alias` - (Optional) a name to associate with the managed scraper. This is for your use, and does not need to be unique.

### `destination`

* `amp` - (Required) Configuration block for an Amazon Managed Prometheus workspace destination. See [`amp`](#amp).

### `amp`

* `workspace_arn` - (Required) The Amazon Resource Name (ARN) of the prometheus workspace.

### `source`

* `eks` - (Required) Configuration block for an EKS cluster source. See [`eks`](#eks).

#### `eks`

* `eks_cluster_arn` - (Required) The Amazon Resource Name (ARN) of the source EKS cluster.
* `subnet_ids` - (Required) List of subnet IDs. Must be in at least two different availability zones.
* `security_group_ids` - (Optional) List of the security group IDs for the Amazon EKS cluster VPC configuration.

## Attribute Reference

This resource exports the following attributes in addition to the arguments above:

* `arn` - The Amazon Resource Name (ARN) of the new scraper.
* `role_arn` - The Amazon Resource Name (ARN) of the IAM role that provides permissions for the scraper to discover, collect, and produce metrics
* `status` - Status of the scraper. One of ACTIVE, CREATING, DELETING, CREATION_FAILED, DELETION_FAILED

## Timeouts

[Configuration options](https://developer.hashicorp.com/terraform/language/resources/syntax#operation-timeouts):

* `create` - (Default `30m`)
* `delete` - (Default `20m`)

## Import

In Terraform v1.5.0 and later, use an [`import` block](https://developer.hashicorp.com/terraform/language/import) to import the Managed Scraper using the scraper
identifier. For example:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.prometheus_scraper import PrometheusScraper
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        PrometheusScraper.generate_config_for_import(self, "example", "s-0123abc-0000-0123-a000-000000000000")
```

Using `terraform import`, import the Managed Scraper using its identifier.
For example:

```console
% terraform import aws_prometheus_scraper.example s-0123abc-0000-0123-a000-000000000000
```

<!-- cache-key: cdktf-0.20.8 input-5a6a1dfc6ef96e7586ca21ecc3d78f062ba48b148420bf9248df3cbd7472d11b -->