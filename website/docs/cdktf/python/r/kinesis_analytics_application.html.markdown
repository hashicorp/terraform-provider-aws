---
subcategory: "Kinesis Analytics"
layout: "aws"
page_title: "AWS: aws_kinesis_analytics_application"
description: |-
  Provides a AWS Kinesis Analytics Application
---


<!-- Please do not edit this file, it is generated. -->
# Resource: aws_kinesis_analytics_application

Provides a Kinesis Analytics Application resource. Kinesis Analytics is a managed service that
allows processing and analyzing streaming data using standard SQL.

For more details, see the [Amazon Kinesis Analytics Documentation][1].

-> **Note:** To manage Amazon Kinesis Data Analytics for Apache Flink applications, use the [`aws_kinesisanalyticsv2_application`](/docs/providers/aws/r/kinesisanalyticsv2_application.html) resource.

## Example Usage

### Kinesis Stream Input

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.kinesis_analytics_application import KinesisAnalyticsApplication
from imports.aws.kinesis_stream import KinesisStream
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        test_stream = KinesisStream(self, "test_stream",
            name="terraform-kinesis-test",
            shard_count=1
        )
        KinesisAnalyticsApplication(self, "test_application",
            inputs=KinesisAnalyticsApplicationInputs(
                kinesis_stream=KinesisAnalyticsApplicationInputsKinesisStream(
                    resource_arn=test_stream.arn,
                    role_arn=test.arn
                ),
                name_prefix="test_prefix",
                parallelism=KinesisAnalyticsApplicationInputsParallelism(
                    count=1
                ),
                schema=KinesisAnalyticsApplicationInputsSchema(
                    record_columns=[KinesisAnalyticsApplicationInputsSchemaRecordColumns(
                        mapping="$.test",
                        name="test",
                        sql_type="VARCHAR(8)"
                    )
                    ],
                    record_encoding="UTF-8",
                    record_format=KinesisAnalyticsApplicationInputsSchemaRecordFormat(
                        mapping_parameters=KinesisAnalyticsApplicationInputsSchemaRecordFormatMappingParameters(
                            json=KinesisAnalyticsApplicationInputsSchemaRecordFormatMappingParametersJson(
                                record_row_path="$"
                            )
                        )
                    )
                )
            ),
            name="kinesis-analytics-application-test"
        )
```

### Starting An Application

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import Token, TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.cloudwatch_log_group import CloudwatchLogGroup
from imports.aws.cloudwatch_log_stream import CloudwatchLogStream
from imports.aws.kinesis_analytics_application import KinesisAnalyticsApplication
from imports.aws.kinesis_firehose_delivery_stream import KinesisFirehoseDeliveryStream
from imports.aws.kinesis_stream import KinesisStream
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = CloudwatchLogGroup(self, "example",
            name="analytics"
        )
        aws_cloudwatch_log_stream_example = CloudwatchLogStream(self, "example_1",
            log_group_name=example.name,
            name="example-kinesis-application"
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_cloudwatch_log_stream_example.override_logical_id("example")
        aws_kinesis_firehose_delivery_stream_example =
        KinesisFirehoseDeliveryStream(self, "example_2",
            destination="extended_s3",
            extended_s3_configuration=KinesisFirehoseDeliveryStreamExtendedS3Configuration(
                bucket_arn=Token.as_string(aws_s3_bucket_example.arn),
                role_arn=Token.as_string(aws_iam_role_example.arn)
            ),
            name="example-kinesis-delivery-stream"
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_kinesis_firehose_delivery_stream_example.override_logical_id("example")
        aws_kinesis_stream_example = KinesisStream(self, "example_3",
            name="example-kinesis-stream",
            shard_count=1
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_kinesis_stream_example.override_logical_id("example")
        KinesisAnalyticsApplication(self, "test",
            cloudwatch_logging_options=KinesisAnalyticsApplicationCloudwatchLoggingOptions(
                log_stream_arn=Token.as_string(aws_cloudwatch_log_stream_example.arn),
                role_arn=Token.as_string(aws_iam_role_example.arn)
            ),
            inputs=KinesisAnalyticsApplicationInputs(
                kinesis_stream=KinesisAnalyticsApplicationInputsKinesisStream(
                    resource_arn=Token.as_string(aws_kinesis_stream_example.arn),
                    role_arn=Token.as_string(aws_iam_role_example.arn)
                ),
                name_prefix="example_prefix",
                schema=KinesisAnalyticsApplicationInputsSchema(
                    record_columns=[KinesisAnalyticsApplicationInputsSchemaRecordColumns(
                        name="COLUMN_1",
                        sql_type="INTEGER"
                    )
                    ],
                    record_format=KinesisAnalyticsApplicationInputsSchemaRecordFormat(
                        mapping_parameters=KinesisAnalyticsApplicationInputsSchemaRecordFormatMappingParameters(
                            csv=KinesisAnalyticsApplicationInputsSchemaRecordFormatMappingParametersCsv(
                                record_column_delimiter=",",
                                record_row_delimiter="|"
                            )
                        )
                    )
                ),
                starting_position_configuration=[KinesisAnalyticsApplicationInputsStartingPositionConfiguration(
                    starting_position="NOW"
                )
                ]
            ),
            name="example-application",
            outputs=[KinesisAnalyticsApplicationOutputs(
                kinesis_firehose=KinesisAnalyticsApplicationOutputsKinesisFirehose(
                    resource_arn=Token.as_string(aws_kinesis_firehose_delivery_stream_example.arn),
                    role_arn=Token.as_string(aws_iam_role_example.arn)
                ),
                name="OUTPUT_1",
                schema=KinesisAnalyticsApplicationOutputsSchema(
                    record_format_type="CSV"
                )
            )
            ],
            start_application=True
        )
```

## Argument Reference

This resource supports the following arguments:

* `name` - (Required) Name of the Kinesis Analytics Application.
* `code` - (Optional) SQL Code to transform input data, and generate output.
* `description` - (Optional) Description of the application.
* `cloudwatch_logging_options` - (Optional) The CloudWatch log stream options to monitor application errors.
See [CloudWatch Logging Options](#cloudwatch-logging-options) below for more details.
* `inputs` - (Optional) Input configuration of the application. See [Inputs](#inputs) below for more details.
* `outputs` - (Optional) Output destination configuration of the application. See [Outputs](#outputs) below for more details.
* `reference_data_sources` - (Optional) An S3 Reference Data Source for the application.
See [Reference Data Sources](#reference-data-sources) below for more details.
* `start_application` - (Optional) Whether to start or stop the Kinesis Analytics Application. To start an application, an input with a defined `starting_position` must be configured.
To modify an application's starting position, first stop the application by setting `start_application = false`, then update `starting_position` and set `start_application = true`.
* `tags` - Key-value map of tags for the Kinesis Analytics Application. If configured with a provider [`default_tags` configuration block](https://registry.terraform.io/providers/hashicorp/aws/latest/docs#default_tags-configuration-block) present, tags with matching keys will overwrite those defined at the provider-level.

### CloudWatch Logging Options

Configure a CloudWatch Log Stream to monitor application errors.

The `cloudwatch_logging_options` block supports the following:

* `log_stream_arn` - (Required) The ARN of the CloudWatch Log Stream.
* `role_arn` - (Required) The ARN of the IAM Role used to send application messages.

### Inputs

Configure an Input for the Kinesis Analytics Application. You can only have 1 Input configured.

The `inputs` block supports the following:

* `name_prefix` - (Required) The Name Prefix to use when creating an in-application stream.
* `schema` - (Required) The Schema format of the data in the streaming source. See [Source Schema](#source-schema) below for more details.
* `kinesis_firehose` - (Optional) The Kinesis Firehose configuration for the streaming source. Conflicts with `kinesis_stream`.
See [Kinesis Firehose](#kinesis-firehose) below for more details.
* `kinesis_stream` - (Optional) The Kinesis Stream configuration for the streaming source. Conflicts with `kinesis_firehose`.
See [Kinesis Stream](#kinesis-stream) below for more details.
* `parallelism` - (Optional) The number of Parallel in-application streams to create.
See [Parallelism](#parallelism) below for more details.
* `processing_configuration` - (Optional) The Processing Configuration to transform records as they are received from the stream.
See [Processing Configuration](#processing-configuration) below for more details.
* `starting_position_configuration` (Optional) The point at which the application starts processing records from the streaming source.
See [Starting Position Configuration](#starting-position-configuration) below for more details.

### Outputs

Configure Output destinations for the Kinesis Analytics Application. You can have a maximum of 3 destinations configured.

The `outputs` block supports the following:

* `name` - (Required) The Name of the in-application stream.
* `schema` - (Required) The Schema format of the data written to the destination. See [Destination Schema](#destination-schema) below for more details.
* `kinesis_firehose` - (Optional) The Kinesis Firehose configuration for the destination stream. Conflicts with `kinesis_stream`.
See [Kinesis Firehose](#kinesis-firehose) below for more details.
* `kinesis_stream` - (Optional) The Kinesis Stream configuration for the destination stream. Conflicts with `kinesis_firehose`.
See [Kinesis Stream](#kinesis-stream) below for more details.
* `lambda` - (Optional) The Lambda function destination. See [Lambda](#lambda) below for more details.

### Reference Data Sources

Add a Reference Data Source to the Kinesis Analytics Application. You can only have 1 Reference Data Source.

The `reference_data_sources` block supports the following:

* `schema` - (Required) The Schema format of the data in the streaming source. See [Source Schema](#source-schema) below for more details.
* `table_name` - (Required) The in-application Table Name.
* `s3` - (Optional) The S3 configuration for the reference data source. See [S3 Reference](#s3-reference) below for more details.

#### Kinesis Firehose

Configuration for a Kinesis Firehose delivery stream.

The `kinesis_firehose` block supports the following:

* `resource_arn` - (Required) The ARN of the Kinesis Firehose delivery stream.
* `role_arn` - (Required) The ARN of the IAM Role used to access the stream.

#### Kinesis Stream

Configuration for a Kinesis Stream.

The `kinesis_stream` block supports the following:

* `resource_arn` - (Required) The ARN of the Kinesis Stream.
* `role_arn` - (Required) The ARN of the IAM Role used to access the stream.

#### Destination Schema

The Schema format of the data in the destination.

The `schema` block supports the following:

* `record_format_type` - (Required) The Format Type of the records on the output stream. Can be `CSV` or `JSON`.

#### Source Schema

The Schema format of the data in the streaming source.

The `schema` block supports the following:

* `record_columns` - (Required) The Record Column mapping for the streaming source data element.
See [Record Columns](#record-columns) below for more details.
* `record_format` - (Required) The Record Format and mapping information to schematize a record.
See [Record Format](#record-format) below for more details.
* `record_encoding` - (Optional) The Encoding of the record in the streaming source.

#### Parallelism

Configures the number of Parallel in-application streams to create.

The `parallelism` block supports the following:

* `count` - (Required) The Count of streams.

#### Processing Configuration

The Processing Configuration to transform records as they are received from the stream.

The `processing_configuration` block supports the following:

* `lambda` - (Required) The Lambda function configuration. See [Lambda](#lambda) below for more details.

#### Lambda

The Lambda function that pre-processes records in the stream.

The `lambda` block supports the following:

* `resource_arn` - (Required) The ARN of the Lambda function.
* `role_arn` - (Required) The ARN of the IAM Role used to access the Lambda function.

#### Starting Position Configuration

The point at which the application reads from the streaming source.

The `starting_position_configuration` block supports the following:

* `starting_position` - (Required) The starting position on the stream. Valid values: `LAST_STOPPED_POINT`, `NOW`, `TRIM_HORIZON`.

#### Record Columns

The Column mapping of each data element in the streaming source to the corresponding column in the in-application stream.

The `record_columns` block supports the following:

* `name` - (Required) Name of the column.
* `sql_type` - (Required) The SQL Type of the column.
* `mapping` - (Optional) The Mapping reference to the data element.

#### Record Format

The Record Format and relevant mapping information that should be applied to schematize the records on the stream.

The `record_format` block supports the following:

* `record_format_type` - (Required) The type of Record Format. Can be `CSV` or `JSON`.
* `mapping_parameters` - (Optional) The Mapping Information for the record format.
See [Mapping Parameters](#mapping-parameters) below for more details.

#### Mapping Parameters

Provides Mapping information specific to the record format on the streaming source.

The `mapping_parameters` block supports the following:

* `csv` - (Optional) Mapping information when the record format uses delimiters.
See [CSV Mapping Parameters](#csv-mapping-parameters) below for more details.
* `json` - (Optional) Mapping information when JSON is the record format on the streaming source.
See [JSON Mapping Parameters](#json-mapping-parameters) below for more details.

#### CSV Mapping Parameters

Mapping information when the record format uses delimiters.

The `csv` block supports the following:

* `record_column_delimiter` - (Required) The Column Delimiter.
* `record_row_delimiter` - (Required) The Row Delimiter.

#### JSON Mapping Parameters

Mapping information when JSON is the record format on the streaming source.

The `json` block supports the following:

* `record_row_path` - (Required) Path to the top-level parent that contains the records.

#### S3 Reference

Identifies the S3 bucket and object that contains the reference data.

The `s3` blcok supports the following:

* `bucket_arn` - (Required) The S3 Bucket ARN.
* `file_key` - (Required) The File Key name containing reference data.
* `role_arn` - (Required) The IAM Role ARN to read the data.

## Attribute Reference

This resource exports the following attributes in addition to the arguments above:

* `id` - The ARN of the Kinesis Analytics Application.
* `arn` - The ARN of the Kinesis Analytics Appliation.
* `create_timestamp` - The Timestamp when the application version was created.
* `last_update_timestamp` - The Timestamp when the application was last updated.
* `status` - The Status of the application.
* `version` - The Version of the application.
* `tags_all` - A map of tags assigned to the resource, including those inherited from the provider [`default_tags` configuration block](https://registry.terraform.io/providers/hashicorp/aws/latest/docs#default_tags-configuration-block).

[1]: https://docs.aws.amazon.com/kinesisanalytics/latest/dev/what-is.html

## Import

In Terraform v1.5.0 and later, use an [`import` block](https://developer.hashicorp.com/terraform/language/import) to import Kinesis Analytics Application using ARN. For example:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.kinesis_analytics_application import KinesisAnalyticsApplication
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        KinesisAnalyticsApplication.generate_config_for_import(self, "example", "arn:aws:kinesisanalytics:us-west-2:1234567890:application/example")
```

Using `terraform import`, import Kinesis Analytics Application using ARN. For example:

```console
% terraform import aws_kinesis_analytics_application.example arn:aws:kinesisanalytics:us-west-2:1234567890:application/example
```

<!-- cache-key: cdktf-0.20.8 input-14782ea879b9f73b5ee5ad2ef9ec8d20965c88da8a7cb0cb2534adfe79bb7ca6 -->