---
subcategory: "CloudTrail"
layout: "aws"
page_title: "AWS: aws_cloudtrail"
description: |-
  Provides a CloudTrail resource.
---


<!-- Please do not edit this file, it is generated. -->
# Resource: aws_cloudtrail

Provides a CloudTrail resource.

-> **Tip:** For a multi-region trail, this resource must be in the home region of the trail.

-> **Tip:** For an organization trail, this resource must be in the master account of the organization.

## Example Usage

### Basic

Enable CloudTrail to capture all compatible management events in region.
For capturing events from services like IAM, `include_global_service_events` must be enabled.

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import Token, TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.cloudtrail import Cloudtrail
from imports.aws.data_aws_caller_identity import DataAwsCallerIdentity
from imports.aws.data_aws_iam_policy_document import DataAwsIamPolicyDocument
from imports.aws.data_aws_partition import DataAwsPartition
from imports.aws.data_aws_region import DataAwsRegion
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_policy import S3BucketPolicy
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="tf-test-trail",
            force_destroy=True
        )
        current = DataAwsCallerIdentity(self, "current")
        data_aws_partition_current = DataAwsPartition(self, "current_2")
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        data_aws_partition_current.override_logical_id("current")
        data_aws_region_current = DataAwsRegion(self, "current_3")
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        data_aws_region_current.override_logical_id("current")
        data_aws_iam_policy_document_example = DataAwsIamPolicyDocument(self, "example_4",
            statement=[DataAwsIamPolicyDocumentStatement(
                actions=["s3:GetBucketAcl"],
                condition=[DataAwsIamPolicyDocumentStatementCondition(
                    test="StringEquals",
                    values=["arn:${" + data_aws_partition_current.partition + "}:cloudtrail:${" + data_aws_region_current.name + "}:${" + current.account_id + "}:trail/example"
                    ],
                    variable="aws:SourceArn"
                )
                ],
                effect="Allow",
                principals=[DataAwsIamPolicyDocumentStatementPrincipals(
                    identifiers=["cloudtrail.amazonaws.com"],
                    type="Service"
                )
                ],
                resources=[example.arn],
                sid="AWSCloudTrailAclCheck"
            ), DataAwsIamPolicyDocumentStatement(
                actions=["s3:PutObject"],
                condition=[DataAwsIamPolicyDocumentStatementCondition(
                    test="StringEquals",
                    values=["bucket-owner-full-control"],
                    variable="s3:x-amz-acl"
                ), DataAwsIamPolicyDocumentStatementCondition(
                    test="StringEquals",
                    values=["arn:${" + data_aws_partition_current.partition + "}:cloudtrail:${" + data_aws_region_current.name + "}:${" + current.account_id + "}:trail/example"
                    ],
                    variable="aws:SourceArn"
                )
                ],
                effect="Allow",
                principals=[DataAwsIamPolicyDocumentStatementPrincipals(
                    identifiers=["cloudtrail.amazonaws.com"],
                    type="Service"
                )
                ],
                resources=["${" + example.arn + "}/prefix/AWSLogs/${" + current.account_id + "}/*"
                ],
                sid="AWSCloudTrailWrite"
            )
            ]
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        data_aws_iam_policy_document_example.override_logical_id("example")
        aws_s3_bucket_policy_example = S3BucketPolicy(self, "example_5",
            bucket=example.id,
            policy=Token.as_string(data_aws_iam_policy_document_example.json)
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_policy_example.override_logical_id("example")
        aws_cloudtrail_example = Cloudtrail(self, "example_6",
            depends_on=[aws_s3_bucket_policy_example],
            include_global_service_events=False,
            name="example",
            s3_bucket_name=example.id,
            s3_key_prefix="prefix"
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_cloudtrail_example.override_logical_id("example")
```

### Data Event Logging

CloudTrail can log [Data Events](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-cloudtrail.html) for certain services such as S3 objects and Lambda function invocations. Additional information about data event configuration can be found in the following links:

* [CloudTrail API DataResource documentation](https://docs.aws.amazon.com/awscloudtrail/latest/APIReference/API_DataResource.html) (for basic event selector).
* [CloudTrail API AdvancedFieldSelector documentation](https://docs.aws.amazon.com/awscloudtrail/latest/APIReference/API_AdvancedFieldSelector.html) (for advanced event selector).

#### Logging All Lambda Function Invocations By Using Basic Event Selectors

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.cloudtrail import Cloudtrail
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name, *, name, s3BucketName):
        super().__init__(scope, name)
        Cloudtrail(self, "example",
            event_selector=[CloudtrailEventSelector(
                data_resource=[CloudtrailEventSelectorDataResource(
                    type="AWS::Lambda::Function",
                    values=["arn:aws:lambda"]
                )
                ],
                include_management_events=True,
                read_write_type="All"
            )
            ],
            name=name,
            s3_bucket_name=s3_bucket_name
        )
```

#### Logging All S3 Object Events By Using Basic Event Selectors

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.cloudtrail import Cloudtrail
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name, *, name, s3BucketName):
        super().__init__(scope, name)
        Cloudtrail(self, "example",
            event_selector=[CloudtrailEventSelector(
                data_resource=[CloudtrailEventSelectorDataResource(
                    type="AWS::S3::Object",
                    values=["arn:aws:s3"]
                )
                ],
                include_management_events=True,
                read_write_type="All"
            )
            ],
            name=name,
            s3_bucket_name=s3_bucket_name
        )
```

#### Logging Individual S3 Bucket Events By Using Basic Event Selectors

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.cloudtrail import Cloudtrail
from imports.aws.data_aws_s3_bucket import DataAwsS3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name, *, name, s3BucketName):
        super().__init__(scope, name)
        important_bucket = DataAwsS3Bucket(self, "important-bucket",
            bucket="important-bucket"
        )
        Cloudtrail(self, "example",
            event_selector=[CloudtrailEventSelector(
                data_resource=[CloudtrailEventSelectorDataResource(
                    type="AWS::S3::Object",
                    values=["${" + important_bucket.arn + "}/"]
                )
                ],
                include_management_events=True,
                read_write_type="All"
            )
            ],
            name=name,
            s3_bucket_name=s3_bucket_name
        )
```

#### Logging All S3 Object Events Except For Two S3 Buckets By Using Advanced Event Selectors

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.cloudtrail import Cloudtrail
from imports.aws.data_aws_s3_bucket import DataAwsS3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name, *, name, s3BucketName):
        super().__init__(scope, name)
        not_important_bucket1 = DataAwsS3Bucket(self, "not-important-bucket-1",
            bucket="not-important-bucket-1"
        )
        not_important_bucket2 = DataAwsS3Bucket(self, "not-important-bucket-2",
            bucket="not-important-bucket-2"
        )
        Cloudtrail(self, "example",
            advanced_event_selector=[CloudtrailAdvancedEventSelector(
                field_selector=[CloudtrailAdvancedEventSelectorFieldSelector(
                    equal_to=["Data"],
                    field="eventCategory"
                ), CloudtrailAdvancedEventSelectorFieldSelector(
                    field="resources.ARN",
                    not_starts_with=["${" + not_important_bucket1.arn + "}/", "${" + not_important_bucket2.arn + "}/"
                    ]
                ), CloudtrailAdvancedEventSelectorFieldSelector(
                    equal_to=["AWS::S3::Object"],
                    field="resources.type"
                )
                ],
                name="Log all S3 objects events except for two S3 buckets"
            ), CloudtrailAdvancedEventSelector(
                field_selector=[CloudtrailAdvancedEventSelectorFieldSelector(
                    equal_to=["Management"],
                    field="eventCategory"
                )
                ],
                name="Log readOnly and writeOnly management events"
            )
            ],
            name=name,
            s3_bucket_name=s3_bucket_name
        )
```

#### Logging Individual S3 Buckets And Specific Event Names By Using Advanced Event Selectors

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.cloudtrail import Cloudtrail
from imports.aws.data_aws_s3_bucket import DataAwsS3Bucket
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name, *, name, s3BucketName):
        super().__init__(scope, name)
        important_bucket1 = DataAwsS3Bucket(self, "important-bucket-1",
            bucket="important-bucket-1"
        )
        important_bucket2 = DataAwsS3Bucket(self, "important-bucket-2",
            bucket="important-bucket-2"
        )
        important_bucket3 = DataAwsS3Bucket(self, "important-bucket-3",
            bucket="important-bucket-3"
        )
        Cloudtrail(self, "example",
            advanced_event_selector=[CloudtrailAdvancedEventSelector(
                field_selector=[CloudtrailAdvancedEventSelectorFieldSelector(
                    equal_to=["Data"],
                    field="eventCategory"
                ), CloudtrailAdvancedEventSelectorFieldSelector(
                    equal_to=["PutObject", "DeleteObject"],
                    field="eventName"
                ), CloudtrailAdvancedEventSelectorFieldSelector(
                    field="resources.ARN",
                    starts_with=["${" + important_bucket1.arn + "}/", "${" + important_bucket2.arn + "}/"
                    ]
                ), CloudtrailAdvancedEventSelectorFieldSelector(
                    equal_to=["false"],
                    field="readOnly"
                ), CloudtrailAdvancedEventSelectorFieldSelector(
                    equal_to=["AWS::S3::Object"],
                    field="resources.type"
                )
                ],
                name="Log PutObject and DeleteObject events for two S3 buckets"
            ), CloudtrailAdvancedEventSelector(
                field_selector=[CloudtrailAdvancedEventSelectorFieldSelector(
                    equal_to=["Data"],
                    field="eventCategory"
                ), CloudtrailAdvancedEventSelectorFieldSelector(
                    field="eventName",
                    starts_with=["Delete"]
                ), CloudtrailAdvancedEventSelectorFieldSelector(
                    equal_to=["${" + important_bucket3.arn + "}/important-prefix"],
                    field="resources.ARN"
                ), CloudtrailAdvancedEventSelectorFieldSelector(
                    equal_to=["false"],
                    field="readOnly"
                ), CloudtrailAdvancedEventSelectorFieldSelector(
                    equal_to=["AWS::S3::Object"],
                    field="resources.type"
                )
                ],
                name="Log Delete* events for one S3 bucket"
            )
            ],
            name=name,
            s3_bucket_name=s3_bucket_name
        )
```

#### Sending Events to CloudWatch Logs

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.cloudtrail import Cloudtrail
from imports.aws.cloudwatch_log_group import CloudwatchLogGroup
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name, *, name, s3BucketName):
        super().__init__(scope, name)
        example = CloudwatchLogGroup(self, "example",
            name="Example"
        )
        aws_cloudtrail_example = Cloudtrail(self, "example_1",
            cloud_watch_logs_group_arn="${" + example.arn + "}:*",
            name=name,
            s3_bucket_name=s3_bucket_name
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_cloudtrail_example.override_logical_id("example")
```

## Argument Reference

The following arguments are required:

* `name` - (Required) Name of the trail.
* `s3_bucket_name` - (Required) Name of the S3 bucket designated for publishing log files.

The following arguments are optional:

* `advanced_event_selector` - (Optional) Specifies an advanced event selector for enabling data event logging. Fields documented below. Conflicts with `event_selector`.
* `cloud_watch_logs_group_arn` - (Optional) Log group name using an ARN that represents the log group to which CloudTrail logs will be delivered. Note that CloudTrail requires the Log Stream wildcard.
* `cloud_watch_logs_role_arn` - (Optional) Role for the CloudWatch Logs endpoint to assume to write to a userâ€™s log group.
* `enable_log_file_validation` - (Optional) Whether log file integrity validation is enabled. Defaults to `false`.
* `enable_logging` - (Optional) Enables logging for the trail. Defaults to `true`. Setting this to `false` will pause logging.
* `event_selector` - (Optional) Specifies an event selector for enabling data event logging. Fields documented below. Please note the [CloudTrail limits](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/WhatIsCloudTrail-Limits.html) when configuring these. Conflicts with `advanced_event_selector`.
* `include_global_service_events` - (Optional) Whether the trail is publishing events from global services such as IAM to the log files. Defaults to `true`.
* `insight_selector` - (Optional) Configuration block for identifying unusual operational activity. See details below.
* `is_multi_region_trail` - (Optional) Whether the trail is created in the current region or in all regions. Defaults to `false`.
* `is_organization_trail` - (Optional) Whether the trail is an AWS Organizations trail. Organization trails log events for the master account and all member accounts. Can only be created in the organization master account. Defaults to `false`.
* `kms_key_id` - (Optional) KMS key ARN to use to encrypt the logs delivered by CloudTrail.
* `s3_key_prefix` - (Optional) S3 key prefix that follows the name of the bucket you have designated for log file delivery.
* `sns_topic_name` - (Optional) Name of the Amazon SNS topic defined for notification of log file delivery.
* `tags` - (Optional) Map of tags to assign to the trail. If configured with a provider [`default_tags` configuration block](https://registry.terraform.io/providers/hashicorp/aws/latest/docs#default_tags-configuration-block) present, tags with matching keys will overwrite those defined at the provider-level.

### event_selector

* `data_resource` - (Optional) Configuration block for data events. See details below.
* `exclude_management_event_sources` (Optional) -  A set of event sources to exclude. Valid values include: `kms.amazonaws.com` and `rdsdata.amazonaws.com`. `include_management_events` must be set to`true` to allow this.
* `include_management_events` - (Optional) Whether to include management events for your trail. Defaults to `true`.
* `read_write_type` - (Optional) Type of events to log. Valid values are `ReadOnly`, `WriteOnly`, `All`. Default value is `All`.

#### data_resource

* `type` - (Required) Resource type in which you want to log data events. You can specify only the following value: "AWS::S3::Object", "AWS::Lambda::Function" and "AWS::DynamoDB::Table".
* `values` - (Required) List of ARN strings or partial ARN strings to specify selectors for data audit events over data resources. ARN list is specific to single-valued `type`. For example, `arn:aws:s3:::<bucket name>/` for all objects in a bucket, `arn:aws:s3:::<bucket name>/key` for specific objects, `arn:aws:lambda` for all lambda events within an account, `arn:aws:lambda:<region>:<account number>:function:<function name>` for a specific Lambda function, `arn:aws:dynamodb` for all DDB events for all tables within an account, or `arn:aws:dynamodb:<region>:<account number>:table/<table name>` for a specific DynamoDB table.

### insight_selector

* `insight_type` - (Optional) Type of insights to log on a trail. Valid values are: `ApiCallRateInsight` and `ApiErrorRateInsight`.

### Advanced Event Selector Arguments

* `field_selector` (Required) - Specifies the selector statements in an advanced event selector. Fields documented below.
* `name` (Optional) - Name of the advanced event selector.

#### Field Selector Arguments

* `field` (Required) - Field in an event record on which to filter events to be logged. You can specify only the following values: `readOnly`, `eventSource`, `eventName`, `eventCategory`, `resources.type`, `resources.ARN`.
* `ends_with` (Optional) - A list of values that includes events that match the last few characters of the event record field specified as the value of `field`.
* `equals` (Optional) - A list of values that includes events that match the exact value of the event record field specified as the value of `field`. This is the only valid operator that you can use with the `readOnly`, `eventCategory`, and `resources.type` fields.
* `not_ends_with` (Optional) - A list of values that excludes events that match the last few characters of the event record field specified as the value of `field`.
* `not_equals` (Optional) - A list of values that excludes events that match the exact value of the event record field specified as the value of `field`.
* `not_starts_with` (Optional) - A list of values that excludes events that match the first few characters of the event record field specified as the value of `field`.
* `starts_with` (Optional) - A list of values that includes events that match the first few characters of the event record field specified as the value of `field`.

## Attribute Reference

This resource exports the following attributes in addition to the arguments above:

* `arn` - ARN of the trail.
* `home_region` - Region in which the trail was created.
* `id` - ARN of the trail.
* `tags_all` - Map of tags assigned to the resource, including those inherited from the provider [`default_tags` configuration block](https://registry.terraform.io/providers/hashicorp/aws/latest/docs#default_tags-configuration-block).

## Import

In Terraform v1.5.0 and later, use an [`import` block](https://developer.hashicorp.com/terraform/language/import) to import Cloudtrail Trails using the `arn`. For example:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
```

Using `terraform import`, import Cloudtrails using the `arn`. For example:

```console
% terraform import aws_cloudtrail.sample arn:aws:cloudtrail:us-east-1:123456789012:trail/my-sample-trail
```

<!-- cache-key: cdktf-0.19.0 input-5ed397d637a150a8f46c866261ad00abfa51b67e2e83e6d4564b95a1f22de811 -->