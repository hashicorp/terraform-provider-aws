---
subcategory: "S3 (Simple Storage)"
layout: "aws"
page_title: "AWS: aws_s3_bucket_logging"
description: |-
  Provides an S3 bucket (server access) logging resource.
---


<!-- Please do not edit this file, it is generated. -->
# Resource: aws_s3_bucket_logging

Provides an S3 bucket (server access) logging resource. For more information, see [Logging requests using server access logging](https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerLogs.html)
in the AWS S3 User Guide.

~> **Note:** Amazon S3 supports server access logging, AWS CloudTrail, or a combination of both. Refer to the [Logging options for Amazon S3](https://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-S3.html)
to decide which method meets your requirements.

-> This resource cannot be used with S3 directory buckets.

## Example Usage

### Grant permission by using bucket policy

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import Token, TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.data_aws_caller_identity import DataAwsCallerIdentity
from imports.aws.data_aws_iam_policy_document import DataAwsIamPolicyDocument
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_logging import S3BucketLoggingA
from imports.aws.s3_bucket_policy import S3BucketPolicy
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="example-bucket"
        )
        logging = S3Bucket(self, "logging",
            bucket="access-logging-bucket"
        )
        aws_s3_bucket_logging_example = S3BucketLoggingA(self, "example_2",
            bucket=example.bucket,
            target_bucket=logging.bucket,
            target_object_key_format=S3BucketLoggingTargetObjectKeyFormat(
                partitioned_prefix=S3BucketLoggingTargetObjectKeyFormatPartitionedPrefix(
                    partition_date_source="EventTime"
                )
            ),
            target_prefix="log/"
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_logging_example.override_logical_id("example")
        current = DataAwsCallerIdentity(self, "current")
        logging_bucket_policy = DataAwsIamPolicyDocument(self, "logging_bucket_policy",
            statement=[DataAwsIamPolicyDocumentStatement(
                actions=["s3:PutObject"],
                condition=[DataAwsIamPolicyDocumentStatementCondition(
                    test="StringEquals",
                    values=[Token.as_string(current.account_id)],
                    variable="aws:SourceAccount"
                )
                ],
                principals=[DataAwsIamPolicyDocumentStatementPrincipals(
                    identifiers=["logging.s3.amazonaws.com"],
                    type="Service"
                )
                ],
                resources=["${" + logging.arn + "}/*"]
            )
            ]
        )
        aws_s3_bucket_policy_logging = S3BucketPolicy(self, "logging_5",
            bucket=logging.bucket,
            policy=Token.as_string(logging_bucket_policy.json)
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_policy_logging.override_logical_id("logging")
```

### Grant permission by using bucket ACL

The [AWS Documentation](https://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-server-access-logging.html) does not recommend using the ACL.

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket import S3Bucket
from imports.aws.s3_bucket_acl import S3BucketAcl
from imports.aws.s3_bucket_logging import S3BucketLoggingA
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        example = S3Bucket(self, "example",
            bucket="my-tf-example-bucket"
        )
        log_bucket = S3Bucket(self, "log_bucket",
            bucket="my-tf-log-bucket"
        )
        aws_s3_bucket_acl_example = S3BucketAcl(self, "example_2",
            acl="private",
            bucket=example.id
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_acl_example.override_logical_id("example")
        S3BucketAcl(self, "log_bucket_acl",
            acl="log-delivery-write",
            bucket=log_bucket.id
        )
        aws_s3_bucket_logging_example = S3BucketLoggingA(self, "example_4",
            bucket=example.id,
            target_bucket=log_bucket.id,
            target_prefix="log/"
        )
        # This allows the Terraform resource name to match the original name. You can remove the call if you don't need them to match.
        aws_s3_bucket_logging_example.override_logical_id("example")
```

## Argument Reference

This resource supports the following arguments:

* `region` - (Optional) Region where this resource will be [managed](https://docs.aws.amazon.com/general/latest/gr/rande.html#regional-endpoints). Defaults to the Region set in the [provider configuration](https://registry.terraform.io/providers/hashicorp/aws/latest/docs#aws-configuration-reference).
* `bucket` - (Required, Forces new resource) Name of the bucket.
* `expected_bucket_owner` - (Optional, Forces new resource) Account ID of the expected bucket owner.
* `target_bucket` - (Required) Name of the bucket where you want Amazon S3 to store server access logs.
* `target_prefix` - (Required) Prefix for all log object keys.
* `target_grant` - (Optional) Set of configuration blocks with information for granting permissions. [See below](#target_grant).
* `target_object_key_format` - (Optional) Amazon S3 key format for log objects. [See below](#target_object_key_format).

### target_grant

The `target_grant` configuration block supports the following arguments:

* `grantee` - (Required) Configuration block for the person being granted permissions. [See below](#grantee).
* `permission` - (Required) Logging permissions assigned to the grantee for the bucket. Valid values: `FULL_CONTROL`, `READ`, `WRITE`.

### grantee

The `grantee` configuration block supports the following arguments:

* `email_address` - (Optional) Email address of the grantee. See [Regions and Endpoints](https://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region) for supported AWS regions where this argument can be specified.
* `id` - (Optional) Canonical user ID of the grantee.
* `type` - (Required) Type of grantee. Valid values: `CanonicalUser`, `AmazonCustomerByEmail`, `Group`.
* `uri` - (Optional) URI of the grantee group.

### target_object_key_format

The `target_object_key_format` configuration block supports the following arguments:

* `partitioned_prefix` - (Optional) Partitioned S3 key for log objects, in the form `[target_prefix][SourceAccountId]/[SourceRegion]/[SourceBucket]/[YYYY]/[MM]/[DD]/[YYYY]-[MM]-[DD]-[hh]-[mm]-[ss]-[UniqueString]`. Conflicts with `simple_prefix`. [See below](#partitioned_prefix).
* `simple_prefix` - (Optional) Use the simple format for S3 keys for log objects, in the form `[target_prefix][YYYY]-[MM]-[DD]-[hh]-[mm]-[ss]-[UniqueString]`. To use, set `simple_prefix {}`. Conflicts with `partitioned_prefix`.

### partitioned_prefix

The `partitioned_prefix` configuration block supports the following arguments:

* `partition_date_source` - (Required) Specifies the partition date source for the partitioned prefix. Valid values: `EventTime`, `DeliveryTime`.

## Attribute Reference

This resource exports the following attributes in addition to the arguments above:

* `id` - The `bucket` or `bucket` and `expected_bucket_owner` separated by a comma (`,`) if the latter is provided.

## Import

In Terraform v1.5.0 and later, use an [`import` block](https://developer.hashicorp.com/terraform/language/import) to import S3 bucket logging using the `bucket` or using the `bucket` and `expected_bucket_owner` separated by a comma (`,`). For example:

If the owner (account ID) of the source bucket is the same account used to configure the Terraform AWS Provider, import using the `bucket`:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket_logging import S3BucketLoggingA
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3BucketLoggingA.generate_config_for_import(self, "example", "bucket-name")
```

If the owner (account ID) of the source bucket differs from the account used to configure the Terraform AWS Provider, import using the `bucket` and `expected_bucket_owner` separated by a comma (`,`):

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.s3_bucket_logging import S3BucketLoggingA
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        S3BucketLoggingA.generate_config_for_import(self, "example", "bucket-name,123456789012")
```

**Using `terraform import` to import** S3 bucket logging using the `bucket` or using the `bucket` and `expected_bucket_owner` separated by a comma (`,`). For example:

If the owner (account ID) of the source bucket is the same account used to configure the Terraform AWS Provider, import using the `bucket`:

```console
% terraform import aws_s3_bucket_logging.example bucket-name
```

If the owner (account ID) of the source bucket differs from the account used to configure the Terraform AWS Provider, import using the `bucket` and `expected_bucket_owner` separated by a comma (`,`):

```console
% terraform import aws_s3_bucket_logging.example bucket-name,123456789012
```

<!-- cache-key: cdktf-0.20.8 input-b27b4872fa7e8fde0743bcee1cdce11f0221000141618443eeb69aaad06f5fa9 -->