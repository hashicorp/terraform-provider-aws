---
subcategory: "Redshift"
layout: "aws"
page_title: "AWS: aws_redshift_logging"
description: |-
  Terraform resource for managing an AWS Redshift Logging configuration.
---

<!-- Please do not edit this file, it is generated. -->
# Resource: aws_redshift_logging

Terraform resource for managing an AWS Redshift Logging configuration.

## Example Usage

### Basic Usage

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import Token, TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.redshift_logging import RedshiftLogging
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        RedshiftLogging(self, "example",
            cluster_identifier=Token.as_string(aws_redshift_cluster_example.id),
            log_destination_type="cloudwatch",
            log_exports=["connectionlog", "userlog"]
        )
```

### S3 Destination Type

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import Token, TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.redshift_logging import RedshiftLogging
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        RedshiftLogging(self, "example",
            bucket_name=Token.as_string(aws_s3_bucket_example.id),
            cluster_identifier=Token.as_string(aws_redshift_cluster_example.id),
            log_destination_type="s3",
            s3_key_prefix="example-prefix/"
        )
```

## Argument Reference

The following arguments are required:

* `cluster_identifier` - (Required) Identifier of the source cluster.

The following arguments are optional:

* `bucket_name` - (Optional) Name of an existing S3 bucket where the log files are to be stored. Required when `log_destination_type` is `s3`. Must be in the same region as the cluster and the cluster must have read bucket and put object permissions. For more information on the permissions required for the bucket, please read the AWS [documentation](http://docs.aws.amazon.com/redshift/latest/mgmt/db-auditing.html#db-auditing-enable-logging)
* `log_destination_type` - (Optional) Log destination type. Valid values are `s3` and `cloudwatch`.
* `log_exports` - (Optional) Collection of exported log types. Required when `log_destination_type` is `cloudwatch`. Valid values are `connectionlog`, `useractivitylog`, and `userlog`.
* `s3_key_prefix` - (Optional) Prefix applied to the log file names.

## Attribute Reference

This resource exports the following attributes in addition to the arguments above:

* `id` - Identifier of the source cluster.

## Import

In Terraform v1.5.0 and later, use an [`import` block](https://developer.hashicorp.com/terraform/language/import) to import Redshift Logging using the `id`. For example:

```python
# DO NOT EDIT. Code generated by 'cdktf convert' - Please report bugs at https://cdk.tf/bug
from constructs import Construct
from cdktf import TerraformStack
#
# Provider bindings are generated by running `cdktf get`.
# See https://cdk.tf/provider-generation for more details.
#
from imports.aws.redshift_logging import RedshiftLogging
class MyConvertedCode(TerraformStack):
    def __init__(self, scope, name):
        super().__init__(scope, name)
        RedshiftLogging.generate_config_for_import(self, "example", "cluster-id-12345678")
```

Using `terraform import`, import Redshift Logging using the `id`. For example:

```console
% terraform import aws_redshift_logging.example cluster-id-12345678
```

<!-- cache-key: cdktf-0.20.1 input-3a484d99a52bc2d9b765c721c1393918623d13b3e2374fc295ee32d34fe8c944 -->